{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('images', exist_ok=True)\n",
    "# parser is not used\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='size of the batches')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='adam: learning rate')\n",
    "parser.add_argument('--b1', type=float, default=0.5, help='adam: decay of first order momentum of gradient')\n",
    "parser.add_argument('--b2', type=float, default=0.999, help='adam: decay of first order momentum of gradient')\n",
    "parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "parser.add_argument('--latent_dim', type=int, default=100, help='dimensionality of the latent space')\n",
    "parser.add_argument('--img_size', type=int, default=32, help='size of each image dimension')\n",
    "parser.add_argument('--channels', type=int, default=1, help='number of image channels')\n",
    "parser.add_argument('--sample_interval', type=int, default=400, help='interval between image sampling')\n",
    "opt = parser.parse_args()\n",
    "#print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genarator class \n",
    "-  no pooling layer \n",
    "-  added batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = 32 // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(100, 128*self.init_size**2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [   nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(1, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = 32 // 2**4\n",
    "        self.adv_layer = nn.Sequential( nn.Linear(128*ds_size**2, 1),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoad(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def load_data(self):\n",
    "        batch_size = 100\n",
    "        # Image processing\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "        # MNIST dataset\n",
    "        mnist = torchvision.datasets.MNIST(root='E:/DataSet/MNIST',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=True)\n",
    "\n",
    "        # Data loader\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "                \n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Dropout2d(p=0.25)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Dropout2d(p=0.25)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (9): Dropout2d(p=0.25)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (13): Dropout2d(p=0.25)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=DataLoad()\n",
    "dataloader=obj.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\pyt\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/600] [D loss: 0.693282] [G loss: 0.678581]\n",
      "[Epoch 0/200] [Batch 1/600] [D loss: 0.693218] [G loss: 0.679163]\n",
      "[Epoch 0/200] [Batch 2/600] [D loss: 0.693152] [G loss: 0.679794]\n",
      "[Epoch 0/200] [Batch 3/600] [D loss: 0.693105] [G loss: 0.680377]\n",
      "[Epoch 0/200] [Batch 4/600] [D loss: 0.692938] [G loss: 0.680980]\n",
      "[Epoch 0/200] [Batch 5/600] [D loss: 0.692880] [G loss: 0.681572]\n",
      "[Epoch 0/200] [Batch 6/600] [D loss: 0.692813] [G loss: 0.682306]\n",
      "[Epoch 0/200] [Batch 7/600] [D loss: 0.692641] [G loss: 0.682796]\n",
      "[Epoch 0/200] [Batch 8/600] [D loss: 0.692510] [G loss: 0.683353]\n",
      "[Epoch 0/200] [Batch 9/600] [D loss: 0.692403] [G loss: 0.683873]\n",
      "[Epoch 0/200] [Batch 10/600] [D loss: 0.692168] [G loss: 0.684557]\n",
      "[Epoch 0/200] [Batch 11/600] [D loss: 0.691865] [G loss: 0.685088]\n",
      "[Epoch 0/200] [Batch 12/600] [D loss: 0.691762] [G loss: 0.685529]\n",
      "[Epoch 0/200] [Batch 13/600] [D loss: 0.691438] [G loss: 0.685880]\n",
      "[Epoch 0/200] [Batch 14/600] [D loss: 0.691101] [G loss: 0.686163]\n",
      "[Epoch 0/200] [Batch 15/600] [D loss: 0.690916] [G loss: 0.686025]\n",
      "[Epoch 0/200] [Batch 16/600] [D loss: 0.690483] [G loss: 0.685935]\n",
      "[Epoch 0/200] [Batch 17/600] [D loss: 0.690652] [G loss: 0.685443]\n",
      "[Epoch 0/200] [Batch 18/600] [D loss: 0.690476] [G loss: 0.685494]\n",
      "[Epoch 0/200] [Batch 19/600] [D loss: 0.689377] [G loss: 0.686709]\n",
      "[Epoch 0/200] [Batch 20/600] [D loss: 0.690245] [G loss: 0.685272]\n",
      "[Epoch 0/200] [Batch 21/600] [D loss: 0.689775] [G loss: 0.686299]\n",
      "[Epoch 0/200] [Batch 22/600] [D loss: 0.689486] [G loss: 0.687116]\n",
      "[Epoch 0/200] [Batch 23/600] [D loss: 0.688336] [G loss: 0.689216]\n",
      "[Epoch 0/200] [Batch 24/600] [D loss: 0.688359] [G loss: 0.690117]\n",
      "[Epoch 0/200] [Batch 25/600] [D loss: 0.686676] [G loss: 0.693732]\n",
      "[Epoch 0/200] [Batch 26/600] [D loss: 0.685955] [G loss: 0.696764]\n",
      "[Epoch 0/200] [Batch 27/600] [D loss: 0.684146] [G loss: 0.701116]\n",
      "[Epoch 0/200] [Batch 28/600] [D loss: 0.681807] [G loss: 0.704975]\n",
      "[Epoch 0/200] [Batch 29/600] [D loss: 0.679683] [G loss: 0.709562]\n",
      "[Epoch 0/200] [Batch 30/600] [D loss: 0.676083] [G loss: 0.715547]\n",
      "[Epoch 0/200] [Batch 31/600] [D loss: 0.671223] [G loss: 0.722211]\n",
      "[Epoch 0/200] [Batch 32/600] [D loss: 0.667932] [G loss: 0.726907]\n",
      "[Epoch 0/200] [Batch 33/600] [D loss: 0.662398] [G loss: 0.731624]\n",
      "[Epoch 0/200] [Batch 34/600] [D loss: 0.654794] [G loss: 0.739467]\n",
      "[Epoch 0/200] [Batch 35/600] [D loss: 0.650825] [G loss: 0.746917]\n",
      "[Epoch 0/200] [Batch 36/600] [D loss: 0.644882] [G loss: 0.751760]\n",
      "[Epoch 0/200] [Batch 37/600] [D loss: 0.633251] [G loss: 0.758885]\n",
      "[Epoch 0/200] [Batch 38/600] [D loss: 0.625428] [G loss: 0.761297]\n",
      "[Epoch 0/200] [Batch 39/600] [D loss: 0.615029] [G loss: 0.763296]\n",
      "[Epoch 0/200] [Batch 40/600] [D loss: 0.605339] [G loss: 0.776198]\n",
      "[Epoch 0/200] [Batch 41/600] [D loss: 0.590087] [G loss: 0.793825]\n",
      "[Epoch 0/200] [Batch 42/600] [D loss: 0.575245] [G loss: 0.816056]\n",
      "[Epoch 0/200] [Batch 43/600] [D loss: 0.554675] [G loss: 0.837821]\n",
      "[Epoch 0/200] [Batch 44/600] [D loss: 0.538401] [G loss: 0.861422]\n",
      "[Epoch 0/200] [Batch 45/600] [D loss: 0.527141] [G loss: 0.874457]\n",
      "[Epoch 0/200] [Batch 46/600] [D loss: 0.510760] [G loss: 0.907772]\n",
      "[Epoch 0/200] [Batch 47/600] [D loss: 0.505496] [G loss: 0.906426]\n",
      "[Epoch 0/200] [Batch 48/600] [D loss: 0.493035] [G loss: 0.935550]\n",
      "[Epoch 0/200] [Batch 49/600] [D loss: 0.493924] [G loss: 0.920343]\n",
      "[Epoch 0/200] [Batch 50/600] [D loss: 0.475003] [G loss: 0.977233]\n",
      "[Epoch 0/200] [Batch 51/600] [D loss: 0.458655] [G loss: 1.025640]\n",
      "[Epoch 0/200] [Batch 52/600] [D loss: 0.431261] [G loss: 1.078428]\n",
      "[Epoch 0/200] [Batch 53/600] [D loss: 0.416171] [G loss: 1.134820]\n",
      "[Epoch 0/200] [Batch 54/600] [D loss: 0.392014] [G loss: 1.203270]\n",
      "[Epoch 0/200] [Batch 55/600] [D loss: 0.377223] [G loss: 1.234344]\n",
      "[Epoch 0/200] [Batch 56/600] [D loss: 0.366165] [G loss: 1.245986]\n",
      "[Epoch 0/200] [Batch 57/600] [D loss: 0.368298] [G loss: 1.284170]\n",
      "[Epoch 0/200] [Batch 58/600] [D loss: 0.339551] [G loss: 1.285695]\n",
      "[Epoch 0/200] [Batch 59/600] [D loss: 0.328423] [G loss: 1.316530]\n",
      "[Epoch 0/200] [Batch 60/600] [D loss: 0.327333] [G loss: 1.374524]\n",
      "[Epoch 0/200] [Batch 61/600] [D loss: 0.318245] [G loss: 1.402152]\n",
      "[Epoch 0/200] [Batch 62/600] [D loss: 0.302752] [G loss: 1.450318]\n",
      "[Epoch 0/200] [Batch 63/600] [D loss: 0.291509] [G loss: 1.513479]\n",
      "[Epoch 0/200] [Batch 64/600] [D loss: 0.268913] [G loss: 1.501572]\n",
      "[Epoch 0/200] [Batch 65/600] [D loss: 0.256700] [G loss: 1.623921]\n",
      "[Epoch 0/200] [Batch 66/600] [D loss: 0.263183] [G loss: 1.691077]\n",
      "[Epoch 0/200] [Batch 67/600] [D loss: 0.232585] [G loss: 1.668970]\n",
      "[Epoch 0/200] [Batch 68/600] [D loss: 0.235264] [G loss: 1.739602]\n",
      "[Epoch 0/200] [Batch 69/600] [D loss: 0.227417] [G loss: 1.742445]\n",
      "[Epoch 0/200] [Batch 70/600] [D loss: 0.216359] [G loss: 1.763596]\n",
      "[Epoch 0/200] [Batch 71/600] [D loss: 0.208469] [G loss: 1.772649]\n",
      "[Epoch 0/200] [Batch 72/600] [D loss: 0.195691] [G loss: 2.009168]\n",
      "[Epoch 0/200] [Batch 73/600] [D loss: 0.192892] [G loss: 1.995547]\n",
      "[Epoch 0/200] [Batch 74/600] [D loss: 0.182034] [G loss: 2.017550]\n",
      "[Epoch 0/200] [Batch 75/600] [D loss: 0.188348] [G loss: 2.111517]\n",
      "[Epoch 0/200] [Batch 76/600] [D loss: 0.160206] [G loss: 2.056693]\n",
      "[Epoch 0/200] [Batch 77/600] [D loss: 0.161536] [G loss: 2.158468]\n",
      "[Epoch 0/200] [Batch 78/600] [D loss: 0.162217] [G loss: 2.275905]\n",
      "[Epoch 0/200] [Batch 79/600] [D loss: 0.166751] [G loss: 2.261124]\n",
      "[Epoch 0/200] [Batch 80/600] [D loss: 0.157259] [G loss: 2.286774]\n",
      "[Epoch 0/200] [Batch 81/600] [D loss: 0.133947] [G loss: 2.379616]\n",
      "[Epoch 0/200] [Batch 82/600] [D loss: 0.149435] [G loss: 2.319235]\n",
      "[Epoch 0/200] [Batch 83/600] [D loss: 0.128433] [G loss: 2.352234]\n",
      "[Epoch 0/200] [Batch 84/600] [D loss: 0.125536] [G loss: 2.428649]\n",
      "[Epoch 0/200] [Batch 85/600] [D loss: 0.110658] [G loss: 2.360347]\n",
      "[Epoch 0/200] [Batch 86/600] [D loss: 0.117694] [G loss: 2.559152]\n",
      "[Epoch 0/200] [Batch 87/600] [D loss: 0.099170] [G loss: 2.595259]\n",
      "[Epoch 0/200] [Batch 88/600] [D loss: 0.099885] [G loss: 2.587090]\n",
      "[Epoch 0/200] [Batch 89/600] [D loss: 0.091397] [G loss: 2.652690]\n",
      "[Epoch 0/200] [Batch 90/600] [D loss: 0.095199] [G loss: 2.752755]\n",
      "[Epoch 0/200] [Batch 91/600] [D loss: 0.092138] [G loss: 2.879753]\n",
      "[Epoch 0/200] [Batch 92/600] [D loss: 0.083766] [G loss: 2.859825]\n",
      "[Epoch 0/200] [Batch 93/600] [D loss: 0.073795] [G loss: 2.798002]\n",
      "[Epoch 0/200] [Batch 94/600] [D loss: 0.071425] [G loss: 2.935126]\n",
      "[Epoch 0/200] [Batch 95/600] [D loss: 0.071450] [G loss: 3.080943]\n",
      "[Epoch 0/200] [Batch 96/600] [D loss: 0.070099] [G loss: 2.985715]\n",
      "[Epoch 0/200] [Batch 97/600] [D loss: 0.059858] [G loss: 3.018441]\n",
      "[Epoch 0/200] [Batch 98/600] [D loss: 0.057043] [G loss: 3.075102]\n",
      "[Epoch 0/200] [Batch 99/600] [D loss: 0.070498] [G loss: 3.198659]\n",
      "[Epoch 0/200] [Batch 100/600] [D loss: 0.060444] [G loss: 3.321734]\n",
      "[Epoch 0/200] [Batch 101/600] [D loss: 0.056427] [G loss: 3.313900]\n",
      "[Epoch 0/200] [Batch 102/600] [D loss: 0.048972] [G loss: 3.313383]\n",
      "[Epoch 0/200] [Batch 103/600] [D loss: 0.047704] [G loss: 3.467683]\n",
      "[Epoch 0/200] [Batch 104/600] [D loss: 0.046649] [G loss: 3.466584]\n",
      "[Epoch 0/200] [Batch 105/600] [D loss: 0.046463] [G loss: 3.483458]\n",
      "[Epoch 0/200] [Batch 106/600] [D loss: 0.044256] [G loss: 3.440865]\n",
      "[Epoch 0/200] [Batch 107/600] [D loss: 0.043592] [G loss: 3.613914]\n",
      "[Epoch 0/200] [Batch 108/600] [D loss: 0.036042] [G loss: 3.708246]\n",
      "[Epoch 0/200] [Batch 109/600] [D loss: 0.035542] [G loss: 3.714168]\n",
      "[Epoch 0/200] [Batch 110/600] [D loss: 0.032667] [G loss: 3.721180]\n",
      "[Epoch 0/200] [Batch 111/600] [D loss: 0.030766] [G loss: 3.662657]\n",
      "[Epoch 0/200] [Batch 112/600] [D loss: 0.031070] [G loss: 3.721509]\n",
      "[Epoch 0/200] [Batch 113/600] [D loss: 0.031946] [G loss: 3.810582]\n",
      "[Epoch 0/200] [Batch 114/600] [D loss: 0.031887] [G loss: 3.916694]\n",
      "[Epoch 0/200] [Batch 115/600] [D loss: 0.028188] [G loss: 3.891385]\n",
      "[Epoch 0/200] [Batch 116/600] [D loss: 0.028981] [G loss: 3.808896]\n",
      "[Epoch 0/200] [Batch 117/600] [D loss: 0.024170] [G loss: 4.130422]\n",
      "[Epoch 0/200] [Batch 118/600] [D loss: 0.023547] [G loss: 4.086886]\n",
      "[Epoch 0/200] [Batch 119/600] [D loss: 0.025938] [G loss: 4.189808]\n",
      "[Epoch 0/200] [Batch 120/600] [D loss: 0.020604] [G loss: 4.105546]\n",
      "[Epoch 0/200] [Batch 121/600] [D loss: 0.022381] [G loss: 4.131272]\n",
      "[Epoch 0/200] [Batch 122/600] [D loss: 0.021077] [G loss: 4.209023]\n",
      "[Epoch 0/200] [Batch 123/600] [D loss: 0.022344] [G loss: 4.288737]\n",
      "[Epoch 0/200] [Batch 124/600] [D loss: 0.021931] [G loss: 4.320630]\n",
      "[Epoch 0/200] [Batch 125/600] [D loss: 0.017788] [G loss: 4.283290]\n",
      "[Epoch 0/200] [Batch 126/600] [D loss: 0.018968] [G loss: 4.117746]\n",
      "[Epoch 0/200] [Batch 127/600] [D loss: 0.020955] [G loss: 4.465309]\n",
      "[Epoch 0/200] [Batch 128/600] [D loss: 0.018250] [G loss: 4.281158]\n",
      "[Epoch 0/200] [Batch 129/600] [D loss: 0.015385] [G loss: 4.515330]\n",
      "[Epoch 0/200] [Batch 130/600] [D loss: 0.017906] [G loss: 4.591215]\n",
      "[Epoch 0/200] [Batch 131/600] [D loss: 0.014927] [G loss: 4.431914]\n",
      "[Epoch 0/200] [Batch 132/600] [D loss: 0.016404] [G loss: 4.469184]\n",
      "[Epoch 0/200] [Batch 133/600] [D loss: 0.015287] [G loss: 4.567122]\n",
      "[Epoch 0/200] [Batch 134/600] [D loss: 0.014561] [G loss: 4.753715]\n",
      "[Epoch 0/200] [Batch 135/600] [D loss: 0.015266] [G loss: 4.547439]\n",
      "[Epoch 0/200] [Batch 136/600] [D loss: 0.016137] [G loss: 4.616260]\n",
      "[Epoch 0/200] [Batch 137/600] [D loss: 0.012798] [G loss: 4.582748]\n",
      "[Epoch 0/200] [Batch 138/600] [D loss: 0.013386] [G loss: 4.553386]\n",
      "[Epoch 0/200] [Batch 139/600] [D loss: 0.013278] [G loss: 4.591734]\n",
      "[Epoch 0/200] [Batch 140/600] [D loss: 0.015549] [G loss: 4.828659]\n",
      "[Epoch 0/200] [Batch 141/600] [D loss: 0.014329] [G loss: 4.736979]\n",
      "[Epoch 0/200] [Batch 142/600] [D loss: 0.013221] [G loss: 4.472136]\n",
      "[Epoch 0/200] [Batch 143/600] [D loss: 0.012063] [G loss: 4.701753]\n",
      "[Epoch 0/200] [Batch 144/600] [D loss: 0.014614] [G loss: 4.821199]\n",
      "[Epoch 0/200] [Batch 145/600] [D loss: 0.012131] [G loss: 4.879587]\n",
      "[Epoch 0/200] [Batch 146/600] [D loss: 0.011622] [G loss: 4.658503]\n",
      "[Epoch 0/200] [Batch 147/600] [D loss: 0.012115] [G loss: 4.817234]\n",
      "[Epoch 0/200] [Batch 148/600] [D loss: 0.011607] [G loss: 4.699037]\n",
      "[Epoch 0/200] [Batch 149/600] [D loss: 0.011620] [G loss: 4.920210]\n",
      "[Epoch 0/200] [Batch 150/600] [D loss: 0.013467] [G loss: 4.851859]\n",
      "[Epoch 0/200] [Batch 151/600] [D loss: 0.011387] [G loss: 4.902278]\n",
      "[Epoch 0/200] [Batch 152/600] [D loss: 0.010409] [G loss: 5.158293]\n",
      "[Epoch 0/200] [Batch 153/600] [D loss: 0.008592] [G loss: 4.865026]\n",
      "[Epoch 0/200] [Batch 154/600] [D loss: 0.010238] [G loss: 5.110872]\n",
      "[Epoch 0/200] [Batch 155/600] [D loss: 0.009548] [G loss: 5.112895]\n",
      "[Epoch 0/200] [Batch 156/600] [D loss: 0.009671] [G loss: 4.899890]\n",
      "[Epoch 0/200] [Batch 157/600] [D loss: 0.008675] [G loss: 4.916851]\n",
      "[Epoch 0/200] [Batch 158/600] [D loss: 0.009722] [G loss: 4.893603]\n",
      "[Epoch 0/200] [Batch 159/600] [D loss: 0.008687] [G loss: 4.960860]\n",
      "[Epoch 0/200] [Batch 160/600] [D loss: 0.008952] [G loss: 5.270000]\n",
      "[Epoch 0/200] [Batch 161/600] [D loss: 0.009044] [G loss: 5.004159]\n",
      "[Epoch 0/200] [Batch 162/600] [D loss: 0.008654] [G loss: 4.936857]\n",
      "[Epoch 0/200] [Batch 163/600] [D loss: 0.008560] [G loss: 5.199690]\n",
      "[Epoch 0/200] [Batch 164/600] [D loss: 0.008028] [G loss: 5.000802]\n",
      "[Epoch 0/200] [Batch 165/600] [D loss: 0.008442] [G loss: 5.306839]\n",
      "[Epoch 0/200] [Batch 166/600] [D loss: 0.007211] [G loss: 5.388351]\n",
      "[Epoch 0/200] [Batch 167/600] [D loss: 0.008337] [G loss: 5.264839]\n",
      "[Epoch 0/200] [Batch 168/600] [D loss: 0.007662] [G loss: 5.312461]\n",
      "[Epoch 0/200] [Batch 169/600] [D loss: 0.008329] [G loss: 5.392563]\n",
      "[Epoch 0/200] [Batch 170/600] [D loss: 0.007439] [G loss: 5.310425]\n",
      "[Epoch 0/200] [Batch 171/600] [D loss: 0.010203] [G loss: 5.343095]\n",
      "[Epoch 0/200] [Batch 172/600] [D loss: 0.008170] [G loss: 5.217494]\n",
      "[Epoch 0/200] [Batch 173/600] [D loss: 0.007069] [G loss: 5.513824]\n",
      "[Epoch 0/200] [Batch 174/600] [D loss: 0.006248] [G loss: 5.361607]\n",
      "[Epoch 0/200] [Batch 175/600] [D loss: 0.006075] [G loss: 5.358975]\n",
      "[Epoch 0/200] [Batch 176/600] [D loss: 0.007450] [G loss: 5.328995]\n",
      "[Epoch 0/200] [Batch 177/600] [D loss: 0.008017] [G loss: 5.452761]\n",
      "[Epoch 0/200] [Batch 178/600] [D loss: 0.005920] [G loss: 5.379492]\n",
      "[Epoch 0/200] [Batch 179/600] [D loss: 0.007100] [G loss: 5.242063]\n",
      "[Epoch 0/200] [Batch 180/600] [D loss: 0.007232] [G loss: 5.277927]\n",
      "[Epoch 0/200] [Batch 181/600] [D loss: 0.006851] [G loss: 5.510120]\n",
      "[Epoch 0/200] [Batch 182/600] [D loss: 0.006546] [G loss: 5.504628]\n",
      "[Epoch 0/200] [Batch 183/600] [D loss: 0.005537] [G loss: 5.422020]\n",
      "[Epoch 0/200] [Batch 184/600] [D loss: 0.005809] [G loss: 5.484656]\n",
      "[Epoch 0/200] [Batch 185/600] [D loss: 0.006047] [G loss: 5.618894]\n",
      "[Epoch 0/200] [Batch 186/600] [D loss: 0.008326] [G loss: 5.500329]\n",
      "[Epoch 0/200] [Batch 187/600] [D loss: 0.005874] [G loss: 5.602679]\n",
      "[Epoch 0/200] [Batch 188/600] [D loss: 0.006607] [G loss: 5.470942]\n",
      "[Epoch 0/200] [Batch 189/600] [D loss: 0.006714] [G loss: 5.577098]\n",
      "[Epoch 0/200] [Batch 190/600] [D loss: 0.004603] [G loss: 5.509523]\n",
      "[Epoch 0/200] [Batch 191/600] [D loss: 0.006144] [G loss: 5.362908]\n",
      "[Epoch 0/200] [Batch 192/600] [D loss: 0.006888] [G loss: 5.572717]\n",
      "[Epoch 0/200] [Batch 193/600] [D loss: 0.006555] [G loss: 5.723540]\n",
      "[Epoch 0/200] [Batch 194/600] [D loss: 0.004843] [G loss: 5.692355]\n",
      "[Epoch 0/200] [Batch 195/600] [D loss: 0.006378] [G loss: 5.710320]\n",
      "[Epoch 0/200] [Batch 196/600] [D loss: 0.004605] [G loss: 5.808060]\n",
      "[Epoch 0/200] [Batch 197/600] [D loss: 0.005018] [G loss: 5.783089]\n",
      "[Epoch 0/200] [Batch 198/600] [D loss: 0.004003] [G loss: 5.780527]\n",
      "[Epoch 0/200] [Batch 199/600] [D loss: 0.006907] [G loss: 5.573674]\n",
      "[Epoch 0/200] [Batch 200/600] [D loss: 0.005585] [G loss: 5.775239]\n",
      "[Epoch 0/200] [Batch 201/600] [D loss: 0.005057] [G loss: 5.839466]\n",
      "[Epoch 0/200] [Batch 202/600] [D loss: 0.005042] [G loss: 5.871661]\n",
      "[Epoch 0/200] [Batch 203/600] [D loss: 0.004116] [G loss: 5.970462]\n",
      "[Epoch 0/200] [Batch 204/600] [D loss: 0.005261] [G loss: 5.850563]\n",
      "[Epoch 0/200] [Batch 205/600] [D loss: 0.005164] [G loss: 5.880153]\n",
      "[Epoch 0/200] [Batch 206/600] [D loss: 0.004269] [G loss: 5.794638]\n",
      "[Epoch 0/200] [Batch 207/600] [D loss: 0.004159] [G loss: 5.658925]\n",
      "[Epoch 0/200] [Batch 208/600] [D loss: 0.004682] [G loss: 5.855405]\n",
      "[Epoch 0/200] [Batch 209/600] [D loss: 0.003922] [G loss: 5.847651]\n",
      "[Epoch 0/200] [Batch 210/600] [D loss: 0.003841] [G loss: 5.894667]\n",
      "[Epoch 0/200] [Batch 211/600] [D loss: 0.004264] [G loss: 5.919817]\n",
      "[Epoch 0/200] [Batch 212/600] [D loss: 0.005697] [G loss: 5.923229]\n",
      "[Epoch 0/200] [Batch 213/600] [D loss: 0.005446] [G loss: 5.850254]\n",
      "[Epoch 0/200] [Batch 214/600] [D loss: 0.004050] [G loss: 5.970024]\n",
      "[Epoch 0/200] [Batch 215/600] [D loss: 0.003992] [G loss: 5.851651]\n",
      "[Epoch 0/200] [Batch 216/600] [D loss: 0.003418] [G loss: 5.918384]\n",
      "[Epoch 0/200] [Batch 217/600] [D loss: 0.004087] [G loss: 5.839018]\n",
      "[Epoch 0/200] [Batch 218/600] [D loss: 0.004356] [G loss: 5.980493]\n",
      "[Epoch 0/200] [Batch 219/600] [D loss: 0.003555] [G loss: 5.968410]\n",
      "[Epoch 0/200] [Batch 220/600] [D loss: 0.004096] [G loss: 6.098114]\n",
      "[Epoch 0/200] [Batch 221/600] [D loss: 0.003435] [G loss: 6.311133]\n",
      "[Epoch 0/200] [Batch 222/600] [D loss: 0.004054] [G loss: 6.136437]\n",
      "[Epoch 0/200] [Batch 223/600] [D loss: 0.003477] [G loss: 5.891283]\n",
      "[Epoch 0/200] [Batch 224/600] [D loss: 0.004457] [G loss: 5.860542]\n",
      "[Epoch 0/200] [Batch 225/600] [D loss: 0.003235] [G loss: 6.049455]\n",
      "[Epoch 0/200] [Batch 226/600] [D loss: 0.004645] [G loss: 6.017524]\n",
      "[Epoch 0/200] [Batch 227/600] [D loss: 0.003773] [G loss: 5.949223]\n",
      "[Epoch 0/200] [Batch 228/600] [D loss: 0.003463] [G loss: 6.129854]\n",
      "[Epoch 0/200] [Batch 229/600] [D loss: 0.003262] [G loss: 6.238985]\n",
      "[Epoch 0/200] [Batch 230/600] [D loss: 0.003845] [G loss: 6.114763]\n",
      "[Epoch 0/200] [Batch 231/600] [D loss: 0.003441] [G loss: 6.155237]\n",
      "[Epoch 0/200] [Batch 232/600] [D loss: 0.003344] [G loss: 6.076403]\n",
      "[Epoch 0/200] [Batch 233/600] [D loss: 0.002687] [G loss: 6.262667]\n",
      "[Epoch 0/200] [Batch 234/600] [D loss: 0.003367] [G loss: 6.432677]\n",
      "[Epoch 0/200] [Batch 235/600] [D loss: 0.003158] [G loss: 6.192349]\n",
      "[Epoch 0/200] [Batch 236/600] [D loss: 0.003177] [G loss: 6.277714]\n",
      "[Epoch 0/200] [Batch 237/600] [D loss: 0.002943] [G loss: 6.343099]\n",
      "[Epoch 0/200] [Batch 238/600] [D loss: 0.003849] [G loss: 6.196329]\n",
      "[Epoch 0/200] [Batch 239/600] [D loss: 0.002640] [G loss: 6.121091]\n",
      "[Epoch 0/200] [Batch 240/600] [D loss: 0.002577] [G loss: 6.006419]\n",
      "[Epoch 0/200] [Batch 241/600] [D loss: 0.002430] [G loss: 6.251812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 242/600] [D loss: 0.003182] [G loss: 6.253016]\n",
      "[Epoch 0/200] [Batch 243/600] [D loss: 0.002791] [G loss: 6.330124]\n",
      "[Epoch 0/200] [Batch 244/600] [D loss: 0.002594] [G loss: 6.226155]\n",
      "[Epoch 0/200] [Batch 245/600] [D loss: 0.002746] [G loss: 6.256879]\n",
      "[Epoch 0/200] [Batch 246/600] [D loss: 0.002882] [G loss: 6.232729]\n",
      "[Epoch 0/200] [Batch 247/600] [D loss: 0.003653] [G loss: 6.452109]\n",
      "[Epoch 0/200] [Batch 248/600] [D loss: 0.002937] [G loss: 6.470602]\n",
      "[Epoch 0/200] [Batch 249/600] [D loss: 0.003109] [G loss: 6.499826]\n",
      "[Epoch 0/200] [Batch 250/600] [D loss: 0.002817] [G loss: 6.385987]\n",
      "[Epoch 0/200] [Batch 251/600] [D loss: 0.002815] [G loss: 6.418725]\n",
      "[Epoch 0/200] [Batch 252/600] [D loss: 0.003766] [G loss: 6.478392]\n",
      "[Epoch 0/200] [Batch 253/600] [D loss: 0.002572] [G loss: 6.461334]\n",
      "[Epoch 0/200] [Batch 254/600] [D loss: 0.002012] [G loss: 6.422729]\n",
      "[Epoch 0/200] [Batch 255/600] [D loss: 0.002766] [G loss: 6.334690]\n",
      "[Epoch 0/200] [Batch 256/600] [D loss: 0.002778] [G loss: 6.509555]\n",
      "[Epoch 0/200] [Batch 257/600] [D loss: 0.002524] [G loss: 6.546119]\n",
      "[Epoch 0/200] [Batch 258/600] [D loss: 0.003213] [G loss: 6.392547]\n",
      "[Epoch 0/200] [Batch 259/600] [D loss: 0.003009] [G loss: 6.399504]\n",
      "[Epoch 0/200] [Batch 260/600] [D loss: 0.002425] [G loss: 6.479276]\n",
      "[Epoch 0/200] [Batch 261/600] [D loss: 0.003364] [G loss: 6.495512]\n",
      "[Epoch 0/200] [Batch 262/600] [D loss: 0.002299] [G loss: 6.585131]\n",
      "[Epoch 0/200] [Batch 263/600] [D loss: 0.001973] [G loss: 6.481660]\n",
      "[Epoch 0/200] [Batch 264/600] [D loss: 0.002358] [G loss: 6.489553]\n",
      "[Epoch 0/200] [Batch 265/600] [D loss: 0.002253] [G loss: 6.669103]\n",
      "[Epoch 0/200] [Batch 266/600] [D loss: 0.002685] [G loss: 6.623599]\n",
      "[Epoch 0/200] [Batch 267/600] [D loss: 0.002498] [G loss: 6.546038]\n",
      "[Epoch 0/200] [Batch 268/600] [D loss: 0.002447] [G loss: 6.294732]\n",
      "[Epoch 0/200] [Batch 269/600] [D loss: 0.002460] [G loss: 6.463602]\n",
      "[Epoch 0/200] [Batch 270/600] [D loss: 0.002182] [G loss: 6.666301]\n",
      "[Epoch 0/200] [Batch 271/600] [D loss: 0.002641] [G loss: 6.715086]\n",
      "[Epoch 0/200] [Batch 272/600] [D loss: 0.002864] [G loss: 6.696723]\n",
      "[Epoch 0/200] [Batch 273/600] [D loss: 0.002371] [G loss: 6.380270]\n",
      "[Epoch 0/200] [Batch 274/600] [D loss: 0.002472] [G loss: 6.403193]\n",
      "[Epoch 0/200] [Batch 275/600] [D loss: 0.002495] [G loss: 6.572179]\n",
      "[Epoch 0/200] [Batch 276/600] [D loss: 0.002773] [G loss: 6.692774]\n",
      "[Epoch 0/200] [Batch 277/600] [D loss: 0.001957] [G loss: 6.708063]\n",
      "[Epoch 0/200] [Batch 278/600] [D loss: 0.002110] [G loss: 6.576724]\n",
      "[Epoch 0/200] [Batch 279/600] [D loss: 0.004309] [G loss: 6.621181]\n",
      "[Epoch 0/200] [Batch 280/600] [D loss: 0.003119] [G loss: 6.667487]\n",
      "[Epoch 0/200] [Batch 281/600] [D loss: 0.002977] [G loss: 6.561053]\n",
      "[Epoch 0/200] [Batch 282/600] [D loss: 0.001909] [G loss: 6.664641]\n",
      "[Epoch 0/200] [Batch 283/600] [D loss: 0.002212] [G loss: 6.656837]\n",
      "[Epoch 0/200] [Batch 284/600] [D loss: 0.002460] [G loss: 6.688039]\n",
      "[Epoch 0/200] [Batch 285/600] [D loss: 0.001746] [G loss: 6.688993]\n",
      "[Epoch 0/200] [Batch 286/600] [D loss: 0.002029] [G loss: 6.917709]\n",
      "[Epoch 0/200] [Batch 287/600] [D loss: 0.002088] [G loss: 7.005578]\n",
      "[Epoch 0/200] [Batch 288/600] [D loss: 0.001906] [G loss: 6.854388]\n",
      "[Epoch 0/200] [Batch 289/600] [D loss: 0.001944] [G loss: 6.630894]\n",
      "[Epoch 0/200] [Batch 290/600] [D loss: 0.002104] [G loss: 6.621373]\n",
      "[Epoch 0/200] [Batch 291/600] [D loss: 0.002010] [G loss: 6.811454]\n",
      "[Epoch 0/200] [Batch 292/600] [D loss: 0.002043] [G loss: 6.733494]\n",
      "[Epoch 0/200] [Batch 293/600] [D loss: 0.001849] [G loss: 6.774603]\n",
      "[Epoch 0/200] [Batch 294/600] [D loss: 0.002053] [G loss: 6.708780]\n",
      "[Epoch 0/200] [Batch 295/600] [D loss: 0.002374] [G loss: 6.882157]\n",
      "[Epoch 0/200] [Batch 296/600] [D loss: 0.001648] [G loss: 6.685613]\n",
      "[Epoch 0/200] [Batch 297/600] [D loss: 0.001768] [G loss: 6.684281]\n",
      "[Epoch 0/200] [Batch 298/600] [D loss: 0.001869] [G loss: 6.519003]\n",
      "[Epoch 0/200] [Batch 299/600] [D loss: 0.001862] [G loss: 6.859581]\n",
      "[Epoch 0/200] [Batch 300/600] [D loss: 0.001939] [G loss: 6.771684]\n",
      "[Epoch 0/200] [Batch 301/600] [D loss: 0.001690] [G loss: 6.818830]\n",
      "[Epoch 0/200] [Batch 302/600] [D loss: 0.001892] [G loss: 6.618521]\n",
      "[Epoch 0/200] [Batch 303/600] [D loss: 0.001903] [G loss: 7.104682]\n",
      "[Epoch 0/200] [Batch 304/600] [D loss: 0.002267] [G loss: 6.922761]\n",
      "[Epoch 0/200] [Batch 305/600] [D loss: 0.001783] [G loss: 6.864249]\n",
      "[Epoch 0/200] [Batch 306/600] [D loss: 0.001959] [G loss: 6.758720]\n",
      "[Epoch 0/200] [Batch 307/600] [D loss: 0.001592] [G loss: 6.969420]\n",
      "[Epoch 0/200] [Batch 308/600] [D loss: 0.001758] [G loss: 6.748267]\n",
      "[Epoch 0/200] [Batch 309/600] [D loss: 0.001786] [G loss: 6.938024]\n",
      "[Epoch 0/200] [Batch 310/600] [D loss: 0.001672] [G loss: 6.987255]\n",
      "[Epoch 0/200] [Batch 311/600] [D loss: 0.001566] [G loss: 7.263247]\n",
      "[Epoch 0/200] [Batch 312/600] [D loss: 0.001850] [G loss: 6.971910]\n",
      "[Epoch 0/200] [Batch 313/600] [D loss: 0.001804] [G loss: 6.860446]\n",
      "[Epoch 0/200] [Batch 314/600] [D loss: 0.001647] [G loss: 6.930996]\n",
      "[Epoch 0/200] [Batch 315/600] [D loss: 0.001880] [G loss: 6.993860]\n",
      "[Epoch 0/200] [Batch 316/600] [D loss: 0.001726] [G loss: 6.684554]\n",
      "[Epoch 0/200] [Batch 317/600] [D loss: 0.001652] [G loss: 6.754825]\n",
      "[Epoch 0/200] [Batch 318/600] [D loss: 0.001672] [G loss: 7.012414]\n",
      "[Epoch 0/200] [Batch 319/600] [D loss: 0.001845] [G loss: 6.712574]\n",
      "[Epoch 0/200] [Batch 320/600] [D loss: 0.001636] [G loss: 6.992630]\n",
      "[Epoch 0/200] [Batch 321/600] [D loss: 0.002952] [G loss: 6.771114]\n",
      "[Epoch 0/200] [Batch 322/600] [D loss: 0.001732] [G loss: 6.632413]\n",
      "[Epoch 0/200] [Batch 323/600] [D loss: 0.001781] [G loss: 6.834048]\n",
      "[Epoch 0/200] [Batch 324/600] [D loss: 0.001607] [G loss: 6.955249]\n",
      "[Epoch 0/200] [Batch 325/600] [D loss: 0.001933] [G loss: 7.034789]\n",
      "[Epoch 0/200] [Batch 326/600] [D loss: 0.001725] [G loss: 6.641117]\n",
      "[Epoch 0/200] [Batch 327/600] [D loss: 0.001579] [G loss: 7.140604]\n",
      "[Epoch 0/200] [Batch 328/600] [D loss: 0.002366] [G loss: 6.961725]\n",
      "[Epoch 0/200] [Batch 329/600] [D loss: 0.001345] [G loss: 7.128584]\n",
      "[Epoch 0/200] [Batch 330/600] [D loss: 0.001847] [G loss: 6.951820]\n",
      "[Epoch 0/200] [Batch 331/600] [D loss: 0.001426] [G loss: 7.065218]\n",
      "[Epoch 0/200] [Batch 332/600] [D loss: 0.001413] [G loss: 6.973816]\n",
      "[Epoch 0/200] [Batch 333/600] [D loss: 0.001779] [G loss: 7.000554]\n",
      "[Epoch 0/200] [Batch 334/600] [D loss: 0.001520] [G loss: 6.999774]\n",
      "[Epoch 0/200] [Batch 335/600] [D loss: 0.001503] [G loss: 6.993456]\n",
      "[Epoch 0/200] [Batch 336/600] [D loss: 0.001561] [G loss: 7.080743]\n",
      "[Epoch 0/200] [Batch 337/600] [D loss: 0.001207] [G loss: 6.997395]\n",
      "[Epoch 0/200] [Batch 338/600] [D loss: 0.001132] [G loss: 7.135355]\n",
      "[Epoch 0/200] [Batch 339/600] [D loss: 0.001613] [G loss: 7.098282]\n",
      "[Epoch 0/200] [Batch 340/600] [D loss: 0.001059] [G loss: 7.212681]\n",
      "[Epoch 0/200] [Batch 341/600] [D loss: 0.001883] [G loss: 7.209037]\n",
      "[Epoch 0/200] [Batch 342/600] [D loss: 0.001208] [G loss: 7.245837]\n",
      "[Epoch 0/200] [Batch 343/600] [D loss: 0.001423] [G loss: 6.974228]\n",
      "[Epoch 0/200] [Batch 344/600] [D loss: 0.001249] [G loss: 6.973453]\n",
      "[Epoch 0/200] [Batch 345/600] [D loss: 0.001217] [G loss: 7.081855]\n",
      "[Epoch 0/200] [Batch 346/600] [D loss: 0.001281] [G loss: 7.231452]\n",
      "[Epoch 0/200] [Batch 347/600] [D loss: 0.001174] [G loss: 7.064403]\n",
      "[Epoch 0/200] [Batch 348/600] [D loss: 0.001168] [G loss: 7.277727]\n",
      "[Epoch 0/200] [Batch 349/600] [D loss: 0.001532] [G loss: 7.060419]\n",
      "[Epoch 0/200] [Batch 350/600] [D loss: 0.002281] [G loss: 7.252944]\n",
      "[Epoch 0/200] [Batch 351/600] [D loss: 0.001437] [G loss: 7.056830]\n",
      "[Epoch 0/200] [Batch 352/600] [D loss: 0.001241] [G loss: 7.119233]\n",
      "[Epoch 0/200] [Batch 353/600] [D loss: 0.001507] [G loss: 7.207169]\n",
      "[Epoch 0/200] [Batch 354/600] [D loss: 0.001373] [G loss: 7.412737]\n",
      "[Epoch 0/200] [Batch 355/600] [D loss: 0.001670] [G loss: 7.465017]\n",
      "[Epoch 0/200] [Batch 356/600] [D loss: 0.001399] [G loss: 7.350376]\n",
      "[Epoch 0/200] [Batch 357/600] [D loss: 0.001131] [G loss: 7.223336]\n",
      "[Epoch 0/200] [Batch 358/600] [D loss: 0.001152] [G loss: 7.413553]\n",
      "[Epoch 0/200] [Batch 359/600] [D loss: 0.001159] [G loss: 7.310060]\n",
      "[Epoch 0/200] [Batch 360/600] [D loss: 0.001093] [G loss: 7.263814]\n",
      "[Epoch 0/200] [Batch 361/600] [D loss: 0.001531] [G loss: 7.340286]\n",
      "[Epoch 0/200] [Batch 362/600] [D loss: 0.001413] [G loss: 7.315142]\n",
      "[Epoch 0/200] [Batch 363/600] [D loss: 0.001345] [G loss: 7.084249]\n",
      "[Epoch 0/200] [Batch 364/600] [D loss: 0.001210] [G loss: 7.193521]\n",
      "[Epoch 0/200] [Batch 365/600] [D loss: 0.001051] [G loss: 7.477862]\n",
      "[Epoch 0/200] [Batch 366/600] [D loss: 0.001116] [G loss: 7.338713]\n",
      "[Epoch 0/200] [Batch 367/600] [D loss: 0.001157] [G loss: 7.470685]\n",
      "[Epoch 0/200] [Batch 368/600] [D loss: 0.001052] [G loss: 7.281004]\n",
      "[Epoch 0/200] [Batch 369/600] [D loss: 0.001285] [G loss: 7.238807]\n",
      "[Epoch 0/200] [Batch 370/600] [D loss: 0.001368] [G loss: 7.401204]\n",
      "[Epoch 0/200] [Batch 371/600] [D loss: 0.001416] [G loss: 7.500761]\n",
      "[Epoch 0/200] [Batch 372/600] [D loss: 0.000938] [G loss: 7.356736]\n",
      "[Epoch 0/200] [Batch 373/600] [D loss: 0.001102] [G loss: 7.354896]\n",
      "[Epoch 0/200] [Batch 374/600] [D loss: 0.000972] [G loss: 7.529205]\n",
      "[Epoch 0/200] [Batch 375/600] [D loss: 0.001346] [G loss: 7.437930]\n",
      "[Epoch 0/200] [Batch 376/600] [D loss: 0.000975] [G loss: 7.320469]\n",
      "[Epoch 0/200] [Batch 377/600] [D loss: 0.001162] [G loss: 7.504315]\n",
      "[Epoch 0/200] [Batch 378/600] [D loss: 0.001082] [G loss: 7.219375]\n",
      "[Epoch 0/200] [Batch 379/600] [D loss: 0.001747] [G loss: 7.281391]\n",
      "[Epoch 0/200] [Batch 380/600] [D loss: 0.001088] [G loss: 7.179632]\n",
      "[Epoch 0/200] [Batch 381/600] [D loss: 0.001171] [G loss: 7.323148]\n",
      "[Epoch 0/200] [Batch 382/600] [D loss: 0.001050] [G loss: 7.339386]\n",
      "[Epoch 0/200] [Batch 383/600] [D loss: 0.001171] [G loss: 7.358585]\n",
      "[Epoch 0/200] [Batch 384/600] [D loss: 0.001204] [G loss: 7.666541]\n",
      "[Epoch 0/200] [Batch 385/600] [D loss: 0.000962] [G loss: 7.223236]\n",
      "[Epoch 0/200] [Batch 386/600] [D loss: 0.001193] [G loss: 7.320589]\n",
      "[Epoch 0/200] [Batch 387/600] [D loss: 0.000869] [G loss: 7.428744]\n",
      "[Epoch 0/200] [Batch 388/600] [D loss: 0.001158] [G loss: 7.464139]\n",
      "[Epoch 0/200] [Batch 389/600] [D loss: 0.001018] [G loss: 7.497166]\n",
      "[Epoch 0/200] [Batch 390/600] [D loss: 0.001137] [G loss: 7.519970]\n",
      "[Epoch 0/200] [Batch 391/600] [D loss: 0.001095] [G loss: 7.372471]\n",
      "[Epoch 0/200] [Batch 392/600] [D loss: 0.001098] [G loss: 7.471782]\n",
      "[Epoch 0/200] [Batch 393/600] [D loss: 0.001031] [G loss: 7.345393]\n",
      "[Epoch 0/200] [Batch 394/600] [D loss: 0.001095] [G loss: 7.456232]\n",
      "[Epoch 0/200] [Batch 395/600] [D loss: 0.001110] [G loss: 7.483619]\n",
      "[Epoch 0/200] [Batch 396/600] [D loss: 0.001263] [G loss: 7.292990]\n",
      "[Epoch 0/200] [Batch 397/600] [D loss: 0.001324] [G loss: 7.529652]\n",
      "[Epoch 0/200] [Batch 398/600] [D loss: 0.000936] [G loss: 7.654532]\n",
      "[Epoch 0/200] [Batch 399/600] [D loss: 0.001053] [G loss: 7.589163]\n",
      "[Epoch 0/200] [Batch 400/600] [D loss: 0.001282] [G loss: 7.623408]\n",
      "[Epoch 0/200] [Batch 401/600] [D loss: 0.001040] [G loss: 7.591545]\n",
      "[Epoch 0/200] [Batch 402/600] [D loss: 0.001049] [G loss: 7.381361]\n",
      "[Epoch 0/200] [Batch 403/600] [D loss: 0.000945] [G loss: 7.486291]\n",
      "[Epoch 0/200] [Batch 404/600] [D loss: 0.001075] [G loss: 7.572117]\n",
      "[Epoch 0/200] [Batch 405/600] [D loss: 0.000951] [G loss: 7.394597]\n",
      "[Epoch 0/200] [Batch 406/600] [D loss: 0.001042] [G loss: 7.688930]\n",
      "[Epoch 0/200] [Batch 407/600] [D loss: 0.000841] [G loss: 7.388183]\n",
      "[Epoch 0/200] [Batch 408/600] [D loss: 0.000781] [G loss: 7.402412]\n",
      "[Epoch 0/200] [Batch 409/600] [D loss: 0.000783] [G loss: 7.608671]\n",
      "[Epoch 0/200] [Batch 410/600] [D loss: 0.001154] [G loss: 7.701357]\n",
      "[Epoch 0/200] [Batch 411/600] [D loss: 0.000976] [G loss: 7.399729]\n",
      "[Epoch 0/200] [Batch 412/600] [D loss: 0.001045] [G loss: 7.923062]\n",
      "[Epoch 0/200] [Batch 413/600] [D loss: 0.001040] [G loss: 7.534359]\n",
      "[Epoch 0/200] [Batch 414/600] [D loss: 0.001625] [G loss: 7.713622]\n",
      "[Epoch 0/200] [Batch 415/600] [D loss: 0.001001] [G loss: 7.447158]\n",
      "[Epoch 0/200] [Batch 416/600] [D loss: 0.000974] [G loss: 7.594149]\n",
      "[Epoch 0/200] [Batch 417/600] [D loss: 0.001174] [G loss: 7.402642]\n",
      "[Epoch 0/200] [Batch 418/600] [D loss: 0.000827] [G loss: 7.751308]\n",
      "[Epoch 0/200] [Batch 419/600] [D loss: 0.001221] [G loss: 7.500043]\n",
      "[Epoch 0/200] [Batch 420/600] [D loss: 0.000824] [G loss: 7.590827]\n",
      "[Epoch 0/200] [Batch 421/600] [D loss: 0.001127] [G loss: 7.627280]\n",
      "[Epoch 0/200] [Batch 422/600] [D loss: 0.000815] [G loss: 7.738658]\n",
      "[Epoch 0/200] [Batch 423/600] [D loss: 0.000958] [G loss: 7.496073]\n",
      "[Epoch 0/200] [Batch 424/600] [D loss: 0.000804] [G loss: 7.577548]\n",
      "[Epoch 0/200] [Batch 425/600] [D loss: 0.000959] [G loss: 7.626978]\n",
      "[Epoch 0/200] [Batch 426/600] [D loss: 0.000791] [G loss: 7.774092]\n",
      "[Epoch 0/200] [Batch 427/600] [D loss: 0.000855] [G loss: 7.339035]\n",
      "[Epoch 0/200] [Batch 428/600] [D loss: 0.000942] [G loss: 7.511539]\n",
      "[Epoch 0/200] [Batch 429/600] [D loss: 0.000981] [G loss: 7.607718]\n",
      "[Epoch 0/200] [Batch 430/600] [D loss: 0.000775] [G loss: 7.549687]\n",
      "[Epoch 0/200] [Batch 431/600] [D loss: 0.000882] [G loss: 7.731454]\n",
      "[Epoch 0/200] [Batch 432/600] [D loss: 0.000729] [G loss: 7.509377]\n",
      "[Epoch 0/200] [Batch 433/600] [D loss: 0.001124] [G loss: 7.764024]\n",
      "[Epoch 0/200] [Batch 434/600] [D loss: 0.000978] [G loss: 7.573355]\n",
      "[Epoch 0/200] [Batch 435/600] [D loss: 0.000746] [G loss: 7.715692]\n",
      "[Epoch 0/200] [Batch 436/600] [D loss: 0.000936] [G loss: 7.642327]\n",
      "[Epoch 0/200] [Batch 437/600] [D loss: 0.000841] [G loss: 7.761864]\n",
      "[Epoch 0/200] [Batch 438/600] [D loss: 0.000831] [G loss: 7.797315]\n",
      "[Epoch 0/200] [Batch 439/600] [D loss: 0.000841] [G loss: 7.680069]\n",
      "[Epoch 0/200] [Batch 440/600] [D loss: 0.000849] [G loss: 7.845278]\n",
      "[Epoch 0/200] [Batch 441/600] [D loss: 0.000928] [G loss: 7.577357]\n",
      "[Epoch 0/200] [Batch 442/600] [D loss: 0.000972] [G loss: 7.592126]\n",
      "[Epoch 0/200] [Batch 443/600] [D loss: 0.000740] [G loss: 7.391794]\n",
      "[Epoch 0/200] [Batch 444/600] [D loss: 0.000696] [G loss: 7.407601]\n",
      "[Epoch 0/200] [Batch 445/600] [D loss: 0.000819] [G loss: 7.686470]\n",
      "[Epoch 0/200] [Batch 446/600] [D loss: 0.000766] [G loss: 7.951028]\n",
      "[Epoch 0/200] [Batch 447/600] [D loss: 0.000883] [G loss: 7.705016]\n",
      "[Epoch 0/200] [Batch 448/600] [D loss: 0.000999] [G loss: 7.654018]\n",
      "[Epoch 0/200] [Batch 449/600] [D loss: 0.000830] [G loss: 7.761061]\n",
      "[Epoch 0/200] [Batch 450/600] [D loss: 0.000902] [G loss: 7.836090]\n",
      "[Epoch 0/200] [Batch 451/600] [D loss: 0.000883] [G loss: 7.559121]\n",
      "[Epoch 0/200] [Batch 452/600] [D loss: 0.000758] [G loss: 7.820852]\n",
      "[Epoch 0/200] [Batch 453/600] [D loss: 0.000706] [G loss: 7.836793]\n",
      "[Epoch 0/200] [Batch 454/600] [D loss: 0.000816] [G loss: 7.718261]\n",
      "[Epoch 0/200] [Batch 455/600] [D loss: 0.000852] [G loss: 7.696283]\n",
      "[Epoch 0/200] [Batch 456/600] [D loss: 0.000803] [G loss: 7.789418]\n",
      "[Epoch 0/200] [Batch 457/600] [D loss: 0.000777] [G loss: 7.830455]\n",
      "[Epoch 0/200] [Batch 458/600] [D loss: 0.000790] [G loss: 7.874928]\n",
      "[Epoch 0/200] [Batch 459/600] [D loss: 0.000886] [G loss: 7.855197]\n",
      "[Epoch 0/200] [Batch 460/600] [D loss: 0.000703] [G loss: 7.448440]\n",
      "[Epoch 0/200] [Batch 461/600] [D loss: 0.000741] [G loss: 7.842401]\n",
      "[Epoch 0/200] [Batch 462/600] [D loss: 0.000872] [G loss: 7.778019]\n",
      "[Epoch 0/200] [Batch 463/600] [D loss: 0.000848] [G loss: 7.925674]\n",
      "[Epoch 0/200] [Batch 464/600] [D loss: 0.000605] [G loss: 7.935721]\n",
      "[Epoch 0/200] [Batch 465/600] [D loss: 0.000652] [G loss: 7.632246]\n",
      "[Epoch 0/200] [Batch 466/600] [D loss: 0.000787] [G loss: 7.943552]\n",
      "[Epoch 0/200] [Batch 467/600] [D loss: 0.000520] [G loss: 7.815325]\n",
      "[Epoch 0/200] [Batch 468/600] [D loss: 0.000617] [G loss: 7.615310]\n",
      "[Epoch 0/200] [Batch 469/600] [D loss: 0.000601] [G loss: 7.905518]\n",
      "[Epoch 0/200] [Batch 470/600] [D loss: 0.000772] [G loss: 7.885082]\n",
      "[Epoch 0/200] [Batch 471/600] [D loss: 0.000598] [G loss: 7.844281]\n",
      "[Epoch 0/200] [Batch 472/600] [D loss: 0.000740] [G loss: 7.776195]\n",
      "[Epoch 0/200] [Batch 473/600] [D loss: 0.000670] [G loss: 7.757059]\n",
      "[Epoch 0/200] [Batch 474/600] [D loss: 0.000874] [G loss: 7.731328]\n",
      "[Epoch 0/200] [Batch 475/600] [D loss: 0.000996] [G loss: 7.928568]\n",
      "[Epoch 0/200] [Batch 476/600] [D loss: 0.000696] [G loss: 8.006969]\n",
      "[Epoch 0/200] [Batch 477/600] [D loss: 0.000825] [G loss: 7.757117]\n",
      "[Epoch 0/200] [Batch 478/600] [D loss: 0.000629] [G loss: 8.057679]\n",
      "[Epoch 0/200] [Batch 479/600] [D loss: 0.000603] [G loss: 7.889112]\n",
      "[Epoch 0/200] [Batch 480/600] [D loss: 0.000668] [G loss: 7.955690]\n",
      "[Epoch 0/200] [Batch 481/600] [D loss: 0.000556] [G loss: 8.053990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 482/600] [D loss: 0.000598] [G loss: 7.788920]\n",
      "[Epoch 0/200] [Batch 483/600] [D loss: 0.000745] [G loss: 7.594642]\n",
      "[Epoch 0/200] [Batch 484/600] [D loss: 0.000718] [G loss: 7.785437]\n",
      "[Epoch 0/200] [Batch 485/600] [D loss: 0.000811] [G loss: 7.922599]\n",
      "[Epoch 0/200] [Batch 486/600] [D loss: 0.000614] [G loss: 7.997054]\n",
      "[Epoch 0/200] [Batch 487/600] [D loss: 0.000648] [G loss: 7.892864]\n",
      "[Epoch 0/200] [Batch 488/600] [D loss: 0.000578] [G loss: 7.879824]\n",
      "[Epoch 0/200] [Batch 489/600] [D loss: 0.000540] [G loss: 7.772927]\n",
      "[Epoch 0/200] [Batch 490/600] [D loss: 0.000775] [G loss: 8.086200]\n",
      "[Epoch 0/200] [Batch 491/600] [D loss: 0.000602] [G loss: 7.953145]\n",
      "[Epoch 0/200] [Batch 492/600] [D loss: 0.001056] [G loss: 8.093983]\n",
      "[Epoch 0/200] [Batch 493/600] [D loss: 0.000528] [G loss: 8.172709]\n",
      "[Epoch 0/200] [Batch 494/600] [D loss: 0.000737] [G loss: 7.941971]\n",
      "[Epoch 0/200] [Batch 495/600] [D loss: 0.000586] [G loss: 8.243049]\n",
      "[Epoch 0/200] [Batch 496/600] [D loss: 0.000611] [G loss: 7.837531]\n",
      "[Epoch 0/200] [Batch 497/600] [D loss: 0.000792] [G loss: 7.780771]\n",
      "[Epoch 0/200] [Batch 498/600] [D loss: 0.000480] [G loss: 8.083694]\n",
      "[Epoch 0/200] [Batch 499/600] [D loss: 0.000750] [G loss: 7.890150]\n",
      "[Epoch 0/200] [Batch 500/600] [D loss: 0.000595] [G loss: 7.844739]\n",
      "[Epoch 0/200] [Batch 501/600] [D loss: 0.000565] [G loss: 8.082400]\n",
      "[Epoch 0/200] [Batch 502/600] [D loss: 0.000705] [G loss: 7.854193]\n",
      "[Epoch 0/200] [Batch 503/600] [D loss: 0.000507] [G loss: 8.032744]\n",
      "[Epoch 0/200] [Batch 504/600] [D loss: 0.000716] [G loss: 7.813045]\n",
      "[Epoch 0/200] [Batch 505/600] [D loss: 0.000597] [G loss: 7.941615]\n",
      "[Epoch 0/200] [Batch 506/600] [D loss: 0.000597] [G loss: 7.978348]\n",
      "[Epoch 0/200] [Batch 507/600] [D loss: 0.000538] [G loss: 7.864989]\n",
      "[Epoch 0/200] [Batch 508/600] [D loss: 0.000619] [G loss: 8.102830]\n",
      "[Epoch 0/200] [Batch 509/600] [D loss: 0.000589] [G loss: 8.043242]\n",
      "[Epoch 0/200] [Batch 510/600] [D loss: 0.000650] [G loss: 8.159020]\n",
      "[Epoch 0/200] [Batch 511/600] [D loss: 0.000572] [G loss: 8.041470]\n",
      "[Epoch 0/200] [Batch 512/600] [D loss: 0.000659] [G loss: 8.103718]\n",
      "[Epoch 0/200] [Batch 513/600] [D loss: 0.000562] [G loss: 8.026629]\n",
      "[Epoch 0/200] [Batch 514/600] [D loss: 0.000806] [G loss: 8.266519]\n",
      "[Epoch 0/200] [Batch 515/600] [D loss: 0.000856] [G loss: 8.083122]\n",
      "[Epoch 0/200] [Batch 516/600] [D loss: 0.000544] [G loss: 7.938540]\n",
      "[Epoch 0/200] [Batch 517/600] [D loss: 0.000756] [G loss: 7.879523]\n",
      "[Epoch 0/200] [Batch 518/600] [D loss: 0.000703] [G loss: 8.218925]\n",
      "[Epoch 0/200] [Batch 519/600] [D loss: 0.000613] [G loss: 8.127998]\n",
      "[Epoch 0/200] [Batch 520/600] [D loss: 0.000619] [G loss: 8.056705]\n",
      "[Epoch 0/200] [Batch 521/600] [D loss: 0.000564] [G loss: 8.149380]\n",
      "[Epoch 0/200] [Batch 522/600] [D loss: 0.000537] [G loss: 7.773465]\n",
      "[Epoch 0/200] [Batch 523/600] [D loss: 0.000519] [G loss: 8.114666]\n",
      "[Epoch 0/200] [Batch 524/600] [D loss: 0.000607] [G loss: 8.026503]\n",
      "[Epoch 0/200] [Batch 525/600] [D loss: 0.000557] [G loss: 8.201574]\n",
      "[Epoch 0/200] [Batch 526/600] [D loss: 0.000535] [G loss: 8.351900]\n",
      "[Epoch 0/200] [Batch 527/600] [D loss: 0.000537] [G loss: 8.271499]\n",
      "[Epoch 0/200] [Batch 528/600] [D loss: 0.000505] [G loss: 8.240683]\n",
      "[Epoch 0/200] [Batch 529/600] [D loss: 0.000535] [G loss: 8.181929]\n",
      "[Epoch 0/200] [Batch 530/600] [D loss: 0.000608] [G loss: 7.979209]\n",
      "[Epoch 0/200] [Batch 531/600] [D loss: 0.000566] [G loss: 7.961411]\n",
      "[Epoch 0/200] [Batch 532/600] [D loss: 0.000433] [G loss: 8.172502]\n",
      "[Epoch 0/200] [Batch 533/600] [D loss: 0.000553] [G loss: 8.155000]\n",
      "[Epoch 0/200] [Batch 534/600] [D loss: 0.000777] [G loss: 7.942594]\n",
      "[Epoch 0/200] [Batch 535/600] [D loss: 0.000454] [G loss: 8.108346]\n",
      "[Epoch 0/200] [Batch 536/600] [D loss: 0.000598] [G loss: 8.035748]\n",
      "[Epoch 0/200] [Batch 537/600] [D loss: 0.000656] [G loss: 7.931725]\n",
      "[Epoch 0/200] [Batch 538/600] [D loss: 0.000571] [G loss: 8.031019]\n",
      "[Epoch 0/200] [Batch 539/600] [D loss: 0.000574] [G loss: 8.245199]\n",
      "[Epoch 0/200] [Batch 540/600] [D loss: 0.000592] [G loss: 8.188525]\n",
      "[Epoch 0/200] [Batch 541/600] [D loss: 0.000459] [G loss: 8.107726]\n",
      "[Epoch 0/200] [Batch 542/600] [D loss: 0.000579] [G loss: 8.258213]\n",
      "[Epoch 0/200] [Batch 543/600] [D loss: 0.000512] [G loss: 8.208819]\n",
      "[Epoch 0/200] [Batch 544/600] [D loss: 0.000516] [G loss: 8.174371]\n",
      "[Epoch 0/200] [Batch 545/600] [D loss: 0.000568] [G loss: 8.051010]\n",
      "[Epoch 0/200] [Batch 546/600] [D loss: 0.000546] [G loss: 8.207011]\n",
      "[Epoch 0/200] [Batch 547/600] [D loss: 0.000593] [G loss: 8.264229]\n",
      "[Epoch 0/200] [Batch 548/600] [D loss: 0.000668] [G loss: 8.091742]\n",
      "[Epoch 0/200] [Batch 549/600] [D loss: 0.000465] [G loss: 8.382993]\n",
      "[Epoch 0/200] [Batch 550/600] [D loss: 0.000435] [G loss: 8.140263]\n",
      "[Epoch 0/200] [Batch 551/600] [D loss: 0.000669] [G loss: 7.937155]\n",
      "[Epoch 0/200] [Batch 552/600] [D loss: 0.000621] [G loss: 7.991933]\n",
      "[Epoch 0/200] [Batch 553/600] [D loss: 0.000451] [G loss: 8.043046]\n",
      "[Epoch 0/200] [Batch 554/600] [D loss: 0.000472] [G loss: 8.117646]\n",
      "[Epoch 0/200] [Batch 555/600] [D loss: 0.000639] [G loss: 8.076815]\n",
      "[Epoch 0/200] [Batch 556/600] [D loss: 0.000521] [G loss: 8.095704]\n",
      "[Epoch 0/200] [Batch 557/600] [D loss: 0.000664] [G loss: 8.147762]\n",
      "[Epoch 0/200] [Batch 558/600] [D loss: 0.000667] [G loss: 8.546771]\n",
      "[Epoch 0/200] [Batch 559/600] [D loss: 0.000594] [G loss: 8.107638]\n",
      "[Epoch 0/200] [Batch 560/600] [D loss: 0.000405] [G loss: 8.320591]\n",
      "[Epoch 0/200] [Batch 561/600] [D loss: 0.000456] [G loss: 8.296540]\n",
      "[Epoch 0/200] [Batch 562/600] [D loss: 0.000547] [G loss: 8.110336]\n",
      "[Epoch 0/200] [Batch 563/600] [D loss: 0.000579] [G loss: 8.180275]\n",
      "[Epoch 0/200] [Batch 564/600] [D loss: 0.000393] [G loss: 8.125490]\n",
      "[Epoch 0/200] [Batch 565/600] [D loss: 0.000569] [G loss: 8.343102]\n",
      "[Epoch 0/200] [Batch 566/600] [D loss: 0.000509] [G loss: 8.180338]\n",
      "[Epoch 0/200] [Batch 567/600] [D loss: 0.000498] [G loss: 8.051257]\n",
      "[Epoch 0/200] [Batch 568/600] [D loss: 0.000548] [G loss: 8.458421]\n",
      "[Epoch 0/200] [Batch 569/600] [D loss: 0.000712] [G loss: 8.143044]\n",
      "[Epoch 0/200] [Batch 570/600] [D loss: 0.000531] [G loss: 8.534052]\n",
      "[Epoch 0/200] [Batch 571/600] [D loss: 0.000470] [G loss: 8.303282]\n",
      "[Epoch 0/200] [Batch 572/600] [D loss: 0.000436] [G loss: 8.084882]\n",
      "[Epoch 0/200] [Batch 573/600] [D loss: 0.000388] [G loss: 8.194147]\n",
      "[Epoch 0/200] [Batch 574/600] [D loss: 0.000364] [G loss: 8.572437]\n",
      "[Epoch 0/200] [Batch 575/600] [D loss: 0.000436] [G loss: 8.260455]\n",
      "[Epoch 0/200] [Batch 576/600] [D loss: 0.000534] [G loss: 8.121381]\n",
      "[Epoch 0/200] [Batch 577/600] [D loss: 0.000387] [G loss: 8.313042]\n",
      "[Epoch 0/200] [Batch 578/600] [D loss: 0.000567] [G loss: 8.499543]\n",
      "[Epoch 0/200] [Batch 579/600] [D loss: 0.000544] [G loss: 8.549123]\n",
      "[Epoch 0/200] [Batch 580/600] [D loss: 0.000403] [G loss: 8.356342]\n",
      "[Epoch 0/200] [Batch 581/600] [D loss: 0.000454] [G loss: 8.446925]\n",
      "[Epoch 0/200] [Batch 582/600] [D loss: 0.000546] [G loss: 8.381333]\n",
      "[Epoch 0/200] [Batch 583/600] [D loss: 0.000378] [G loss: 8.357868]\n",
      "[Epoch 0/200] [Batch 584/600] [D loss: 0.000516] [G loss: 8.236579]\n",
      "[Epoch 0/200] [Batch 585/600] [D loss: 0.000455] [G loss: 8.233954]\n",
      "[Epoch 0/200] [Batch 586/600] [D loss: 0.000370] [G loss: 8.321400]\n",
      "[Epoch 0/200] [Batch 587/600] [D loss: 0.000576] [G loss: 8.117311]\n",
      "[Epoch 0/200] [Batch 588/600] [D loss: 0.000676] [G loss: 8.235492]\n",
      "[Epoch 0/200] [Batch 589/600] [D loss: 0.000420] [G loss: 8.190189]\n",
      "[Epoch 0/200] [Batch 590/600] [D loss: 0.000525] [G loss: 8.349314]\n",
      "[Epoch 0/200] [Batch 591/600] [D loss: 0.000382] [G loss: 8.300158]\n",
      "[Epoch 0/200] [Batch 592/600] [D loss: 0.000434] [G loss: 8.658773]\n",
      "[Epoch 0/200] [Batch 593/600] [D loss: 0.000318] [G loss: 8.370574]\n",
      "[Epoch 0/200] [Batch 594/600] [D loss: 0.000383] [G loss: 8.306439]\n",
      "[Epoch 0/200] [Batch 595/600] [D loss: 0.000479] [G loss: 8.321210]\n",
      "[Epoch 0/200] [Batch 596/600] [D loss: 0.000569] [G loss: 8.356797]\n",
      "[Epoch 0/200] [Batch 597/600] [D loss: 0.000429] [G loss: 8.444577]\n",
      "[Epoch 0/200] [Batch 598/600] [D loss: 0.000492] [G loss: 8.403825]\n",
      "[Epoch 0/200] [Batch 599/600] [D loss: 0.000554] [G loss: 8.363370]\n",
      "[Epoch 1/200] [Batch 0/600] [D loss: 0.000477] [G loss: 8.241315]\n",
      "[Epoch 1/200] [Batch 1/600] [D loss: 0.000356] [G loss: 8.380015]\n",
      "[Epoch 1/200] [Batch 2/600] [D loss: 0.000403] [G loss: 8.355804]\n",
      "[Epoch 1/200] [Batch 3/600] [D loss: 0.000467] [G loss: 8.336672]\n",
      "[Epoch 1/200] [Batch 4/600] [D loss: 0.000519] [G loss: 8.487932]\n",
      "[Epoch 1/200] [Batch 5/600] [D loss: 0.000384] [G loss: 8.649987]\n",
      "[Epoch 1/200] [Batch 6/600] [D loss: 0.000535] [G loss: 8.191413]\n",
      "[Epoch 1/200] [Batch 7/600] [D loss: 0.000382] [G loss: 8.443852]\n",
      "[Epoch 1/200] [Batch 8/600] [D loss: 0.000424] [G loss: 8.567081]\n",
      "[Epoch 1/200] [Batch 9/600] [D loss: 0.000452] [G loss: 8.487932]\n",
      "[Epoch 1/200] [Batch 10/600] [D loss: 0.000552] [G loss: 8.294810]\n",
      "[Epoch 1/200] [Batch 11/600] [D loss: 0.000405] [G loss: 8.491304]\n",
      "[Epoch 1/200] [Batch 12/600] [D loss: 0.000427] [G loss: 8.863287]\n",
      "[Epoch 1/200] [Batch 13/600] [D loss: 0.000383] [G loss: 8.459431]\n",
      "[Epoch 1/200] [Batch 14/600] [D loss: 0.000409] [G loss: 8.717350]\n",
      "[Epoch 1/200] [Batch 15/600] [D loss: 0.000436] [G loss: 8.429185]\n",
      "[Epoch 1/200] [Batch 16/600] [D loss: 0.000464] [G loss: 8.464696]\n",
      "[Epoch 1/200] [Batch 17/600] [D loss: 0.000411] [G loss: 8.411127]\n",
      "[Epoch 1/200] [Batch 18/600] [D loss: 0.000512] [G loss: 8.541986]\n",
      "[Epoch 1/200] [Batch 19/600] [D loss: 0.000516] [G loss: 8.449131]\n",
      "[Epoch 1/200] [Batch 20/600] [D loss: 0.000369] [G loss: 8.608059]\n",
      "[Epoch 1/200] [Batch 21/600] [D loss: 0.000341] [G loss: 8.606345]\n",
      "[Epoch 1/200] [Batch 22/600] [D loss: 0.000368] [G loss: 8.470333]\n",
      "[Epoch 1/200] [Batch 23/600] [D loss: 0.000494] [G loss: 8.388910]\n",
      "[Epoch 1/200] [Batch 24/600] [D loss: 0.000399] [G loss: 8.678417]\n",
      "[Epoch 1/200] [Batch 25/600] [D loss: 0.000403] [G loss: 8.494665]\n",
      "[Epoch 1/200] [Batch 26/600] [D loss: 0.000462] [G loss: 8.611324]\n",
      "[Epoch 1/200] [Batch 27/600] [D loss: 0.000306] [G loss: 8.422203]\n",
      "[Epoch 1/200] [Batch 28/600] [D loss: 0.000315] [G loss: 8.549480]\n",
      "[Epoch 1/200] [Batch 29/600] [D loss: 0.000478] [G loss: 8.517396]\n",
      "[Epoch 1/200] [Batch 30/600] [D loss: 0.000349] [G loss: 8.262641]\n",
      "[Epoch 1/200] [Batch 31/600] [D loss: 0.000446] [G loss: 8.778086]\n",
      "[Epoch 1/200] [Batch 32/600] [D loss: 0.000409] [G loss: 8.574826]\n",
      "[Epoch 1/200] [Batch 33/600] [D loss: 0.000535] [G loss: 8.568173]\n",
      "[Epoch 1/200] [Batch 34/600] [D loss: 0.000393] [G loss: 8.633489]\n",
      "[Epoch 1/200] [Batch 35/600] [D loss: 0.000388] [G loss: 8.436095]\n",
      "[Epoch 1/200] [Batch 36/600] [D loss: 0.000405] [G loss: 8.566387]\n",
      "[Epoch 1/200] [Batch 37/600] [D loss: 0.000394] [G loss: 8.620187]\n",
      "[Epoch 1/200] [Batch 38/600] [D loss: 0.000394] [G loss: 8.476032]\n",
      "[Epoch 1/200] [Batch 39/600] [D loss: 0.000369] [G loss: 8.495140]\n",
      "[Epoch 1/200] [Batch 40/600] [D loss: 0.000472] [G loss: 8.558101]\n",
      "[Epoch 1/200] [Batch 41/600] [D loss: 0.000421] [G loss: 8.680831]\n",
      "[Epoch 1/200] [Batch 42/600] [D loss: 0.000400] [G loss: 8.475986]\n",
      "[Epoch 1/200] [Batch 43/600] [D loss: 0.000373] [G loss: 8.534334]\n",
      "[Epoch 1/200] [Batch 44/600] [D loss: 0.000343] [G loss: 8.548919]\n",
      "[Epoch 1/200] [Batch 45/600] [D loss: 0.000381] [G loss: 8.829899]\n",
      "[Epoch 1/200] [Batch 46/600] [D loss: 0.000355] [G loss: 8.532043]\n",
      "[Epoch 1/200] [Batch 47/600] [D loss: 0.000343] [G loss: 8.753085]\n",
      "[Epoch 1/200] [Batch 48/600] [D loss: 0.000383] [G loss: 8.610403]\n",
      "[Epoch 1/200] [Batch 49/600] [D loss: 0.000418] [G loss: 8.427426]\n",
      "[Epoch 1/200] [Batch 50/600] [D loss: 0.000349] [G loss: 8.426530]\n",
      "[Epoch 1/200] [Batch 51/600] [D loss: 0.000369] [G loss: 8.436695]\n",
      "[Epoch 1/200] [Batch 52/600] [D loss: 0.000562] [G loss: 8.763854]\n",
      "[Epoch 1/200] [Batch 53/600] [D loss: 0.000294] [G loss: 8.389245]\n",
      "[Epoch 1/200] [Batch 54/600] [D loss: 0.000426] [G loss: 8.443960]\n",
      "[Epoch 1/200] [Batch 55/600] [D loss: 0.000359] [G loss: 8.437653]\n",
      "[Epoch 1/200] [Batch 56/600] [D loss: 0.000378] [G loss: 8.586917]\n",
      "[Epoch 1/200] [Batch 57/600] [D loss: 0.000331] [G loss: 8.483974]\n",
      "[Epoch 1/200] [Batch 58/600] [D loss: 0.000584] [G loss: 8.660883]\n",
      "[Epoch 1/200] [Batch 59/600] [D loss: 0.000411] [G loss: 8.412622]\n",
      "[Epoch 1/200] [Batch 60/600] [D loss: 0.000399] [G loss: 8.382572]\n",
      "[Epoch 1/200] [Batch 61/600] [D loss: 0.000386] [G loss: 8.790203]\n",
      "[Epoch 1/200] [Batch 62/600] [D loss: 0.000349] [G loss: 8.646856]\n",
      "[Epoch 1/200] [Batch 63/600] [D loss: 0.000276] [G loss: 8.560639]\n",
      "[Epoch 1/200] [Batch 64/600] [D loss: 0.000348] [G loss: 8.687236]\n",
      "[Epoch 1/200] [Batch 65/600] [D loss: 0.000420] [G loss: 8.582668]\n",
      "[Epoch 1/200] [Batch 66/600] [D loss: 0.000342] [G loss: 8.696064]\n",
      "[Epoch 1/200] [Batch 67/600] [D loss: 0.000269] [G loss: 8.632138]\n",
      "[Epoch 1/200] [Batch 68/600] [D loss: 0.000314] [G loss: 8.410892]\n",
      "[Epoch 1/200] [Batch 69/600] [D loss: 0.000290] [G loss: 8.613239]\n",
      "[Epoch 1/200] [Batch 70/600] [D loss: 0.000265] [G loss: 8.627781]\n",
      "[Epoch 1/200] [Batch 71/600] [D loss: 0.000422] [G loss: 8.688920]\n",
      "[Epoch 1/200] [Batch 72/600] [D loss: 0.000347] [G loss: 8.363183]\n",
      "[Epoch 1/200] [Batch 73/600] [D loss: 0.000365] [G loss: 8.644059]\n",
      "[Epoch 1/200] [Batch 74/600] [D loss: 0.000276] [G loss: 8.553593]\n",
      "[Epoch 1/200] [Batch 75/600] [D loss: 0.000384] [G loss: 8.683086]\n",
      "[Epoch 1/200] [Batch 76/600] [D loss: 0.000434] [G loss: 8.865156]\n",
      "[Epoch 1/200] [Batch 77/600] [D loss: 0.000340] [G loss: 8.503244]\n",
      "[Epoch 1/200] [Batch 78/600] [D loss: 0.000335] [G loss: 8.436430]\n",
      "[Epoch 1/200] [Batch 79/600] [D loss: 0.000388] [G loss: 8.610470]\n",
      "[Epoch 1/200] [Batch 80/600] [D loss: 0.000279] [G loss: 8.516750]\n",
      "[Epoch 1/200] [Batch 81/600] [D loss: 0.000542] [G loss: 8.662975]\n",
      "[Epoch 1/200] [Batch 82/600] [D loss: 0.000344] [G loss: 8.645143]\n",
      "[Epoch 1/200] [Batch 83/600] [D loss: 0.000338] [G loss: 8.646270]\n",
      "[Epoch 1/200] [Batch 84/600] [D loss: 0.000307] [G loss: 8.609437]\n",
      "[Epoch 1/200] [Batch 85/600] [D loss: 0.000275] [G loss: 8.900093]\n",
      "[Epoch 1/200] [Batch 86/600] [D loss: 0.000334] [G loss: 8.786522]\n",
      "[Epoch 1/200] [Batch 87/600] [D loss: 0.000315] [G loss: 8.831098]\n",
      "[Epoch 1/200] [Batch 88/600] [D loss: 0.000371] [G loss: 8.804375]\n",
      "[Epoch 1/200] [Batch 89/600] [D loss: 0.000319] [G loss: 8.521928]\n",
      "[Epoch 1/200] [Batch 90/600] [D loss: 0.000314] [G loss: 8.801933]\n",
      "[Epoch 1/200] [Batch 91/600] [D loss: 0.000373] [G loss: 8.836615]\n",
      "[Epoch 1/200] [Batch 92/600] [D loss: 0.000364] [G loss: 8.556238]\n",
      "[Epoch 1/200] [Batch 93/600] [D loss: 0.000277] [G loss: 8.596024]\n",
      "[Epoch 1/200] [Batch 94/600] [D loss: 0.000351] [G loss: 8.553840]\n",
      "[Epoch 1/200] [Batch 95/600] [D loss: 0.000276] [G loss: 8.893122]\n",
      "[Epoch 1/200] [Batch 96/600] [D loss: 0.000278] [G loss: 8.718802]\n",
      "[Epoch 1/200] [Batch 97/600] [D loss: 0.000343] [G loss: 8.781275]\n",
      "[Epoch 1/200] [Batch 98/600] [D loss: 0.000356] [G loss: 8.815319]\n",
      "[Epoch 1/200] [Batch 99/600] [D loss: 0.000405] [G loss: 8.869616]\n",
      "[Epoch 1/200] [Batch 100/600] [D loss: 0.000330] [G loss: 8.776260]\n",
      "[Epoch 1/200] [Batch 101/600] [D loss: 0.000378] [G loss: 8.538607]\n",
      "[Epoch 1/200] [Batch 102/600] [D loss: 0.000317] [G loss: 8.874101]\n",
      "[Epoch 1/200] [Batch 103/600] [D loss: 0.000326] [G loss: 8.914152]\n",
      "[Epoch 1/200] [Batch 104/600] [D loss: 0.000265] [G loss: 8.759122]\n",
      "[Epoch 1/200] [Batch 105/600] [D loss: 0.000310] [G loss: 8.646101]\n",
      "[Epoch 1/200] [Batch 106/600] [D loss: 0.000294] [G loss: 8.640985]\n",
      "[Epoch 1/200] [Batch 107/600] [D loss: 0.000336] [G loss: 8.318216]\n",
      "[Epoch 1/200] [Batch 108/600] [D loss: 0.000348] [G loss: 8.422528]\n",
      "[Epoch 1/200] [Batch 109/600] [D loss: 0.000326] [G loss: 8.730834]\n",
      "[Epoch 1/200] [Batch 110/600] [D loss: 0.000253] [G loss: 8.529044]\n",
      "[Epoch 1/200] [Batch 111/600] [D loss: 0.000317] [G loss: 8.488814]\n",
      "[Epoch 1/200] [Batch 112/600] [D loss: 0.000317] [G loss: 8.815716]\n",
      "[Epoch 1/200] [Batch 113/600] [D loss: 0.000405] [G loss: 8.611329]\n",
      "[Epoch 1/200] [Batch 114/600] [D loss: 0.000273] [G loss: 8.762342]\n",
      "[Epoch 1/200] [Batch 115/600] [D loss: 0.000433] [G loss: 8.719741]\n",
      "[Epoch 1/200] [Batch 116/600] [D loss: 0.000357] [G loss: 8.585577]\n",
      "[Epoch 1/200] [Batch 117/600] [D loss: 0.000319] [G loss: 8.763524]\n",
      "[Epoch 1/200] [Batch 118/600] [D loss: 0.000241] [G loss: 8.644082]\n",
      "[Epoch 1/200] [Batch 119/600] [D loss: 0.000317] [G loss: 8.400207]\n",
      "[Epoch 1/200] [Batch 120/600] [D loss: 0.000355] [G loss: 8.767008]\n",
      "[Epoch 1/200] [Batch 121/600] [D loss: 0.000302] [G loss: 9.059467]\n",
      "[Epoch 1/200] [Batch 122/600] [D loss: 0.000362] [G loss: 8.854342]\n",
      "[Epoch 1/200] [Batch 123/600] [D loss: 0.000305] [G loss: 8.770971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 124/600] [D loss: 0.000283] [G loss: 8.422001]\n",
      "[Epoch 1/200] [Batch 125/600] [D loss: 0.000291] [G loss: 8.666947]\n",
      "[Epoch 1/200] [Batch 126/600] [D loss: 0.000260] [G loss: 8.711805]\n",
      "[Epoch 1/200] [Batch 127/600] [D loss: 0.000282] [G loss: 8.763564]\n",
      "[Epoch 1/200] [Batch 128/600] [D loss: 0.000323] [G loss: 8.957349]\n",
      "[Epoch 1/200] [Batch 129/600] [D loss: 0.000244] [G loss: 8.723634]\n",
      "[Epoch 1/200] [Batch 130/600] [D loss: 0.000249] [G loss: 8.753546]\n",
      "[Epoch 1/200] [Batch 131/600] [D loss: 0.000338] [G loss: 8.498034]\n",
      "[Epoch 1/200] [Batch 132/600] [D loss: 0.000387] [G loss: 8.780226]\n",
      "[Epoch 1/200] [Batch 133/600] [D loss: 0.000355] [G loss: 8.690950]\n",
      "[Epoch 1/200] [Batch 134/600] [D loss: 0.000250] [G loss: 8.713392]\n",
      "[Epoch 1/200] [Batch 135/600] [D loss: 0.000337] [G loss: 8.937205]\n",
      "[Epoch 1/200] [Batch 136/600] [D loss: 0.000254] [G loss: 8.613260]\n",
      "[Epoch 1/200] [Batch 137/600] [D loss: 0.000259] [G loss: 8.693749]\n",
      "[Epoch 1/200] [Batch 138/600] [D loss: 0.000236] [G loss: 8.452335]\n",
      "[Epoch 1/200] [Batch 139/600] [D loss: 0.000301] [G loss: 8.905377]\n",
      "[Epoch 1/200] [Batch 140/600] [D loss: 0.000190] [G loss: 8.939585]\n",
      "[Epoch 1/200] [Batch 141/600] [D loss: 0.000331] [G loss: 8.670588]\n",
      "[Epoch 1/200] [Batch 142/600] [D loss: 0.000318] [G loss: 8.951212]\n",
      "[Epoch 1/200] [Batch 143/600] [D loss: 0.000408] [G loss: 8.641222]\n",
      "[Epoch 1/200] [Batch 144/600] [D loss: 0.000221] [G loss: 8.863879]\n",
      "[Epoch 1/200] [Batch 145/600] [D loss: 0.000294] [G loss: 8.548200]\n",
      "[Epoch 1/200] [Batch 146/600] [D loss: 0.000448] [G loss: 8.906587]\n",
      "[Epoch 1/200] [Batch 147/600] [D loss: 0.000270] [G loss: 8.941913]\n",
      "[Epoch 1/200] [Batch 148/600] [D loss: 0.000404] [G loss: 8.841297]\n",
      "[Epoch 1/200] [Batch 149/600] [D loss: 0.000304] [G loss: 8.624921]\n",
      "[Epoch 1/200] [Batch 150/600] [D loss: 0.000304] [G loss: 8.674519]\n",
      "[Epoch 1/200] [Batch 151/600] [D loss: 0.000263] [G loss: 8.992103]\n",
      "[Epoch 1/200] [Batch 152/600] [D loss: 0.000284] [G loss: 8.473480]\n",
      "[Epoch 1/200] [Batch 153/600] [D loss: 0.000436] [G loss: 8.732242]\n",
      "[Epoch 1/200] [Batch 154/600] [D loss: 0.000267] [G loss: 8.733827]\n",
      "[Epoch 1/200] [Batch 155/600] [D loss: 0.000278] [G loss: 8.530010]\n",
      "[Epoch 1/200] [Batch 156/600] [D loss: 0.000255] [G loss: 8.764071]\n",
      "[Epoch 1/200] [Batch 157/600] [D loss: 0.000337] [G loss: 8.818655]\n",
      "[Epoch 1/200] [Batch 158/600] [D loss: 0.000271] [G loss: 8.799021]\n",
      "[Epoch 1/200] [Batch 159/600] [D loss: 0.000266] [G loss: 8.708893]\n",
      "[Epoch 1/200] [Batch 160/600] [D loss: 0.000242] [G loss: 8.619296]\n",
      "[Epoch 1/200] [Batch 161/600] [D loss: 0.000288] [G loss: 8.955805]\n",
      "[Epoch 1/200] [Batch 162/600] [D loss: 0.000248] [G loss: 8.727160]\n",
      "[Epoch 1/200] [Batch 163/600] [D loss: 0.000280] [G loss: 8.687109]\n",
      "[Epoch 1/200] [Batch 164/600] [D loss: 0.000282] [G loss: 8.493623]\n",
      "[Epoch 1/200] [Batch 165/600] [D loss: 0.000327] [G loss: 8.839982]\n",
      "[Epoch 1/200] [Batch 166/600] [D loss: 0.000313] [G loss: 8.731631]\n",
      "[Epoch 1/200] [Batch 167/600] [D loss: 0.000217] [G loss: 8.819485]\n",
      "[Epoch 1/200] [Batch 168/600] [D loss: 0.000294] [G loss: 8.638613]\n",
      "[Epoch 1/200] [Batch 169/600] [D loss: 0.000216] [G loss: 8.792980]\n",
      "[Epoch 1/200] [Batch 170/600] [D loss: 0.000278] [G loss: 8.680432]\n",
      "[Epoch 1/200] [Batch 171/600] [D loss: 0.000250] [G loss: 8.761007]\n",
      "[Epoch 1/200] [Batch 172/600] [D loss: 0.000308] [G loss: 8.946974]\n",
      "[Epoch 1/200] [Batch 173/600] [D loss: 0.000238] [G loss: 8.880972]\n",
      "[Epoch 1/200] [Batch 174/600] [D loss: 0.000236] [G loss: 8.846082]\n",
      "[Epoch 1/200] [Batch 175/600] [D loss: 0.000269] [G loss: 8.767991]\n",
      "[Epoch 1/200] [Batch 176/600] [D loss: 0.000251] [G loss: 8.848952]\n",
      "[Epoch 1/200] [Batch 177/600] [D loss: 0.000267] [G loss: 8.761706]\n",
      "[Epoch 1/200] [Batch 178/600] [D loss: 0.000342] [G loss: 8.784571]\n",
      "[Epoch 1/200] [Batch 179/600] [D loss: 0.000233] [G loss: 8.902423]\n",
      "[Epoch 1/200] [Batch 180/600] [D loss: 0.000253] [G loss: 8.832600]\n",
      "[Epoch 1/200] [Batch 181/600] [D loss: 0.000388] [G loss: 9.126437]\n",
      "[Epoch 1/200] [Batch 182/600] [D loss: 0.000364] [G loss: 9.035400]\n",
      "[Epoch 1/200] [Batch 183/600] [D loss: 0.000247] [G loss: 8.985157]\n",
      "[Epoch 1/200] [Batch 184/600] [D loss: 0.000270] [G loss: 8.693919]\n",
      "[Epoch 1/200] [Batch 185/600] [D loss: 0.000296] [G loss: 8.982453]\n",
      "[Epoch 1/200] [Batch 186/600] [D loss: 0.000313] [G loss: 8.886726]\n",
      "[Epoch 1/200] [Batch 187/600] [D loss: 0.000245] [G loss: 9.073864]\n",
      "[Epoch 1/200] [Batch 188/600] [D loss: 0.000330] [G loss: 8.850576]\n",
      "[Epoch 1/200] [Batch 189/600] [D loss: 0.000207] [G loss: 8.731359]\n",
      "[Epoch 1/200] [Batch 190/600] [D loss: 0.000248] [G loss: 8.888961]\n",
      "[Epoch 1/200] [Batch 191/600] [D loss: 0.000261] [G loss: 9.020660]\n",
      "[Epoch 1/200] [Batch 192/600] [D loss: 0.000315] [G loss: 9.071584]\n",
      "[Epoch 1/200] [Batch 193/600] [D loss: 0.000249] [G loss: 8.783747]\n",
      "[Epoch 1/200] [Batch 194/600] [D loss: 0.000244] [G loss: 8.682831]\n",
      "[Epoch 1/200] [Batch 195/600] [D loss: 0.000227] [G loss: 8.817092]\n",
      "[Epoch 1/200] [Batch 196/600] [D loss: 0.000248] [G loss: 8.969262]\n",
      "[Epoch 1/200] [Batch 197/600] [D loss: 0.000256] [G loss: 8.638276]\n",
      "[Epoch 1/200] [Batch 198/600] [D loss: 0.000257] [G loss: 9.050922]\n",
      "[Epoch 1/200] [Batch 199/600] [D loss: 0.000194] [G loss: 9.039330]\n",
      "[Epoch 1/200] [Batch 200/600] [D loss: 0.000216] [G loss: 8.900441]\n",
      "[Epoch 1/200] [Batch 201/600] [D loss: 0.000278] [G loss: 9.109101]\n",
      "[Epoch 1/200] [Batch 202/600] [D loss: 0.000208] [G loss: 8.979845]\n",
      "[Epoch 1/200] [Batch 203/600] [D loss: 0.000263] [G loss: 9.032498]\n",
      "[Epoch 1/200] [Batch 204/600] [D loss: 0.000190] [G loss: 9.002556]\n",
      "[Epoch 1/200] [Batch 205/600] [D loss: 0.000280] [G loss: 8.878309]\n",
      "[Epoch 1/200] [Batch 206/600] [D loss: 0.000242] [G loss: 8.931679]\n",
      "[Epoch 1/200] [Batch 207/600] [D loss: 0.000249] [G loss: 8.962790]\n",
      "[Epoch 1/200] [Batch 208/600] [D loss: 0.000219] [G loss: 9.137467]\n",
      "[Epoch 1/200] [Batch 209/600] [D loss: 0.000228] [G loss: 9.369402]\n",
      "[Epoch 1/200] [Batch 210/600] [D loss: 0.000171] [G loss: 8.995618]\n",
      "[Epoch 1/200] [Batch 211/600] [D loss: 0.000232] [G loss: 9.270190]\n",
      "[Epoch 1/200] [Batch 212/600] [D loss: 0.000251] [G loss: 8.970365]\n",
      "[Epoch 1/200] [Batch 213/600] [D loss: 0.000256] [G loss: 9.207206]\n",
      "[Epoch 1/200] [Batch 214/600] [D loss: 0.000242] [G loss: 8.945313]\n",
      "[Epoch 1/200] [Batch 215/600] [D loss: 0.000232] [G loss: 8.940831]\n",
      "[Epoch 1/200] [Batch 216/600] [D loss: 0.000304] [G loss: 9.030662]\n",
      "[Epoch 1/200] [Batch 217/600] [D loss: 0.000284] [G loss: 9.082736]\n",
      "[Epoch 1/200] [Batch 218/600] [D loss: 0.000236] [G loss: 8.804368]\n",
      "[Epoch 1/200] [Batch 219/600] [D loss: 0.000223] [G loss: 9.082396]\n",
      "[Epoch 1/200] [Batch 220/600] [D loss: 0.000238] [G loss: 8.736373]\n",
      "[Epoch 1/200] [Batch 221/600] [D loss: 0.000225] [G loss: 9.023860]\n",
      "[Epoch 1/200] [Batch 222/600] [D loss: 0.000238] [G loss: 8.842251]\n",
      "[Epoch 1/200] [Batch 223/600] [D loss: 0.000329] [G loss: 9.122541]\n",
      "[Epoch 1/200] [Batch 224/600] [D loss: 0.000249] [G loss: 9.089807]\n",
      "[Epoch 1/200] [Batch 225/600] [D loss: 0.000252] [G loss: 9.236634]\n",
      "[Epoch 1/200] [Batch 226/600] [D loss: 0.000289] [G loss: 9.299766]\n",
      "[Epoch 1/200] [Batch 227/600] [D loss: 0.000219] [G loss: 9.105001]\n",
      "[Epoch 1/200] [Batch 228/600] [D loss: 0.000291] [G loss: 9.047916]\n",
      "[Epoch 1/200] [Batch 229/600] [D loss: 0.000239] [G loss: 9.088377]\n",
      "[Epoch 1/200] [Batch 230/600] [D loss: 0.000333] [G loss: 8.899753]\n",
      "[Epoch 1/200] [Batch 231/600] [D loss: 0.000275] [G loss: 8.875418]\n",
      "[Epoch 1/200] [Batch 232/600] [D loss: 0.000245] [G loss: 9.053144]\n",
      "[Epoch 1/200] [Batch 233/600] [D loss: 0.000307] [G loss: 8.999575]\n",
      "[Epoch 1/200] [Batch 234/600] [D loss: 0.000191] [G loss: 9.278756]\n",
      "[Epoch 1/200] [Batch 235/600] [D loss: 0.000291] [G loss: 9.149647]\n",
      "[Epoch 1/200] [Batch 236/600] [D loss: 0.000197] [G loss: 9.150371]\n",
      "[Epoch 1/200] [Batch 237/600] [D loss: 0.000273] [G loss: 8.942229]\n",
      "[Epoch 1/200] [Batch 238/600] [D loss: 0.000241] [G loss: 8.933002]\n",
      "[Epoch 1/200] [Batch 239/600] [D loss: 0.000283] [G loss: 9.044537]\n",
      "[Epoch 1/200] [Batch 240/600] [D loss: 0.000191] [G loss: 9.162937]\n",
      "[Epoch 1/200] [Batch 241/600] [D loss: 0.000211] [G loss: 8.972860]\n",
      "[Epoch 1/200] [Batch 242/600] [D loss: 0.000300] [G loss: 9.258018]\n",
      "[Epoch 1/200] [Batch 243/600] [D loss: 0.000207] [G loss: 9.042751]\n",
      "[Epoch 1/200] [Batch 244/600] [D loss: 0.000238] [G loss: 9.097810]\n",
      "[Epoch 1/200] [Batch 245/600] [D loss: 0.000180] [G loss: 9.065725]\n",
      "[Epoch 1/200] [Batch 246/600] [D loss: 0.000224] [G loss: 9.163878]\n",
      "[Epoch 1/200] [Batch 247/600] [D loss: 0.000223] [G loss: 9.021933]\n",
      "[Epoch 1/200] [Batch 248/600] [D loss: 0.000321] [G loss: 9.066735]\n",
      "[Epoch 1/200] [Batch 249/600] [D loss: 0.000218] [G loss: 8.993239]\n",
      "[Epoch 1/200] [Batch 250/600] [D loss: 0.000245] [G loss: 9.045287]\n",
      "[Epoch 1/200] [Batch 251/600] [D loss: 0.000253] [G loss: 9.477096]\n",
      "[Epoch 1/200] [Batch 252/600] [D loss: 0.000178] [G loss: 8.902780]\n",
      "[Epoch 1/200] [Batch 253/600] [D loss: 0.000320] [G loss: 8.967316]\n",
      "[Epoch 1/200] [Batch 254/600] [D loss: 0.000204] [G loss: 9.111383]\n",
      "[Epoch 1/200] [Batch 255/600] [D loss: 0.000224] [G loss: 9.298922]\n",
      "[Epoch 1/200] [Batch 256/600] [D loss: 0.000187] [G loss: 9.171731]\n",
      "[Epoch 1/200] [Batch 257/600] [D loss: 0.000210] [G loss: 9.119342]\n",
      "[Epoch 1/200] [Batch 258/600] [D loss: 0.000230] [G loss: 9.195798]\n",
      "[Epoch 1/200] [Batch 259/600] [D loss: 0.000268] [G loss: 9.339868]\n",
      "[Epoch 1/200] [Batch 260/600] [D loss: 0.000181] [G loss: 9.123380]\n",
      "[Epoch 1/200] [Batch 261/600] [D loss: 0.000200] [G loss: 9.031839]\n",
      "[Epoch 1/200] [Batch 262/600] [D loss: 0.000210] [G loss: 9.138631]\n",
      "[Epoch 1/200] [Batch 263/600] [D loss: 0.000230] [G loss: 8.963842]\n",
      "[Epoch 1/200] [Batch 264/600] [D loss: 0.000233] [G loss: 8.999334]\n",
      "[Epoch 1/200] [Batch 265/600] [D loss: 0.000183] [G loss: 8.914349]\n",
      "[Epoch 1/200] [Batch 266/600] [D loss: 0.000239] [G loss: 9.015690]\n",
      "[Epoch 1/200] [Batch 267/600] [D loss: 0.000224] [G loss: 9.124454]\n",
      "[Epoch 1/200] [Batch 268/600] [D loss: 0.000307] [G loss: 9.112508]\n",
      "[Epoch 1/200] [Batch 269/600] [D loss: 0.000253] [G loss: 8.912719]\n",
      "[Epoch 1/200] [Batch 270/600] [D loss: 0.000214] [G loss: 9.050564]\n",
      "[Epoch 1/200] [Batch 271/600] [D loss: 0.000191] [G loss: 9.116035]\n",
      "[Epoch 1/200] [Batch 272/600] [D loss: 0.000234] [G loss: 9.195476]\n",
      "[Epoch 1/200] [Batch 273/600] [D loss: 0.000193] [G loss: 9.153942]\n",
      "[Epoch 1/200] [Batch 274/600] [D loss: 0.000271] [G loss: 9.235621]\n",
      "[Epoch 1/200] [Batch 275/600] [D loss: 0.000152] [G loss: 9.180984]\n",
      "[Epoch 1/200] [Batch 276/600] [D loss: 0.000199] [G loss: 9.365146]\n",
      "[Epoch 1/200] [Batch 277/600] [D loss: 0.000176] [G loss: 9.275895]\n",
      "[Epoch 1/200] [Batch 278/600] [D loss: 0.000177] [G loss: 9.216412]\n",
      "[Epoch 1/200] [Batch 279/600] [D loss: 0.000300] [G loss: 9.087872]\n",
      "[Epoch 1/200] [Batch 280/600] [D loss: 0.000228] [G loss: 9.271101]\n",
      "[Epoch 1/200] [Batch 281/600] [D loss: 0.000224] [G loss: 8.956739]\n",
      "[Epoch 1/200] [Batch 282/600] [D loss: 0.000154] [G loss: 8.938120]\n",
      "[Epoch 1/200] [Batch 283/600] [D loss: 0.000216] [G loss: 8.990986]\n",
      "[Epoch 1/200] [Batch 284/600] [D loss: 0.000251] [G loss: 9.141155]\n",
      "[Epoch 1/200] [Batch 285/600] [D loss: 0.000180] [G loss: 9.106030]\n",
      "[Epoch 1/200] [Batch 286/600] [D loss: 0.000210] [G loss: 9.060418]\n",
      "[Epoch 1/200] [Batch 287/600] [D loss: 0.000155] [G loss: 9.273328]\n",
      "[Epoch 1/200] [Batch 288/600] [D loss: 0.000231] [G loss: 9.231398]\n",
      "[Epoch 1/200] [Batch 289/600] [D loss: 0.000165] [G loss: 9.168626]\n",
      "[Epoch 1/200] [Batch 290/600] [D loss: 0.000177] [G loss: 9.271508]\n",
      "[Epoch 1/200] [Batch 291/600] [D loss: 0.000219] [G loss: 9.345441]\n",
      "[Epoch 1/200] [Batch 292/600] [D loss: 0.000294] [G loss: 8.837306]\n",
      "[Epoch 1/200] [Batch 293/600] [D loss: 0.000192] [G loss: 8.958691]\n",
      "[Epoch 1/200] [Batch 294/600] [D loss: 0.000217] [G loss: 9.045979]\n",
      "[Epoch 1/200] [Batch 295/600] [D loss: 0.000176] [G loss: 9.136274]\n",
      "[Epoch 1/200] [Batch 296/600] [D loss: 0.000178] [G loss: 9.172405]\n",
      "[Epoch 1/200] [Batch 297/600] [D loss: 0.000217] [G loss: 9.091770]\n",
      "[Epoch 1/200] [Batch 298/600] [D loss: 0.000207] [G loss: 9.225446]\n",
      "[Epoch 1/200] [Batch 299/600] [D loss: 0.000241] [G loss: 9.266369]\n",
      "[Epoch 1/200] [Batch 300/600] [D loss: 0.000225] [G loss: 9.239354]\n",
      "[Epoch 1/200] [Batch 301/600] [D loss: 0.000168] [G loss: 9.146748]\n",
      "[Epoch 1/200] [Batch 302/600] [D loss: 0.000165] [G loss: 9.334225]\n",
      "[Epoch 1/200] [Batch 303/600] [D loss: 0.000214] [G loss: 8.839042]\n",
      "[Epoch 1/200] [Batch 304/600] [D loss: 0.000200] [G loss: 9.163702]\n",
      "[Epoch 1/200] [Batch 305/600] [D loss: 0.000204] [G loss: 9.213529]\n",
      "[Epoch 1/200] [Batch 306/600] [D loss: 0.000146] [G loss: 9.157821]\n",
      "[Epoch 1/200] [Batch 307/600] [D loss: 0.000173] [G loss: 9.053397]\n",
      "[Epoch 1/200] [Batch 308/600] [D loss: 0.000245] [G loss: 9.196655]\n",
      "[Epoch 1/200] [Batch 309/600] [D loss: 0.000193] [G loss: 9.211248]\n",
      "[Epoch 1/200] [Batch 310/600] [D loss: 0.000148] [G loss: 9.096593]\n",
      "[Epoch 1/200] [Batch 311/600] [D loss: 0.000211] [G loss: 9.271594]\n",
      "[Epoch 1/200] [Batch 312/600] [D loss: 0.000212] [G loss: 9.212621]\n",
      "[Epoch 1/200] [Batch 313/600] [D loss: 0.000225] [G loss: 9.434064]\n",
      "[Epoch 1/200] [Batch 314/600] [D loss: 0.000150] [G loss: 9.228361]\n",
      "[Epoch 1/200] [Batch 315/600] [D loss: 0.000191] [G loss: 9.235418]\n",
      "[Epoch 1/200] [Batch 316/600] [D loss: 0.000193] [G loss: 9.532609]\n",
      "[Epoch 1/200] [Batch 317/600] [D loss: 0.000184] [G loss: 9.115408]\n",
      "[Epoch 1/200] [Batch 318/600] [D loss: 0.000184] [G loss: 9.004117]\n",
      "[Epoch 1/200] [Batch 319/600] [D loss: 0.000241] [G loss: 9.233538]\n",
      "[Epoch 1/200] [Batch 320/600] [D loss: 0.000241] [G loss: 9.203056]\n",
      "[Epoch 1/200] [Batch 321/600] [D loss: 0.000148] [G loss: 8.868901]\n",
      "[Epoch 1/200] [Batch 322/600] [D loss: 0.000147] [G loss: 9.072494]\n",
      "[Epoch 1/200] [Batch 323/600] [D loss: 0.000285] [G loss: 9.236924]\n",
      "[Epoch 1/200] [Batch 324/600] [D loss: 0.000209] [G loss: 9.065396]\n",
      "[Epoch 1/200] [Batch 325/600] [D loss: 0.000218] [G loss: 9.132929]\n",
      "[Epoch 1/200] [Batch 326/600] [D loss: 0.000225] [G loss: 9.079101]\n",
      "[Epoch 1/200] [Batch 327/600] [D loss: 0.000221] [G loss: 9.432252]\n",
      "[Epoch 1/200] [Batch 328/600] [D loss: 0.000161] [G loss: 9.224619]\n",
      "[Epoch 1/200] [Batch 329/600] [D loss: 0.000200] [G loss: 9.536456]\n",
      "[Epoch 1/200] [Batch 330/600] [D loss: 0.000217] [G loss: 9.185080]\n",
      "[Epoch 1/200] [Batch 331/600] [D loss: 0.000279] [G loss: 9.212716]\n",
      "[Epoch 1/200] [Batch 332/600] [D loss: 0.000185] [G loss: 9.249044]\n",
      "[Epoch 1/200] [Batch 333/600] [D loss: 0.000164] [G loss: 9.032855]\n",
      "[Epoch 1/200] [Batch 334/600] [D loss: 0.000210] [G loss: 9.026959]\n",
      "[Epoch 1/200] [Batch 335/600] [D loss: 0.000204] [G loss: 9.182130]\n",
      "[Epoch 1/200] [Batch 336/600] [D loss: 0.000165] [G loss: 9.360466]\n",
      "[Epoch 1/200] [Batch 337/600] [D loss: 0.000165] [G loss: 9.213376]\n",
      "[Epoch 1/200] [Batch 338/600] [D loss: 0.000204] [G loss: 9.263793]\n",
      "[Epoch 1/200] [Batch 339/600] [D loss: 0.000159] [G loss: 9.098801]\n",
      "[Epoch 1/200] [Batch 340/600] [D loss: 0.000181] [G loss: 9.103051]\n",
      "[Epoch 1/200] [Batch 341/600] [D loss: 0.000160] [G loss: 9.189432]\n",
      "[Epoch 1/200] [Batch 342/600] [D loss: 0.000148] [G loss: 9.583518]\n",
      "[Epoch 1/200] [Batch 343/600] [D loss: 0.000157] [G loss: 9.199331]\n",
      "[Epoch 1/200] [Batch 344/600] [D loss: 0.000177] [G loss: 9.371424]\n",
      "[Epoch 1/200] [Batch 345/600] [D loss: 0.000178] [G loss: 9.400368]\n",
      "[Epoch 1/200] [Batch 346/600] [D loss: 0.000184] [G loss: 9.364703]\n",
      "[Epoch 1/200] [Batch 347/600] [D loss: 0.000170] [G loss: 9.357519]\n",
      "[Epoch 1/200] [Batch 348/600] [D loss: 0.000201] [G loss: 9.379382]\n",
      "[Epoch 1/200] [Batch 349/600] [D loss: 0.000187] [G loss: 9.340791]\n",
      "[Epoch 1/200] [Batch 350/600] [D loss: 0.000225] [G loss: 9.312137]\n",
      "[Epoch 1/200] [Batch 351/600] [D loss: 0.000195] [G loss: 9.333220]\n",
      "[Epoch 1/200] [Batch 352/600] [D loss: 0.000166] [G loss: 9.163008]\n",
      "[Epoch 1/200] [Batch 353/600] [D loss: 0.000183] [G loss: 9.271207]\n",
      "[Epoch 1/200] [Batch 354/600] [D loss: 0.000180] [G loss: 9.326080]\n",
      "[Epoch 1/200] [Batch 355/600] [D loss: 0.000180] [G loss: 9.202646]\n",
      "[Epoch 1/200] [Batch 356/600] [D loss: 0.000127] [G loss: 9.388658]\n",
      "[Epoch 1/200] [Batch 357/600] [D loss: 0.000215] [G loss: 9.126545]\n",
      "[Epoch 1/200] [Batch 358/600] [D loss: 0.000154] [G loss: 9.401668]\n",
      "[Epoch 1/200] [Batch 359/600] [D loss: 0.000174] [G loss: 9.558086]\n",
      "[Epoch 1/200] [Batch 360/600] [D loss: 0.000184] [G loss: 9.408038]\n",
      "[Epoch 1/200] [Batch 361/600] [D loss: 0.000158] [G loss: 9.538083]\n",
      "[Epoch 1/200] [Batch 362/600] [D loss: 0.000159] [G loss: 9.109715]\n",
      "[Epoch 1/200] [Batch 363/600] [D loss: 0.000129] [G loss: 9.273929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 364/600] [D loss: 0.000163] [G loss: 9.275195]\n",
      "[Epoch 1/200] [Batch 365/600] [D loss: 0.000191] [G loss: 9.403320]\n",
      "[Epoch 1/200] [Batch 366/600] [D loss: 0.000170] [G loss: 9.357193]\n",
      "[Epoch 1/200] [Batch 367/600] [D loss: 0.000212] [G loss: 9.282433]\n",
      "[Epoch 1/200] [Batch 368/600] [D loss: 0.000196] [G loss: 9.276418]\n",
      "[Epoch 1/200] [Batch 369/600] [D loss: 0.000163] [G loss: 9.298134]\n",
      "[Epoch 1/200] [Batch 370/600] [D loss: 0.000129] [G loss: 9.189229]\n",
      "[Epoch 1/200] [Batch 371/600] [D loss: 0.000161] [G loss: 9.332725]\n",
      "[Epoch 1/200] [Batch 372/600] [D loss: 0.000169] [G loss: 9.334762]\n",
      "[Epoch 1/200] [Batch 373/600] [D loss: 0.000176] [G loss: 9.527009]\n",
      "[Epoch 1/200] [Batch 374/600] [D loss: 0.000216] [G loss: 9.053519]\n",
      "[Epoch 1/200] [Batch 375/600] [D loss: 0.000156] [G loss: 9.390020]\n",
      "[Epoch 1/200] [Batch 376/600] [D loss: 0.000229] [G loss: 9.473807]\n",
      "[Epoch 1/200] [Batch 377/600] [D loss: 0.000170] [G loss: 9.347260]\n",
      "[Epoch 1/200] [Batch 378/600] [D loss: 0.000180] [G loss: 9.289369]\n",
      "[Epoch 1/200] [Batch 379/600] [D loss: 0.000203] [G loss: 9.469214]\n",
      "[Epoch 1/200] [Batch 380/600] [D loss: 0.000154] [G loss: 9.611553]\n",
      "[Epoch 1/200] [Batch 381/600] [D loss: 0.000107] [G loss: 9.238990]\n",
      "[Epoch 1/200] [Batch 382/600] [D loss: 0.000201] [G loss: 9.170306]\n",
      "[Epoch 1/200] [Batch 383/600] [D loss: 0.000148] [G loss: 9.331952]\n",
      "[Epoch 1/200] [Batch 384/600] [D loss: 0.000209] [G loss: 9.140811]\n",
      "[Epoch 1/200] [Batch 385/600] [D loss: 0.000191] [G loss: 9.261723]\n",
      "[Epoch 1/200] [Batch 386/600] [D loss: 0.000189] [G loss: 9.347194]\n",
      "[Epoch 1/200] [Batch 387/600] [D loss: 0.000158] [G loss: 9.200749]\n",
      "[Epoch 1/200] [Batch 388/600] [D loss: 0.000168] [G loss: 9.330114]\n",
      "[Epoch 1/200] [Batch 389/600] [D loss: 0.000180] [G loss: 8.926813]\n",
      "[Epoch 1/200] [Batch 390/600] [D loss: 0.000165] [G loss: 9.414347]\n",
      "[Epoch 1/200] [Batch 391/600] [D loss: 0.000150] [G loss: 9.304454]\n",
      "[Epoch 1/200] [Batch 392/600] [D loss: 0.000175] [G loss: 9.348927]\n",
      "[Epoch 1/200] [Batch 393/600] [D loss: 0.000196] [G loss: 9.516716]\n",
      "[Epoch 1/200] [Batch 394/600] [D loss: 0.000199] [G loss: 9.243150]\n",
      "[Epoch 1/200] [Batch 395/600] [D loss: 0.000177] [G loss: 9.272879]\n",
      "[Epoch 1/200] [Batch 396/600] [D loss: 0.000232] [G loss: 9.389290]\n",
      "[Epoch 1/200] [Batch 397/600] [D loss: 0.000152] [G loss: 9.613213]\n",
      "[Epoch 1/200] [Batch 398/600] [D loss: 0.000152] [G loss: 9.304887]\n",
      "[Epoch 1/200] [Batch 399/600] [D loss: 0.000162] [G loss: 9.205247]\n",
      "[Epoch 1/200] [Batch 400/600] [D loss: 0.000104] [G loss: 9.331921]\n",
      "[Epoch 1/200] [Batch 401/600] [D loss: 0.000171] [G loss: 9.186963]\n",
      "[Epoch 1/200] [Batch 402/600] [D loss: 0.000240] [G loss: 9.211689]\n",
      "[Epoch 1/200] [Batch 403/600] [D loss: 0.000141] [G loss: 9.389401]\n",
      "[Epoch 1/200] [Batch 404/600] [D loss: 0.000138] [G loss: 9.340551]\n",
      "[Epoch 1/200] [Batch 405/600] [D loss: 0.000218] [G loss: 9.216599]\n",
      "[Epoch 1/200] [Batch 406/600] [D loss: 0.000149] [G loss: 9.689564]\n",
      "[Epoch 1/200] [Batch 407/600] [D loss: 0.000143] [G loss: 9.329433]\n",
      "[Epoch 1/200] [Batch 408/600] [D loss: 0.000153] [G loss: 9.273425]\n",
      "[Epoch 1/200] [Batch 409/600] [D loss: 0.000150] [G loss: 9.371034]\n",
      "[Epoch 1/200] [Batch 410/600] [D loss: 0.000161] [G loss: 9.185086]\n",
      "[Epoch 1/200] [Batch 411/600] [D loss: 0.000179] [G loss: 9.418402]\n",
      "[Epoch 1/200] [Batch 412/600] [D loss: 0.000150] [G loss: 9.362384]\n",
      "[Epoch 1/200] [Batch 413/600] [D loss: 0.000157] [G loss: 9.233471]\n",
      "[Epoch 1/200] [Batch 414/600] [D loss: 0.000179] [G loss: 9.290003]\n",
      "[Epoch 1/200] [Batch 415/600] [D loss: 0.000181] [G loss: 9.457895]\n",
      "[Epoch 1/200] [Batch 416/600] [D loss: 0.000122] [G loss: 9.534602]\n",
      "[Epoch 1/200] [Batch 417/600] [D loss: 0.000148] [G loss: 9.529640]\n",
      "[Epoch 1/200] [Batch 418/600] [D loss: 0.000132] [G loss: 9.194328]\n",
      "[Epoch 1/200] [Batch 419/600] [D loss: 0.000172] [G loss: 9.473044]\n",
      "[Epoch 1/200] [Batch 420/600] [D loss: 0.000157] [G loss: 9.445310]\n",
      "[Epoch 1/200] [Batch 421/600] [D loss: 0.000125] [G loss: 9.434546]\n",
      "[Epoch 1/200] [Batch 422/600] [D loss: 0.000120] [G loss: 9.564566]\n",
      "[Epoch 1/200] [Batch 423/600] [D loss: 0.000207] [G loss: 9.202303]\n",
      "[Epoch 1/200] [Batch 424/600] [D loss: 0.000128] [G loss: 9.390020]\n",
      "[Epoch 1/200] [Batch 425/600] [D loss: 0.000179] [G loss: 9.383101]\n",
      "[Epoch 1/200] [Batch 426/600] [D loss: 0.000138] [G loss: 9.218418]\n",
      "[Epoch 1/200] [Batch 427/600] [D loss: 0.000125] [G loss: 9.384176]\n",
      "[Epoch 1/200] [Batch 428/600] [D loss: 0.000102] [G loss: 9.551294]\n",
      "[Epoch 1/200] [Batch 429/600] [D loss: 0.000161] [G loss: 9.314993]\n",
      "[Epoch 1/200] [Batch 430/600] [D loss: 0.000142] [G loss: 9.665094]\n",
      "[Epoch 1/200] [Batch 431/600] [D loss: 0.000152] [G loss: 9.604110]\n",
      "[Epoch 1/200] [Batch 432/600] [D loss: 0.000133] [G loss: 9.357162]\n",
      "[Epoch 1/200] [Batch 433/600] [D loss: 0.000148] [G loss: 9.483581]\n",
      "[Epoch 1/200] [Batch 434/600] [D loss: 0.000161] [G loss: 9.454776]\n",
      "[Epoch 1/200] [Batch 435/600] [D loss: 0.000160] [G loss: 9.330108]\n",
      "[Epoch 1/200] [Batch 436/600] [D loss: 0.000151] [G loss: 9.688519]\n",
      "[Epoch 1/200] [Batch 437/600] [D loss: 0.000152] [G loss: 9.453405]\n",
      "[Epoch 1/200] [Batch 438/600] [D loss: 0.000155] [G loss: 9.521150]\n",
      "[Epoch 1/200] [Batch 439/600] [D loss: 0.000124] [G loss: 9.394676]\n",
      "[Epoch 1/200] [Batch 440/600] [D loss: 0.000194] [G loss: 9.533905]\n",
      "[Epoch 1/200] [Batch 441/600] [D loss: 0.000143] [G loss: 9.317901]\n",
      "[Epoch 1/200] [Batch 442/600] [D loss: 0.000125] [G loss: 9.268118]\n",
      "[Epoch 1/200] [Batch 443/600] [D loss: 0.000164] [G loss: 9.335807]\n",
      "[Epoch 1/200] [Batch 444/600] [D loss: 0.000187] [G loss: 9.638796]\n",
      "[Epoch 1/200] [Batch 445/600] [D loss: 0.000140] [G loss: 9.430539]\n",
      "[Epoch 1/200] [Batch 446/600] [D loss: 0.000136] [G loss: 9.542056]\n",
      "[Epoch 1/200] [Batch 447/600] [D loss: 0.000184] [G loss: 9.275693]\n",
      "[Epoch 1/200] [Batch 448/600] [D loss: 0.000131] [G loss: 9.353745]\n",
      "[Epoch 1/200] [Batch 449/600] [D loss: 0.000179] [G loss: 9.526272]\n",
      "[Epoch 1/200] [Batch 450/600] [D loss: 0.000146] [G loss: 9.553719]\n",
      "[Epoch 1/200] [Batch 451/600] [D loss: 0.000171] [G loss: 9.573726]\n",
      "[Epoch 1/200] [Batch 452/600] [D loss: 0.000128] [G loss: 9.391475]\n",
      "[Epoch 1/200] [Batch 453/600] [D loss: 0.000148] [G loss: 9.680646]\n",
      "[Epoch 1/200] [Batch 454/600] [D loss: 0.000153] [G loss: 9.641221]\n",
      "[Epoch 1/200] [Batch 455/600] [D loss: 0.000135] [G loss: 9.527092]\n",
      "[Epoch 1/200] [Batch 456/600] [D loss: 0.000163] [G loss: 9.380504]\n",
      "[Epoch 1/200] [Batch 457/600] [D loss: 0.000157] [G loss: 9.460087]\n",
      "[Epoch 1/200] [Batch 458/600] [D loss: 0.000203] [G loss: 9.377366]\n",
      "[Epoch 1/200] [Batch 459/600] [D loss: 0.000150] [G loss: 9.373573]\n",
      "[Epoch 1/200] [Batch 460/600] [D loss: 0.000200] [G loss: 9.663631]\n",
      "[Epoch 1/200] [Batch 461/600] [D loss: 0.000186] [G loss: 9.517901]\n",
      "[Epoch 1/200] [Batch 462/600] [D loss: 0.000128] [G loss: 9.634335]\n",
      "[Epoch 1/200] [Batch 463/600] [D loss: 0.000165] [G loss: 9.819878]\n",
      "[Epoch 1/200] [Batch 464/600] [D loss: 0.000159] [G loss: 9.326576]\n",
      "[Epoch 1/200] [Batch 465/600] [D loss: 0.000154] [G loss: 9.617555]\n",
      "[Epoch 1/200] [Batch 466/600] [D loss: 0.000127] [G loss: 9.427798]\n",
      "[Epoch 1/200] [Batch 467/600] [D loss: 0.000145] [G loss: 9.527864]\n",
      "[Epoch 1/200] [Batch 468/600] [D loss: 0.000143] [G loss: 9.440361]\n",
      "[Epoch 1/200] [Batch 469/600] [D loss: 0.000152] [G loss: 9.590424]\n",
      "[Epoch 1/200] [Batch 470/600] [D loss: 0.000171] [G loss: 9.411132]\n",
      "[Epoch 1/200] [Batch 471/600] [D loss: 0.000136] [G loss: 9.517686]\n",
      "[Epoch 1/200] [Batch 472/600] [D loss: 0.000136] [G loss: 9.412592]\n",
      "[Epoch 1/200] [Batch 473/600] [D loss: 0.000131] [G loss: 9.460511]\n",
      "[Epoch 1/200] [Batch 474/600] [D loss: 0.000136] [G loss: 9.506200]\n",
      "[Epoch 1/200] [Batch 475/600] [D loss: 0.000158] [G loss: 9.182114]\n",
      "[Epoch 1/200] [Batch 476/600] [D loss: 0.000100] [G loss: 9.398045]\n",
      "[Epoch 1/200] [Batch 477/600] [D loss: 0.000121] [G loss: 9.776770]\n",
      "[Epoch 1/200] [Batch 478/600] [D loss: 0.000194] [G loss: 9.520886]\n",
      "[Epoch 1/200] [Batch 479/600] [D loss: 0.000145] [G loss: 9.644027]\n",
      "[Epoch 1/200] [Batch 480/600] [D loss: 0.000189] [G loss: 9.400455]\n",
      "[Epoch 1/200] [Batch 481/600] [D loss: 0.000153] [G loss: 9.583688]\n",
      "[Epoch 1/200] [Batch 482/600] [D loss: 0.000169] [G loss: 9.449192]\n",
      "[Epoch 1/200] [Batch 483/600] [D loss: 0.000198] [G loss: 9.463542]\n",
      "[Epoch 1/200] [Batch 484/600] [D loss: 0.000145] [G loss: 9.454664]\n",
      "[Epoch 1/200] [Batch 485/600] [D loss: 0.000130] [G loss: 9.588468]\n",
      "[Epoch 1/200] [Batch 486/600] [D loss: 0.000126] [G loss: 9.607467]\n",
      "[Epoch 1/200] [Batch 487/600] [D loss: 0.000227] [G loss: 9.632053]\n",
      "[Epoch 1/200] [Batch 488/600] [D loss: 0.000131] [G loss: 9.464742]\n",
      "[Epoch 1/200] [Batch 489/600] [D loss: 0.000155] [G loss: 9.656670]\n",
      "[Epoch 1/200] [Batch 490/600] [D loss: 0.000149] [G loss: 9.596817]\n",
      "[Epoch 1/200] [Batch 491/600] [D loss: 0.000161] [G loss: 9.629207]\n",
      "[Epoch 1/200] [Batch 492/600] [D loss: 0.000133] [G loss: 9.361911]\n",
      "[Epoch 1/200] [Batch 493/600] [D loss: 0.000109] [G loss: 9.681174]\n",
      "[Epoch 1/200] [Batch 494/600] [D loss: 0.000190] [G loss: 9.715180]\n",
      "[Epoch 1/200] [Batch 495/600] [D loss: 0.000179] [G loss: 9.416943]\n",
      "[Epoch 1/200] [Batch 496/600] [D loss: 0.000132] [G loss: 9.682207]\n",
      "[Epoch 1/200] [Batch 497/600] [D loss: 0.000151] [G loss: 9.669892]\n",
      "[Epoch 1/200] [Batch 498/600] [D loss: 0.000127] [G loss: 9.729563]\n",
      "[Epoch 1/200] [Batch 499/600] [D loss: 0.000141] [G loss: 9.434597]\n",
      "[Epoch 1/200] [Batch 500/600] [D loss: 0.000138] [G loss: 9.803096]\n",
      "[Epoch 1/200] [Batch 501/600] [D loss: 0.000177] [G loss: 9.888566]\n",
      "[Epoch 1/200] [Batch 502/600] [D loss: 0.000099] [G loss: 9.504126]\n",
      "[Epoch 1/200] [Batch 503/600] [D loss: 0.000137] [G loss: 9.550337]\n",
      "[Epoch 1/200] [Batch 504/600] [D loss: 0.000177] [G loss: 9.996476]\n",
      "[Epoch 1/200] [Batch 505/600] [D loss: 0.000148] [G loss: 9.518609]\n",
      "[Epoch 1/200] [Batch 506/600] [D loss: 0.000131] [G loss: 9.526546]\n",
      "[Epoch 1/200] [Batch 507/600] [D loss: 0.000134] [G loss: 9.635349]\n",
      "[Epoch 1/200] [Batch 508/600] [D loss: 0.000103] [G loss: 9.672077]\n",
      "[Epoch 1/200] [Batch 509/600] [D loss: 0.000122] [G loss: 9.698830]\n",
      "[Epoch 1/200] [Batch 510/600] [D loss: 0.000154] [G loss: 9.722280]\n",
      "[Epoch 1/200] [Batch 511/600] [D loss: 0.000139] [G loss: 9.616435]\n",
      "[Epoch 1/200] [Batch 512/600] [D loss: 0.000134] [G loss: 9.546717]\n",
      "[Epoch 1/200] [Batch 513/600] [D loss: 0.000116] [G loss: 9.567201]\n",
      "[Epoch 1/200] [Batch 514/600] [D loss: 0.000169] [G loss: 9.995309]\n",
      "[Epoch 1/200] [Batch 515/600] [D loss: 0.000160] [G loss: 9.732579]\n",
      "[Epoch 1/200] [Batch 516/600] [D loss: 0.000113] [G loss: 9.448689]\n",
      "[Epoch 1/200] [Batch 517/600] [D loss: 0.000157] [G loss: 9.482879]\n",
      "[Epoch 1/200] [Batch 518/600] [D loss: 0.000089] [G loss: 9.508367]\n",
      "[Epoch 1/200] [Batch 519/600] [D loss: 0.000123] [G loss: 9.608883]\n",
      "[Epoch 1/200] [Batch 520/600] [D loss: 0.000140] [G loss: 9.398691]\n",
      "[Epoch 1/200] [Batch 521/600] [D loss: 0.000124] [G loss: 9.742897]\n",
      "[Epoch 1/200] [Batch 522/600] [D loss: 0.000117] [G loss: 9.531269]\n",
      "[Epoch 1/200] [Batch 523/600] [D loss: 0.000320] [G loss: 9.470227]\n",
      "[Epoch 1/200] [Batch 524/600] [D loss: 0.000113] [G loss: 9.604638]\n",
      "[Epoch 1/200] [Batch 525/600] [D loss: 0.000119] [G loss: 9.696124]\n",
      "[Epoch 1/200] [Batch 526/600] [D loss: 0.000155] [G loss: 9.733425]\n",
      "[Epoch 1/200] [Batch 527/600] [D loss: 0.000101] [G loss: 9.714010]\n",
      "[Epoch 1/200] [Batch 528/600] [D loss: 0.000169] [G loss: 9.600231]\n",
      "[Epoch 1/200] [Batch 529/600] [D loss: 0.000162] [G loss: 9.653918]\n",
      "[Epoch 1/200] [Batch 530/600] [D loss: 0.000159] [G loss: 9.687293]\n",
      "[Epoch 1/200] [Batch 531/600] [D loss: 0.000123] [G loss: 9.592730]\n",
      "[Epoch 1/200] [Batch 532/600] [D loss: 0.000097] [G loss: 9.613858]\n",
      "[Epoch 1/200] [Batch 533/600] [D loss: 0.000171] [G loss: 9.582171]\n",
      "[Epoch 1/200] [Batch 534/600] [D loss: 0.000111] [G loss: 9.553066]\n",
      "[Epoch 1/200] [Batch 535/600] [D loss: 0.000110] [G loss: 9.565565]\n",
      "[Epoch 1/200] [Batch 536/600] [D loss: 0.000109] [G loss: 9.785640]\n",
      "[Epoch 1/200] [Batch 537/600] [D loss: 0.000131] [G loss: 9.676125]\n",
      "[Epoch 1/200] [Batch 538/600] [D loss: 0.000148] [G loss: 9.542026]\n",
      "[Epoch 1/200] [Batch 539/600] [D loss: 0.000215] [G loss: 9.928178]\n",
      "[Epoch 1/200] [Batch 540/600] [D loss: 0.000112] [G loss: 9.783981]\n",
      "[Epoch 1/200] [Batch 541/600] [D loss: 0.000107] [G loss: 9.769604]\n",
      "[Epoch 1/200] [Batch 542/600] [D loss: 0.000106] [G loss: 9.728623]\n",
      "[Epoch 1/200] [Batch 543/600] [D loss: 0.000156] [G loss: 10.005120]\n",
      "[Epoch 1/200] [Batch 544/600] [D loss: 0.000095] [G loss: 9.862779]\n",
      "[Epoch 1/200] [Batch 545/600] [D loss: 0.000145] [G loss: 9.640591]\n",
      "[Epoch 1/200] [Batch 546/600] [D loss: 0.000130] [G loss: 9.808531]\n",
      "[Epoch 1/200] [Batch 547/600] [D loss: 0.000137] [G loss: 9.708220]\n",
      "[Epoch 1/200] [Batch 548/600] [D loss: 0.000105] [G loss: 9.674479]\n",
      "[Epoch 1/200] [Batch 549/600] [D loss: 0.000124] [G loss: 9.666380]\n",
      "[Epoch 1/200] [Batch 550/600] [D loss: 0.000106] [G loss: 9.617461]\n",
      "[Epoch 1/200] [Batch 551/600] [D loss: 0.000121] [G loss: 9.551423]\n",
      "[Epoch 1/200] [Batch 552/600] [D loss: 0.000096] [G loss: 9.713690]\n",
      "[Epoch 1/200] [Batch 553/600] [D loss: 0.000108] [G loss: 9.688703]\n",
      "[Epoch 1/200] [Batch 554/600] [D loss: 0.000097] [G loss: 9.793259]\n",
      "[Epoch 1/200] [Batch 555/600] [D loss: 0.000232] [G loss: 9.844953]\n",
      "[Epoch 1/200] [Batch 556/600] [D loss: 0.000112] [G loss: 9.425678]\n",
      "[Epoch 1/200] [Batch 557/600] [D loss: 0.000126] [G loss: 9.767852]\n",
      "[Epoch 1/200] [Batch 558/600] [D loss: 0.000121] [G loss: 9.772278]\n",
      "[Epoch 1/200] [Batch 559/600] [D loss: 0.000091] [G loss: 9.646996]\n",
      "[Epoch 1/200] [Batch 560/600] [D loss: 0.000129] [G loss: 9.881113]\n",
      "[Epoch 1/200] [Batch 561/600] [D loss: 0.000122] [G loss: 9.571609]\n",
      "[Epoch 1/200] [Batch 562/600] [D loss: 0.000123] [G loss: 9.574353]\n",
      "[Epoch 1/200] [Batch 563/600] [D loss: 0.000128] [G loss: 9.647506]\n",
      "[Epoch 1/200] [Batch 564/600] [D loss: 0.000157] [G loss: 9.635150]\n",
      "[Epoch 1/200] [Batch 565/600] [D loss: 0.000140] [G loss: 9.624258]\n",
      "[Epoch 1/200] [Batch 566/600] [D loss: 0.000144] [G loss: 9.891131]\n",
      "[Epoch 1/200] [Batch 567/600] [D loss: 0.000095] [G loss: 9.761327]\n",
      "[Epoch 1/200] [Batch 568/600] [D loss: 0.000140] [G loss: 9.873736]\n",
      "[Epoch 1/200] [Batch 569/600] [D loss: 0.000129] [G loss: 9.717072]\n",
      "[Epoch 1/200] [Batch 570/600] [D loss: 0.000099] [G loss: 9.965899]\n",
      "[Epoch 1/200] [Batch 571/600] [D loss: 0.000152] [G loss: 9.700234]\n",
      "[Epoch 1/200] [Batch 572/600] [D loss: 0.000104] [G loss: 9.650825]\n",
      "[Epoch 1/200] [Batch 573/600] [D loss: 0.000101] [G loss: 9.898507]\n",
      "[Epoch 1/200] [Batch 574/600] [D loss: 0.000122] [G loss: 9.858685]\n",
      "[Epoch 1/200] [Batch 575/600] [D loss: 0.000144] [G loss: 9.657029]\n",
      "[Epoch 1/200] [Batch 576/600] [D loss: 0.000134] [G loss: 9.875038]\n",
      "[Epoch 1/200] [Batch 577/600] [D loss: 0.000100] [G loss: 9.814534]\n",
      "[Epoch 1/200] [Batch 578/600] [D loss: 0.000131] [G loss: 9.733886]\n",
      "[Epoch 1/200] [Batch 579/600] [D loss: 0.000114] [G loss: 9.496372]\n",
      "[Epoch 1/200] [Batch 580/600] [D loss: 0.000110] [G loss: 9.719988]\n",
      "[Epoch 1/200] [Batch 581/600] [D loss: 0.000111] [G loss: 9.530225]\n",
      "[Epoch 1/200] [Batch 582/600] [D loss: 0.000135] [G loss: 9.812715]\n",
      "[Epoch 1/200] [Batch 583/600] [D loss: 0.000118] [G loss: 9.542767]\n",
      "[Epoch 1/200] [Batch 584/600] [D loss: 0.000124] [G loss: 9.957497]\n",
      "[Epoch 1/200] [Batch 585/600] [D loss: 0.000128] [G loss: 9.642581]\n",
      "[Epoch 1/200] [Batch 586/600] [D loss: 0.000121] [G loss: 9.806282]\n",
      "[Epoch 1/200] [Batch 587/600] [D loss: 0.000094] [G loss: 9.961899]\n",
      "[Epoch 1/200] [Batch 588/600] [D loss: 0.000122] [G loss: 9.560053]\n",
      "[Epoch 1/200] [Batch 589/600] [D loss: 0.000099] [G loss: 9.644770]\n",
      "[Epoch 1/200] [Batch 590/600] [D loss: 0.000115] [G loss: 9.502434]\n",
      "[Epoch 1/200] [Batch 591/600] [D loss: 0.000151] [G loss: 9.894467]\n",
      "[Epoch 1/200] [Batch 592/600] [D loss: 0.000148] [G loss: 9.841593]\n",
      "[Epoch 1/200] [Batch 593/600] [D loss: 0.000088] [G loss: 9.803799]\n",
      "[Epoch 1/200] [Batch 594/600] [D loss: 0.000127] [G loss: 9.763317]\n",
      "[Epoch 1/200] [Batch 595/600] [D loss: 0.000116] [G loss: 9.581969]\n",
      "[Epoch 1/200] [Batch 596/600] [D loss: 0.000110] [G loss: 9.709943]\n",
      "[Epoch 1/200] [Batch 597/600] [D loss: 0.000131] [G loss: 9.494030]\n",
      "[Epoch 1/200] [Batch 598/600] [D loss: 0.000099] [G loss: 10.019166]\n",
      "[Epoch 1/200] [Batch 599/600] [D loss: 0.000148] [G loss: 9.868488]\n",
      "[Epoch 2/200] [Batch 0/600] [D loss: 0.000117] [G loss: 9.861654]\n",
      "[Epoch 2/200] [Batch 1/600] [D loss: 0.000121] [G loss: 9.748814]\n",
      "[Epoch 2/200] [Batch 2/600] [D loss: 0.000134] [G loss: 9.720859]\n",
      "[Epoch 2/200] [Batch 3/600] [D loss: 0.000099] [G loss: 9.903821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 4/600] [D loss: 0.000179] [G loss: 9.948008]\n",
      "[Epoch 2/200] [Batch 5/600] [D loss: 0.000102] [G loss: 9.767533]\n",
      "[Epoch 2/200] [Batch 6/600] [D loss: 0.000104] [G loss: 9.743795]\n",
      "[Epoch 2/200] [Batch 7/600] [D loss: 0.000103] [G loss: 9.738871]\n",
      "[Epoch 2/200] [Batch 8/600] [D loss: 0.000091] [G loss: 9.736090]\n",
      "[Epoch 2/200] [Batch 9/600] [D loss: 0.000144] [G loss: 9.407537]\n",
      "[Epoch 2/200] [Batch 10/600] [D loss: 0.000093] [G loss: 9.842372]\n",
      "[Epoch 2/200] [Batch 11/600] [D loss: 0.000134] [G loss: 9.785001]\n",
      "[Epoch 2/200] [Batch 12/600] [D loss: 0.000092] [G loss: 9.678839]\n",
      "[Epoch 2/200] [Batch 13/600] [D loss: 0.000122] [G loss: 9.971008]\n",
      "[Epoch 2/200] [Batch 14/600] [D loss: 0.000140] [G loss: 9.686808]\n",
      "[Epoch 2/200] [Batch 15/600] [D loss: 0.000095] [G loss: 9.890226]\n",
      "[Epoch 2/200] [Batch 16/600] [D loss: 0.000102] [G loss: 9.925119]\n",
      "[Epoch 2/200] [Batch 17/600] [D loss: 0.000101] [G loss: 9.753422]\n",
      "[Epoch 2/200] [Batch 18/600] [D loss: 0.000117] [G loss: 9.854462]\n",
      "[Epoch 2/200] [Batch 19/600] [D loss: 0.000121] [G loss: 10.095948]\n",
      "[Epoch 2/200] [Batch 20/600] [D loss: 0.000113] [G loss: 9.684345]\n",
      "[Epoch 2/200] [Batch 21/600] [D loss: 0.000085] [G loss: 9.619613]\n",
      "[Epoch 2/200] [Batch 22/600] [D loss: 0.000171] [G loss: 9.813642]\n",
      "[Epoch 2/200] [Batch 23/600] [D loss: 0.000103] [G loss: 9.856137]\n",
      "[Epoch 2/200] [Batch 24/600] [D loss: 0.000135] [G loss: 9.561877]\n",
      "[Epoch 2/200] [Batch 25/600] [D loss: 0.000088] [G loss: 9.758281]\n",
      "[Epoch 2/200] [Batch 26/600] [D loss: 0.000104] [G loss: 9.910866]\n",
      "[Epoch 2/200] [Batch 27/600] [D loss: 0.000088] [G loss: 9.898538]\n",
      "[Epoch 2/200] [Batch 28/600] [D loss: 0.000102] [G loss: 9.758147]\n",
      "[Epoch 2/200] [Batch 29/600] [D loss: 0.000082] [G loss: 9.971460]\n",
      "[Epoch 2/200] [Batch 30/600] [D loss: 0.000097] [G loss: 9.787130]\n",
      "[Epoch 2/200] [Batch 31/600] [D loss: 0.000119] [G loss: 9.706769]\n",
      "[Epoch 2/200] [Batch 32/600] [D loss: 0.000127] [G loss: 9.832837]\n",
      "[Epoch 2/200] [Batch 33/600] [D loss: 0.000115] [G loss: 9.660476]\n",
      "[Epoch 2/200] [Batch 34/600] [D loss: 0.000124] [G loss: 9.787830]\n",
      "[Epoch 2/200] [Batch 35/600] [D loss: 0.000131] [G loss: 9.694755]\n",
      "[Epoch 2/200] [Batch 36/600] [D loss: 0.000091] [G loss: 9.902982]\n",
      "[Epoch 2/200] [Batch 37/600] [D loss: 0.000101] [G loss: 9.748177]\n",
      "[Epoch 2/200] [Batch 38/600] [D loss: 0.000096] [G loss: 9.971701]\n",
      "[Epoch 2/200] [Batch 39/600] [D loss: 0.000139] [G loss: 9.818869]\n",
      "[Epoch 2/200] [Batch 40/600] [D loss: 0.000120] [G loss: 9.708726]\n",
      "[Epoch 2/200] [Batch 41/600] [D loss: 0.000163] [G loss: 10.085223]\n",
      "[Epoch 2/200] [Batch 42/600] [D loss: 0.000115] [G loss: 9.849357]\n",
      "[Epoch 2/200] [Batch 43/600] [D loss: 0.000141] [G loss: 9.765733]\n",
      "[Epoch 2/200] [Batch 44/600] [D loss: 0.000087] [G loss: 9.603992]\n",
      "[Epoch 2/200] [Batch 45/600] [D loss: 0.000118] [G loss: 9.711183]\n",
      "[Epoch 2/200] [Batch 46/600] [D loss: 0.000080] [G loss: 9.824955]\n",
      "[Epoch 2/200] [Batch 47/600] [D loss: 0.000143] [G loss: 9.831964]\n",
      "[Epoch 2/200] [Batch 48/600] [D loss: 0.000087] [G loss: 9.877475]\n",
      "[Epoch 2/200] [Batch 49/600] [D loss: 0.000096] [G loss: 9.531625]\n",
      "[Epoch 2/200] [Batch 50/600] [D loss: 0.000131] [G loss: 9.756023]\n",
      "[Epoch 2/200] [Batch 51/600] [D loss: 0.000113] [G loss: 9.822707]\n",
      "[Epoch 2/200] [Batch 52/600] [D loss: 0.000109] [G loss: 9.733951]\n",
      "[Epoch 2/200] [Batch 53/600] [D loss: 0.000107] [G loss: 9.783047]\n",
      "[Epoch 2/200] [Batch 54/600] [D loss: 0.000090] [G loss: 9.893837]\n",
      "[Epoch 2/200] [Batch 55/600] [D loss: 0.000159] [G loss: 9.481070]\n",
      "[Epoch 2/200] [Batch 56/600] [D loss: 0.000108] [G loss: 9.779554]\n",
      "[Epoch 2/200] [Batch 57/600] [D loss: 0.000094] [G loss: 9.629754]\n",
      "[Epoch 2/200] [Batch 58/600] [D loss: 0.000101] [G loss: 9.792156]\n",
      "[Epoch 2/200] [Batch 59/600] [D loss: 0.000105] [G loss: 10.044977]\n",
      "[Epoch 2/200] [Batch 60/600] [D loss: 0.000142] [G loss: 9.916645]\n",
      "[Epoch 2/200] [Batch 61/600] [D loss: 0.000085] [G loss: 9.931179]\n",
      "[Epoch 2/200] [Batch 62/600] [D loss: 0.000104] [G loss: 9.548923]\n",
      "[Epoch 2/200] [Batch 63/600] [D loss: 0.000130] [G loss: 9.875793]\n",
      "[Epoch 2/200] [Batch 64/600] [D loss: 0.000090] [G loss: 9.773215]\n",
      "[Epoch 2/200] [Batch 65/600] [D loss: 0.000107] [G loss: 9.960298]\n",
      "[Epoch 2/200] [Batch 66/600] [D loss: 0.000114] [G loss: 9.833555]\n",
      "[Epoch 2/200] [Batch 67/600] [D loss: 0.000108] [G loss: 9.648194]\n",
      "[Epoch 2/200] [Batch 68/600] [D loss: 0.000085] [G loss: 9.889571]\n",
      "[Epoch 2/200] [Batch 69/600] [D loss: 0.000097] [G loss: 9.883174]\n",
      "[Epoch 2/200] [Batch 70/600] [D loss: 0.000112] [G loss: 9.660502]\n",
      "[Epoch 2/200] [Batch 71/600] [D loss: 0.000109] [G loss: 9.998790]\n",
      "[Epoch 2/200] [Batch 72/600] [D loss: 0.000088] [G loss: 9.939037]\n",
      "[Epoch 2/200] [Batch 73/600] [D loss: 0.000125] [G loss: 9.822041]\n",
      "[Epoch 2/200] [Batch 74/600] [D loss: 0.000085] [G loss: 9.886236]\n",
      "[Epoch 2/200] [Batch 75/600] [D loss: 0.000116] [G loss: 9.428886]\n",
      "[Epoch 2/200] [Batch 76/600] [D loss: 0.000119] [G loss: 10.145476]\n",
      "[Epoch 2/200] [Batch 77/600] [D loss: 0.000107] [G loss: 9.455987]\n",
      "[Epoch 2/200] [Batch 78/600] [D loss: 0.000087] [G loss: 9.960724]\n",
      "[Epoch 2/200] [Batch 79/600] [D loss: 0.000071] [G loss: 9.945633]\n",
      "[Epoch 2/200] [Batch 80/600] [D loss: 0.000086] [G loss: 10.010283]\n",
      "[Epoch 2/200] [Batch 81/600] [D loss: 0.000151] [G loss: 10.159070]\n",
      "[Epoch 2/200] [Batch 82/600] [D loss: 0.000097] [G loss: 10.002704]\n",
      "[Epoch 2/200] [Batch 83/600] [D loss: 0.000087] [G loss: 10.330585]\n",
      "[Epoch 2/200] [Batch 84/600] [D loss: 0.000099] [G loss: 10.094394]\n",
      "[Epoch 2/200] [Batch 85/600] [D loss: 0.000084] [G loss: 9.612179]\n",
      "[Epoch 2/200] [Batch 86/600] [D loss: 0.000092] [G loss: 10.050370]\n",
      "[Epoch 2/200] [Batch 87/600] [D loss: 0.000075] [G loss: 9.966608]\n",
      "[Epoch 2/200] [Batch 88/600] [D loss: 0.000132] [G loss: 9.794320]\n",
      "[Epoch 2/200] [Batch 89/600] [D loss: 0.000083] [G loss: 9.926540]\n",
      "[Epoch 2/200] [Batch 90/600] [D loss: 0.000076] [G loss: 9.827628]\n",
      "[Epoch 2/200] [Batch 91/600] [D loss: 0.000079] [G loss: 9.772934]\n",
      "[Epoch 2/200] [Batch 92/600] [D loss: 0.000114] [G loss: 9.875178]\n",
      "[Epoch 2/200] [Batch 93/600] [D loss: 0.000153] [G loss: 9.846928]\n",
      "[Epoch 2/200] [Batch 94/600] [D loss: 0.000082] [G loss: 9.774282]\n",
      "[Epoch 2/200] [Batch 95/600] [D loss: 0.000095] [G loss: 10.158066]\n",
      "[Epoch 2/200] [Batch 96/600] [D loss: 0.000112] [G loss: 9.746145]\n",
      "[Epoch 2/200] [Batch 97/600] [D loss: 0.000104] [G loss: 9.865590]\n",
      "[Epoch 2/200] [Batch 98/600] [D loss: 0.000088] [G loss: 10.213792]\n",
      "[Epoch 2/200] [Batch 99/600] [D loss: 0.000099] [G loss: 9.909576]\n",
      "[Epoch 2/200] [Batch 100/600] [D loss: 0.000085] [G loss: 9.919297]\n",
      "[Epoch 2/200] [Batch 101/600] [D loss: 0.000109] [G loss: 9.960623]\n",
      "[Epoch 2/200] [Batch 102/600] [D loss: 0.000090] [G loss: 9.996510]\n",
      "[Epoch 2/200] [Batch 103/600] [D loss: 0.000094] [G loss: 9.991953]\n",
      "[Epoch 2/200] [Batch 104/600] [D loss: 0.000106] [G loss: 10.000357]\n",
      "[Epoch 2/200] [Batch 105/600] [D loss: 0.000087] [G loss: 9.693942]\n",
      "[Epoch 2/200] [Batch 106/600] [D loss: 0.000098] [G loss: 9.779332]\n",
      "[Epoch 2/200] [Batch 107/600] [D loss: 0.000114] [G loss: 9.793313]\n",
      "[Epoch 2/200] [Batch 108/600] [D loss: 0.000102] [G loss: 10.223678]\n",
      "[Epoch 2/200] [Batch 109/600] [D loss: 0.000105] [G loss: 10.064976]\n",
      "[Epoch 2/200] [Batch 110/600] [D loss: 0.000093] [G loss: 9.974659]\n",
      "[Epoch 2/200] [Batch 111/600] [D loss: 0.000098] [G loss: 10.038647]\n",
      "[Epoch 2/200] [Batch 112/600] [D loss: 0.000134] [G loss: 9.921523]\n",
      "[Epoch 2/200] [Batch 113/600] [D loss: 0.000075] [G loss: 10.048523]\n",
      "[Epoch 2/200] [Batch 114/600] [D loss: 0.000134] [G loss: 10.075933]\n",
      "[Epoch 2/200] [Batch 115/600] [D loss: 0.000092] [G loss: 9.837757]\n",
      "[Epoch 2/200] [Batch 116/600] [D loss: 0.000125] [G loss: 9.888150]\n",
      "[Epoch 2/200] [Batch 117/600] [D loss: 0.000092] [G loss: 9.858772]\n",
      "[Epoch 2/200] [Batch 118/600] [D loss: 0.000100] [G loss: 10.110950]\n",
      "[Epoch 2/200] [Batch 119/600] [D loss: 0.000127] [G loss: 9.834745]\n",
      "[Epoch 2/200] [Batch 120/600] [D loss: 0.000090] [G loss: 9.695233]\n",
      "[Epoch 2/200] [Batch 121/600] [D loss: 0.000127] [G loss: 9.975580]\n",
      "[Epoch 2/200] [Batch 122/600] [D loss: 0.000098] [G loss: 9.643262]\n",
      "[Epoch 2/200] [Batch 123/600] [D loss: 0.000105] [G loss: 9.929206]\n",
      "[Epoch 2/200] [Batch 124/600] [D loss: 0.000094] [G loss: 9.673951]\n",
      "[Epoch 2/200] [Batch 125/600] [D loss: 0.000107] [G loss: 10.060845]\n",
      "[Epoch 2/200] [Batch 126/600] [D loss: 0.000097] [G loss: 10.020364]\n",
      "[Epoch 2/200] [Batch 127/600] [D loss: 0.000100] [G loss: 9.885877]\n",
      "[Epoch 2/200] [Batch 128/600] [D loss: 0.000094] [G loss: 9.979144]\n",
      "[Epoch 2/200] [Batch 129/600] [D loss: 0.000068] [G loss: 9.922696]\n",
      "[Epoch 2/200] [Batch 130/600] [D loss: 0.000107] [G loss: 9.618382]\n",
      "[Epoch 2/200] [Batch 131/600] [D loss: 0.000072] [G loss: 10.070755]\n",
      "[Epoch 2/200] [Batch 132/600] [D loss: 0.000106] [G loss: 9.952476]\n",
      "[Epoch 2/200] [Batch 133/600] [D loss: 0.000104] [G loss: 9.969261]\n",
      "[Epoch 2/200] [Batch 134/600] [D loss: 0.000107] [G loss: 10.319513]\n",
      "[Epoch 2/200] [Batch 135/600] [D loss: 0.000082] [G loss: 10.086746]\n",
      "[Epoch 2/200] [Batch 136/600] [D loss: 0.000089] [G loss: 10.232975]\n",
      "[Epoch 2/200] [Batch 137/600] [D loss: 0.000081] [G loss: 10.124781]\n",
      "[Epoch 2/200] [Batch 138/600] [D loss: 0.000088] [G loss: 10.059107]\n",
      "[Epoch 2/200] [Batch 139/600] [D loss: 0.000105] [G loss: 9.904478]\n",
      "[Epoch 2/200] [Batch 140/600] [D loss: 0.000107] [G loss: 9.927053]\n",
      "[Epoch 2/200] [Batch 141/600] [D loss: 0.000126] [G loss: 10.094627]\n",
      "[Epoch 2/200] [Batch 142/600] [D loss: 0.000088] [G loss: 10.118052]\n",
      "[Epoch 2/200] [Batch 143/600] [D loss: 0.000078] [G loss: 9.814453]\n",
      "[Epoch 2/200] [Batch 144/600] [D loss: 0.000094] [G loss: 10.078916]\n",
      "[Epoch 2/200] [Batch 145/600] [D loss: 0.000102] [G loss: 9.900558]\n",
      "[Epoch 2/200] [Batch 146/600] [D loss: 0.000100] [G loss: 9.919971]\n",
      "[Epoch 2/200] [Batch 147/600] [D loss: 0.000119] [G loss: 9.816762]\n",
      "[Epoch 2/200] [Batch 148/600] [D loss: 0.000085] [G loss: 10.031216]\n",
      "[Epoch 2/200] [Batch 149/600] [D loss: 0.000098] [G loss: 9.964316]\n",
      "[Epoch 2/200] [Batch 150/600] [D loss: 0.000086] [G loss: 10.164737]\n",
      "[Epoch 2/200] [Batch 151/600] [D loss: 0.000113] [G loss: 9.916757]\n",
      "[Epoch 2/200] [Batch 152/600] [D loss: 0.000097] [G loss: 10.072450]\n",
      "[Epoch 2/200] [Batch 153/600] [D loss: 0.000095] [G loss: 10.139562]\n",
      "[Epoch 2/200] [Batch 154/600] [D loss: 0.000086] [G loss: 10.215091]\n",
      "[Epoch 2/200] [Batch 155/600] [D loss: 0.000106] [G loss: 10.034558]\n",
      "[Epoch 2/200] [Batch 156/600] [D loss: 0.000080] [G loss: 9.959594]\n",
      "[Epoch 2/200] [Batch 157/600] [D loss: 0.000076] [G loss: 10.250995]\n",
      "[Epoch 2/200] [Batch 158/600] [D loss: 0.000140] [G loss: 9.959165]\n",
      "[Epoch 2/200] [Batch 159/600] [D loss: 0.000115] [G loss: 10.007350]\n",
      "[Epoch 2/200] [Batch 160/600] [D loss: 0.000111] [G loss: 9.773376]\n",
      "[Epoch 2/200] [Batch 161/600] [D loss: 0.000066] [G loss: 9.992428]\n",
      "[Epoch 2/200] [Batch 162/600] [D loss: 0.000078] [G loss: 10.203981]\n",
      "[Epoch 2/200] [Batch 163/600] [D loss: 0.000089] [G loss: 9.893630]\n",
      "[Epoch 2/200] [Batch 164/600] [D loss: 0.000109] [G loss: 9.994597]\n",
      "[Epoch 2/200] [Batch 165/600] [D loss: 0.000085] [G loss: 9.960511]\n",
      "[Epoch 2/200] [Batch 166/600] [D loss: 0.000099] [G loss: 9.977738]\n",
      "[Epoch 2/200] [Batch 167/600] [D loss: 0.000093] [G loss: 9.978108]\n",
      "[Epoch 2/200] [Batch 168/600] [D loss: 0.000082] [G loss: 9.930161]\n",
      "[Epoch 2/200] [Batch 169/600] [D loss: 0.000096] [G loss: 10.163949]\n",
      "[Epoch 2/200] [Batch 170/600] [D loss: 0.000099] [G loss: 9.982674]\n",
      "[Epoch 2/200] [Batch 171/600] [D loss: 0.000097] [G loss: 10.102024]\n",
      "[Epoch 2/200] [Batch 172/600] [D loss: 0.000071] [G loss: 10.204282]\n",
      "[Epoch 2/200] [Batch 173/600] [D loss: 0.000073] [G loss: 10.171684]\n",
      "[Epoch 2/200] [Batch 174/600] [D loss: 0.000129] [G loss: 10.089868]\n",
      "[Epoch 2/200] [Batch 175/600] [D loss: 0.000078] [G loss: 9.889566]\n",
      "[Epoch 2/200] [Batch 176/600] [D loss: 0.000103] [G loss: 9.831777]\n",
      "[Epoch 2/200] [Batch 177/600] [D loss: 0.000075] [G loss: 9.921747]\n",
      "[Epoch 2/200] [Batch 178/600] [D loss: 0.000086] [G loss: 10.203226]\n",
      "[Epoch 2/200] [Batch 179/600] [D loss: 0.000122] [G loss: 10.099072]\n",
      "[Epoch 2/200] [Batch 180/600] [D loss: 0.000125] [G loss: 10.205553]\n",
      "[Epoch 2/200] [Batch 181/600] [D loss: 0.000082] [G loss: 10.213407]\n",
      "[Epoch 2/200] [Batch 182/600] [D loss: 0.000087] [G loss: 9.920518]\n",
      "[Epoch 2/200] [Batch 183/600] [D loss: 0.000074] [G loss: 9.858935]\n",
      "[Epoch 2/200] [Batch 184/600] [D loss: 0.000072] [G loss: 10.011460]\n",
      "[Epoch 2/200] [Batch 185/600] [D loss: 0.000107] [G loss: 10.073398]\n",
      "[Epoch 2/200] [Batch 186/600] [D loss: 0.000088] [G loss: 10.317824]\n",
      "[Epoch 2/200] [Batch 187/600] [D loss: 0.000084] [G loss: 10.101174]\n",
      "[Epoch 2/200] [Batch 188/600] [D loss: 0.000114] [G loss: 10.198635]\n",
      "[Epoch 2/200] [Batch 189/600] [D loss: 0.000064] [G loss: 9.994975]\n",
      "[Epoch 2/200] [Batch 190/600] [D loss: 0.000074] [G loss: 10.154079]\n",
      "[Epoch 2/200] [Batch 191/600] [D loss: 0.000107] [G loss: 10.188367]\n",
      "[Epoch 2/200] [Batch 192/600] [D loss: 0.000078] [G loss: 9.903863]\n",
      "[Epoch 2/200] [Batch 193/600] [D loss: 0.000076] [G loss: 10.337643]\n",
      "[Epoch 2/200] [Batch 194/600] [D loss: 0.000072] [G loss: 10.063818]\n",
      "[Epoch 2/200] [Batch 195/600] [D loss: 0.000070] [G loss: 10.087689]\n",
      "[Epoch 2/200] [Batch 196/600] [D loss: 0.000105] [G loss: 9.904700]\n",
      "[Epoch 2/200] [Batch 197/600] [D loss: 0.000076] [G loss: 10.033882]\n",
      "[Epoch 2/200] [Batch 198/600] [D loss: 0.000100] [G loss: 10.393590]\n",
      "[Epoch 2/200] [Batch 199/600] [D loss: 0.000080] [G loss: 9.988085]\n",
      "[Epoch 2/200] [Batch 200/600] [D loss: 0.000099] [G loss: 10.133207]\n",
      "[Epoch 2/200] [Batch 201/600] [D loss: 0.000103] [G loss: 10.286816]\n",
      "[Epoch 2/200] [Batch 202/600] [D loss: 0.000086] [G loss: 10.026412]\n",
      "[Epoch 2/200] [Batch 203/600] [D loss: 0.000090] [G loss: 9.921488]\n",
      "[Epoch 2/200] [Batch 204/600] [D loss: 0.000066] [G loss: 10.207151]\n",
      "[Epoch 2/200] [Batch 205/600] [D loss: 0.000073] [G loss: 10.183490]\n",
      "[Epoch 2/200] [Batch 206/600] [D loss: 0.000080] [G loss: 10.332930]\n",
      "[Epoch 2/200] [Batch 207/600] [D loss: 0.000056] [G loss: 9.826696]\n",
      "[Epoch 2/200] [Batch 208/600] [D loss: 0.000069] [G loss: 10.226707]\n",
      "[Epoch 2/200] [Batch 209/600] [D loss: 0.000106] [G loss: 10.006571]\n",
      "[Epoch 2/200] [Batch 210/600] [D loss: 0.000083] [G loss: 10.283212]\n",
      "[Epoch 2/200] [Batch 211/600] [D loss: 0.000073] [G loss: 10.070532]\n",
      "[Epoch 2/200] [Batch 212/600] [D loss: 0.000085] [G loss: 9.646316]\n",
      "[Epoch 2/200] [Batch 213/600] [D loss: 0.000109] [G loss: 10.227843]\n",
      "[Epoch 2/200] [Batch 214/600] [D loss: 0.000101] [G loss: 10.221482]\n",
      "[Epoch 2/200] [Batch 215/600] [D loss: 0.000075] [G loss: 10.185376]\n",
      "[Epoch 2/200] [Batch 216/600] [D loss: 0.000100] [G loss: 10.022127]\n",
      "[Epoch 2/200] [Batch 217/600] [D loss: 0.000093] [G loss: 9.882204]\n",
      "[Epoch 2/200] [Batch 218/600] [D loss: 0.000116] [G loss: 10.012389]\n",
      "[Epoch 2/200] [Batch 219/600] [D loss: 0.000068] [G loss: 9.949776]\n",
      "[Epoch 2/200] [Batch 220/600] [D loss: 0.000075] [G loss: 10.185771]\n",
      "[Epoch 2/200] [Batch 221/600] [D loss: 0.000066] [G loss: 10.267124]\n",
      "[Epoch 2/200] [Batch 222/600] [D loss: 0.000077] [G loss: 10.427238]\n",
      "[Epoch 2/200] [Batch 223/600] [D loss: 0.000073] [G loss: 9.992920]\n",
      "[Epoch 2/200] [Batch 224/600] [D loss: 0.000077] [G loss: 10.243877]\n",
      "[Epoch 2/200] [Batch 225/600] [D loss: 0.000077] [G loss: 10.301876]\n",
      "[Epoch 2/200] [Batch 226/600] [D loss: 0.000096] [G loss: 10.154832]\n",
      "[Epoch 2/200] [Batch 227/600] [D loss: 0.000124] [G loss: 9.894770]\n",
      "[Epoch 2/200] [Batch 228/600] [D loss: 0.000056] [G loss: 10.264096]\n",
      "[Epoch 2/200] [Batch 229/600] [D loss: 0.000094] [G loss: 10.062438]\n",
      "[Epoch 2/200] [Batch 230/600] [D loss: 0.000058] [G loss: 10.038168]\n",
      "[Epoch 2/200] [Batch 231/600] [D loss: 0.000067] [G loss: 10.023808]\n",
      "[Epoch 2/200] [Batch 232/600] [D loss: 0.000068] [G loss: 10.193136]\n",
      "[Epoch 2/200] [Batch 233/600] [D loss: 0.000057] [G loss: 9.949005]\n",
      "[Epoch 2/200] [Batch 234/600] [D loss: 0.000083] [G loss: 9.954069]\n",
      "[Epoch 2/200] [Batch 235/600] [D loss: 0.000070] [G loss: 10.248463]\n",
      "[Epoch 2/200] [Batch 236/600] [D loss: 0.000088] [G loss: 10.118977]\n",
      "[Epoch 2/200] [Batch 237/600] [D loss: 0.000084] [G loss: 10.175776]\n",
      "[Epoch 2/200] [Batch 238/600] [D loss: 0.000071] [G loss: 10.379810]\n",
      "[Epoch 2/200] [Batch 239/600] [D loss: 0.000078] [G loss: 10.292367]\n",
      "[Epoch 2/200] [Batch 240/600] [D loss: 0.000069] [G loss: 10.169651]\n",
      "[Epoch 2/200] [Batch 241/600] [D loss: 0.000107] [G loss: 10.074954]\n",
      "[Epoch 2/200] [Batch 242/600] [D loss: 0.000095] [G loss: 10.152500]\n",
      "[Epoch 2/200] [Batch 243/600] [D loss: 0.000096] [G loss: 10.148036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 244/600] [D loss: 0.000092] [G loss: 10.149184]\n",
      "[Epoch 2/200] [Batch 245/600] [D loss: 0.000073] [G loss: 10.070426]\n",
      "[Epoch 2/200] [Batch 246/600] [D loss: 0.000095] [G loss: 10.293147]\n",
      "[Epoch 2/200] [Batch 247/600] [D loss: 0.000069] [G loss: 10.126266]\n",
      "[Epoch 2/200] [Batch 248/600] [D loss: 0.000057] [G loss: 10.054312]\n",
      "[Epoch 2/200] [Batch 249/600] [D loss: 0.000069] [G loss: 10.090304]\n",
      "[Epoch 2/200] [Batch 250/600] [D loss: 0.000067] [G loss: 10.087935]\n",
      "[Epoch 2/200] [Batch 251/600] [D loss: 0.000076] [G loss: 10.337615]\n",
      "[Epoch 2/200] [Batch 252/600] [D loss: 0.000080] [G loss: 10.229342]\n",
      "[Epoch 2/200] [Batch 253/600] [D loss: 0.000072] [G loss: 10.083660]\n",
      "[Epoch 2/200] [Batch 254/600] [D loss: 0.000072] [G loss: 10.320019]\n",
      "[Epoch 2/200] [Batch 255/600] [D loss: 0.000077] [G loss: 10.210760]\n",
      "[Epoch 2/200] [Batch 256/600] [D loss: 0.000064] [G loss: 10.111988]\n",
      "[Epoch 2/200] [Batch 257/600] [D loss: 0.000068] [G loss: 9.916177]\n",
      "[Epoch 2/200] [Batch 258/600] [D loss: 0.000080] [G loss: 10.268665]\n",
      "[Epoch 2/200] [Batch 259/600] [D loss: 0.000073] [G loss: 9.982467]\n",
      "[Epoch 2/200] [Batch 260/600] [D loss: 0.000067] [G loss: 10.090116]\n",
      "[Epoch 2/200] [Batch 261/600] [D loss: 0.000069] [G loss: 9.967883]\n",
      "[Epoch 2/200] [Batch 262/600] [D loss: 0.000092] [G loss: 10.334198]\n",
      "[Epoch 2/200] [Batch 263/600] [D loss: 0.000048] [G loss: 10.323308]\n",
      "[Epoch 2/200] [Batch 264/600] [D loss: 0.000075] [G loss: 10.199221]\n",
      "[Epoch 2/200] [Batch 265/600] [D loss: 0.000082] [G loss: 10.363986]\n",
      "[Epoch 2/200] [Batch 266/600] [D loss: 0.000084] [G loss: 10.205560]\n",
      "[Epoch 2/200] [Batch 267/600] [D loss: 0.000077] [G loss: 10.203176]\n",
      "[Epoch 2/200] [Batch 268/600] [D loss: 0.000066] [G loss: 10.151870]\n",
      "[Epoch 2/200] [Batch 269/600] [D loss: 0.000074] [G loss: 10.359056]\n",
      "[Epoch 2/200] [Batch 270/600] [D loss: 0.000088] [G loss: 10.172939]\n",
      "[Epoch 2/200] [Batch 271/600] [D loss: 0.000088] [G loss: 10.200283]\n",
      "[Epoch 2/200] [Batch 272/600] [D loss: 0.000076] [G loss: 10.441367]\n",
      "[Epoch 2/200] [Batch 273/600] [D loss: 0.000097] [G loss: 10.175534]\n",
      "[Epoch 2/200] [Batch 274/600] [D loss: 0.000070] [G loss: 10.049154]\n",
      "[Epoch 2/200] [Batch 275/600] [D loss: 0.000084] [G loss: 10.101858]\n",
      "[Epoch 2/200] [Batch 276/600] [D loss: 0.000086] [G loss: 10.064564]\n",
      "[Epoch 2/200] [Batch 277/600] [D loss: 0.000082] [G loss: 10.384887]\n",
      "[Epoch 2/200] [Batch 278/600] [D loss: 0.000084] [G loss: 10.371129]\n",
      "[Epoch 2/200] [Batch 279/600] [D loss: 0.000058] [G loss: 10.126807]\n",
      "[Epoch 2/200] [Batch 280/600] [D loss: 0.000084] [G loss: 10.320073]\n",
      "[Epoch 2/200] [Batch 281/600] [D loss: 0.000097] [G loss: 10.216400]\n",
      "[Epoch 2/200] [Batch 282/600] [D loss: 0.000073] [G loss: 10.207718]\n",
      "[Epoch 2/200] [Batch 283/600] [D loss: 0.000060] [G loss: 10.043812]\n",
      "[Epoch 2/200] [Batch 284/600] [D loss: 0.000075] [G loss: 9.960289]\n",
      "[Epoch 2/200] [Batch 285/600] [D loss: 0.000067] [G loss: 10.284289]\n",
      "[Epoch 2/200] [Batch 286/600] [D loss: 0.000105] [G loss: 10.312901]\n",
      "[Epoch 2/200] [Batch 287/600] [D loss: 0.000072] [G loss: 10.164692]\n",
      "[Epoch 2/200] [Batch 288/600] [D loss: 0.000066] [G loss: 9.888340]\n",
      "[Epoch 2/200] [Batch 289/600] [D loss: 0.000087] [G loss: 10.463972]\n",
      "[Epoch 2/200] [Batch 290/600] [D loss: 0.000083] [G loss: 10.202542]\n",
      "[Epoch 2/200] [Batch 291/600] [D loss: 0.000081] [G loss: 10.232340]\n",
      "[Epoch 2/200] [Batch 292/600] [D loss: 0.000057] [G loss: 10.439650]\n",
      "[Epoch 2/200] [Batch 293/600] [D loss: 0.000075] [G loss: 10.206738]\n",
      "[Epoch 2/200] [Batch 294/600] [D loss: 0.000086] [G loss: 10.472568]\n",
      "[Epoch 2/200] [Batch 295/600] [D loss: 0.000076] [G loss: 10.281890]\n",
      "[Epoch 2/200] [Batch 296/600] [D loss: 0.000070] [G loss: 10.346928]\n",
      "[Epoch 2/200] [Batch 297/600] [D loss: 0.000071] [G loss: 10.131702]\n",
      "[Epoch 2/200] [Batch 298/600] [D loss: 0.000091] [G loss: 10.602167]\n",
      "[Epoch 2/200] [Batch 299/600] [D loss: 0.000061] [G loss: 9.972476]\n",
      "[Epoch 2/200] [Batch 300/600] [D loss: 0.000061] [G loss: 10.666570]\n",
      "[Epoch 2/200] [Batch 301/600] [D loss: 0.000105] [G loss: 10.212817]\n",
      "[Epoch 2/200] [Batch 302/600] [D loss: 0.000070] [G loss: 10.153391]\n",
      "[Epoch 2/200] [Batch 303/600] [D loss: 0.000081] [G loss: 9.997119]\n",
      "[Epoch 2/200] [Batch 304/600] [D loss: 0.000074] [G loss: 10.113023]\n",
      "[Epoch 2/200] [Batch 305/600] [D loss: 0.000082] [G loss: 10.265137]\n",
      "[Epoch 2/200] [Batch 306/600] [D loss: 0.000064] [G loss: 10.166339]\n",
      "[Epoch 2/200] [Batch 307/600] [D loss: 0.000066] [G loss: 10.254085]\n",
      "[Epoch 2/200] [Batch 308/600] [D loss: 0.000085] [G loss: 10.323911]\n",
      "[Epoch 2/200] [Batch 309/600] [D loss: 0.000053] [G loss: 10.276812]\n",
      "[Epoch 2/200] [Batch 310/600] [D loss: 0.000073] [G loss: 10.518159]\n",
      "[Epoch 2/200] [Batch 311/600] [D loss: 0.000084] [G loss: 10.447821]\n",
      "[Epoch 2/200] [Batch 312/600] [D loss: 0.000091] [G loss: 10.307420]\n",
      "[Epoch 2/200] [Batch 313/600] [D loss: 0.000063] [G loss: 10.443867]\n",
      "[Epoch 2/200] [Batch 314/600] [D loss: 0.000048] [G loss: 10.349320]\n",
      "[Epoch 2/200] [Batch 315/600] [D loss: 0.000088] [G loss: 10.397660]\n",
      "[Epoch 2/200] [Batch 316/600] [D loss: 0.000074] [G loss: 10.309009]\n",
      "[Epoch 2/200] [Batch 317/600] [D loss: 0.000077] [G loss: 10.414459]\n",
      "[Epoch 2/200] [Batch 318/600] [D loss: 0.000076] [G loss: 10.290277]\n",
      "[Epoch 2/200] [Batch 319/600] [D loss: 0.000065] [G loss: 10.483779]\n",
      "[Epoch 2/200] [Batch 320/600] [D loss: 0.000062] [G loss: 10.231514]\n",
      "[Epoch 2/200] [Batch 321/600] [D loss: 0.000076] [G loss: 10.133015]\n",
      "[Epoch 2/200] [Batch 322/600] [D loss: 0.000050] [G loss: 10.422970]\n",
      "[Epoch 2/200] [Batch 323/600] [D loss: 0.000066] [G loss: 10.194001]\n",
      "[Epoch 2/200] [Batch 324/600] [D loss: 0.000055] [G loss: 10.376841]\n",
      "[Epoch 2/200] [Batch 325/600] [D loss: 0.000063] [G loss: 10.482396]\n",
      "[Epoch 2/200] [Batch 326/600] [D loss: 0.000096] [G loss: 10.360496]\n",
      "[Epoch 2/200] [Batch 327/600] [D loss: 0.000067] [G loss: 10.546949]\n",
      "[Epoch 2/200] [Batch 328/600] [D loss: 0.000072] [G loss: 10.345054]\n",
      "[Epoch 2/200] [Batch 329/600] [D loss: 0.000078] [G loss: 10.479502]\n",
      "[Epoch 2/200] [Batch 330/600] [D loss: 0.000064] [G loss: 10.192088]\n",
      "[Epoch 2/200] [Batch 331/600] [D loss: 0.000065] [G loss: 10.143867]\n",
      "[Epoch 2/200] [Batch 332/600] [D loss: 0.000062] [G loss: 10.250768]\n",
      "[Epoch 2/200] [Batch 333/600] [D loss: 0.000054] [G loss: 10.096165]\n",
      "[Epoch 2/200] [Batch 334/600] [D loss: 0.000073] [G loss: 10.319370]\n",
      "[Epoch 2/200] [Batch 335/600] [D loss: 0.000059] [G loss: 10.335884]\n",
      "[Epoch 2/200] [Batch 336/600] [D loss: 0.000062] [G loss: 10.347018]\n",
      "[Epoch 2/200] [Batch 337/600] [D loss: 0.000057] [G loss: 10.287049]\n",
      "[Epoch 2/200] [Batch 338/600] [D loss: 0.000058] [G loss: 10.228529]\n",
      "[Epoch 2/200] [Batch 339/600] [D loss: 0.000096] [G loss: 10.049632]\n",
      "[Epoch 2/200] [Batch 340/600] [D loss: 0.000070] [G loss: 10.510201]\n",
      "[Epoch 2/200] [Batch 341/600] [D loss: 0.000062] [G loss: 10.257020]\n",
      "[Epoch 2/200] [Batch 342/600] [D loss: 0.000090] [G loss: 10.379508]\n",
      "[Epoch 2/200] [Batch 343/600] [D loss: 0.000055] [G loss: 10.392764]\n",
      "[Epoch 2/200] [Batch 344/600] [D loss: 0.000084] [G loss: 10.217361]\n",
      "[Epoch 2/200] [Batch 345/600] [D loss: 0.000115] [G loss: 10.393986]\n",
      "[Epoch 2/200] [Batch 346/600] [D loss: 0.000056] [G loss: 10.326747]\n",
      "[Epoch 2/200] [Batch 347/600] [D loss: 0.000091] [G loss: 10.182198]\n",
      "[Epoch 2/200] [Batch 348/600] [D loss: 0.000061] [G loss: 10.525653]\n",
      "[Epoch 2/200] [Batch 349/600] [D loss: 0.000079] [G loss: 10.410087]\n",
      "[Epoch 2/200] [Batch 350/600] [D loss: 0.000076] [G loss: 10.159264]\n",
      "[Epoch 2/200] [Batch 351/600] [D loss: 0.000055] [G loss: 10.488784]\n",
      "[Epoch 2/200] [Batch 352/600] [D loss: 0.000075] [G loss: 10.116396]\n",
      "[Epoch 2/200] [Batch 353/600] [D loss: 0.000067] [G loss: 10.260104]\n",
      "[Epoch 2/200] [Batch 354/600] [D loss: 0.000057] [G loss: 10.713174]\n",
      "[Epoch 2/200] [Batch 355/600] [D loss: 0.000067] [G loss: 10.198803]\n",
      "[Epoch 2/200] [Batch 356/600] [D loss: 0.000056] [G loss: 10.465228]\n",
      "[Epoch 2/200] [Batch 357/600] [D loss: 0.000072] [G loss: 10.599702]\n",
      "[Epoch 2/200] [Batch 358/600] [D loss: 0.000065] [G loss: 9.966037]\n",
      "[Epoch 2/200] [Batch 359/600] [D loss: 0.000072] [G loss: 10.311653]\n",
      "[Epoch 2/200] [Batch 360/600] [D loss: 0.000056] [G loss: 10.075491]\n",
      "[Epoch 2/200] [Batch 361/600] [D loss: 0.000057] [G loss: 10.498693]\n",
      "[Epoch 2/200] [Batch 362/600] [D loss: 0.000056] [G loss: 10.530412]\n",
      "[Epoch 2/200] [Batch 363/600] [D loss: 0.000051] [G loss: 10.253527]\n",
      "[Epoch 2/200] [Batch 364/600] [D loss: 0.000064] [G loss: 10.136085]\n",
      "[Epoch 2/200] [Batch 365/600] [D loss: 0.000149] [G loss: 10.447631]\n",
      "[Epoch 2/200] [Batch 366/600] [D loss: 0.000072] [G loss: 10.658112]\n",
      "[Epoch 2/200] [Batch 367/600] [D loss: 0.000050] [G loss: 10.284209]\n",
      "[Epoch 2/200] [Batch 368/600] [D loss: 0.000085] [G loss: 10.504969]\n",
      "[Epoch 2/200] [Batch 369/600] [D loss: 0.000087] [G loss: 10.397244]\n",
      "[Epoch 2/200] [Batch 370/600] [D loss: 0.000060] [G loss: 10.352365]\n",
      "[Epoch 2/200] [Batch 371/600] [D loss: 0.000073] [G loss: 10.252197]\n",
      "[Epoch 2/200] [Batch 372/600] [D loss: 0.000090] [G loss: 10.360988]\n",
      "[Epoch 2/200] [Batch 373/600] [D loss: 0.000053] [G loss: 10.111605]\n",
      "[Epoch 2/200] [Batch 374/600] [D loss: 0.000083] [G loss: 10.290546]\n",
      "[Epoch 2/200] [Batch 375/600] [D loss: 0.000072] [G loss: 10.195401]\n",
      "[Epoch 2/200] [Batch 376/600] [D loss: 0.000051] [G loss: 10.570482]\n",
      "[Epoch 2/200] [Batch 377/600] [D loss: 0.000077] [G loss: 10.338146]\n",
      "[Epoch 2/200] [Batch 378/600] [D loss: 0.000075] [G loss: 10.378417]\n",
      "[Epoch 2/200] [Batch 379/600] [D loss: 0.000063] [G loss: 10.575178]\n",
      "[Epoch 2/200] [Batch 380/600] [D loss: 0.000055] [G loss: 10.422194]\n",
      "[Epoch 2/200] [Batch 381/600] [D loss: 0.000073] [G loss: 10.488072]\n",
      "[Epoch 2/200] [Batch 382/600] [D loss: 0.000066] [G loss: 10.312451]\n",
      "[Epoch 2/200] [Batch 383/600] [D loss: 0.000055] [G loss: 10.410854]\n",
      "[Epoch 2/200] [Batch 384/600] [D loss: 0.000062] [G loss: 10.341636]\n",
      "[Epoch 2/200] [Batch 385/600] [D loss: 0.000057] [G loss: 10.345112]\n",
      "[Epoch 2/200] [Batch 386/600] [D loss: 0.000069] [G loss: 10.388013]\n",
      "[Epoch 2/200] [Batch 387/600] [D loss: 0.000059] [G loss: 10.571240]\n",
      "[Epoch 2/200] [Batch 388/600] [D loss: 0.000064] [G loss: 10.357278]\n",
      "[Epoch 2/200] [Batch 389/600] [D loss: 0.000066] [G loss: 10.323586]\n",
      "[Epoch 2/200] [Batch 390/600] [D loss: 0.000061] [G loss: 10.357409]\n",
      "[Epoch 2/200] [Batch 391/600] [D loss: 0.000078] [G loss: 10.317583]\n",
      "[Epoch 2/200] [Batch 392/600] [D loss: 0.000052] [G loss: 10.619146]\n",
      "[Epoch 2/200] [Batch 393/600] [D loss: 0.000060] [G loss: 10.025612]\n",
      "[Epoch 2/200] [Batch 394/600] [D loss: 0.000053] [G loss: 10.447335]\n",
      "[Epoch 2/200] [Batch 395/600] [D loss: 0.000067] [G loss: 10.344702]\n",
      "[Epoch 2/200] [Batch 396/600] [D loss: 0.000056] [G loss: 10.335237]\n",
      "[Epoch 2/200] [Batch 397/600] [D loss: 0.000063] [G loss: 10.554569]\n",
      "[Epoch 2/200] [Batch 398/600] [D loss: 0.000057] [G loss: 10.469943]\n",
      "[Epoch 2/200] [Batch 399/600] [D loss: 0.000059] [G loss: 10.350721]\n",
      "[Epoch 2/200] [Batch 400/600] [D loss: 0.000063] [G loss: 10.418331]\n",
      "[Epoch 2/200] [Batch 401/600] [D loss: 0.000080] [G loss: 10.711750]\n",
      "[Epoch 2/200] [Batch 402/600] [D loss: 0.000058] [G loss: 10.459747]\n",
      "[Epoch 2/200] [Batch 403/600] [D loss: 0.000057] [G loss: 10.317763]\n",
      "[Epoch 2/200] [Batch 404/600] [D loss: 0.000055] [G loss: 10.449150]\n",
      "[Epoch 2/200] [Batch 405/600] [D loss: 0.000059] [G loss: 10.477399]\n",
      "[Epoch 2/200] [Batch 406/600] [D loss: 0.000076] [G loss: 10.250990]\n",
      "[Epoch 2/200] [Batch 407/600] [D loss: 0.000049] [G loss: 10.613459]\n",
      "[Epoch 2/200] [Batch 408/600] [D loss: 0.000067] [G loss: 10.489514]\n",
      "[Epoch 2/200] [Batch 409/600] [D loss: 0.000074] [G loss: 10.457944]\n",
      "[Epoch 2/200] [Batch 410/600] [D loss: 0.000053] [G loss: 10.486991]\n",
      "[Epoch 2/200] [Batch 411/600] [D loss: 0.000072] [G loss: 10.417954]\n",
      "[Epoch 2/200] [Batch 412/600] [D loss: 0.000049] [G loss: 10.442440]\n",
      "[Epoch 2/200] [Batch 413/600] [D loss: 0.000096] [G loss: 10.626676]\n",
      "[Epoch 2/200] [Batch 414/600] [D loss: 0.000052] [G loss: 10.567059]\n",
      "[Epoch 2/200] [Batch 415/600] [D loss: 0.000048] [G loss: 10.380615]\n",
      "[Epoch 2/200] [Batch 416/600] [D loss: 0.000072] [G loss: 10.476964]\n",
      "[Epoch 2/200] [Batch 417/600] [D loss: 0.000066] [G loss: 10.179798]\n",
      "[Epoch 2/200] [Batch 418/600] [D loss: 0.000055] [G loss: 10.660595]\n",
      "[Epoch 2/200] [Batch 419/600] [D loss: 0.000072] [G loss: 10.309188]\n",
      "[Epoch 2/200] [Batch 420/600] [D loss: 0.000066] [G loss: 10.541394]\n",
      "[Epoch 2/200] [Batch 421/600] [D loss: 0.000043] [G loss: 10.315518]\n",
      "[Epoch 2/200] [Batch 422/600] [D loss: 0.000055] [G loss: 10.669167]\n",
      "[Epoch 2/200] [Batch 423/600] [D loss: 0.000058] [G loss: 10.707163]\n",
      "[Epoch 2/200] [Batch 424/600] [D loss: 0.000060] [G loss: 10.622851]\n",
      "[Epoch 2/200] [Batch 425/600] [D loss: 0.000056] [G loss: 10.420380]\n",
      "[Epoch 2/200] [Batch 426/600] [D loss: 0.000075] [G loss: 10.277527]\n",
      "[Epoch 2/200] [Batch 427/600] [D loss: 0.000072] [G loss: 10.044441]\n",
      "[Epoch 2/200] [Batch 428/600] [D loss: 0.000069] [G loss: 10.678329]\n",
      "[Epoch 2/200] [Batch 429/600] [D loss: 0.000049] [G loss: 10.669949]\n",
      "[Epoch 2/200] [Batch 430/600] [D loss: 0.000069] [G loss: 10.610682]\n",
      "[Epoch 2/200] [Batch 431/600] [D loss: 0.000073] [G loss: 10.360622]\n",
      "[Epoch 2/200] [Batch 432/600] [D loss: 0.000076] [G loss: 10.687262]\n",
      "[Epoch 2/200] [Batch 433/600] [D loss: 0.000070] [G loss: 10.443601]\n",
      "[Epoch 2/200] [Batch 434/600] [D loss: 0.000060] [G loss: 10.623362]\n",
      "[Epoch 2/200] [Batch 435/600] [D loss: 0.000075] [G loss: 10.499469]\n",
      "[Epoch 2/200] [Batch 436/600] [D loss: 0.000068] [G loss: 10.575929]\n",
      "[Epoch 2/200] [Batch 437/600] [D loss: 0.000057] [G loss: 10.803341]\n",
      "[Epoch 2/200] [Batch 438/600] [D loss: 0.000069] [G loss: 10.300449]\n",
      "[Epoch 2/200] [Batch 439/600] [D loss: 0.000069] [G loss: 10.427760]\n",
      "[Epoch 2/200] [Batch 440/600] [D loss: 0.000069] [G loss: 10.549068]\n",
      "[Epoch 2/200] [Batch 441/600] [D loss: 0.000072] [G loss: 10.364047]\n",
      "[Epoch 2/200] [Batch 442/600] [D loss: 0.000072] [G loss: 10.383771]\n",
      "[Epoch 2/200] [Batch 443/600] [D loss: 0.000059] [G loss: 10.692635]\n",
      "[Epoch 2/200] [Batch 444/600] [D loss: 0.000048] [G loss: 10.517867]\n",
      "[Epoch 2/200] [Batch 445/600] [D loss: 0.000071] [G loss: 10.548594]\n",
      "[Epoch 2/200] [Batch 446/600] [D loss: 0.000075] [G loss: 10.627450]\n",
      "[Epoch 2/200] [Batch 447/600] [D loss: 0.000058] [G loss: 10.452184]\n",
      "[Epoch 2/200] [Batch 448/600] [D loss: 0.000051] [G loss: 10.637799]\n",
      "[Epoch 2/200] [Batch 449/600] [D loss: 0.000064] [G loss: 10.335526]\n",
      "[Epoch 2/200] [Batch 450/600] [D loss: 0.000066] [G loss: 10.675815]\n",
      "[Epoch 2/200] [Batch 451/600] [D loss: 0.000055] [G loss: 10.304070]\n",
      "[Epoch 2/200] [Batch 452/600] [D loss: 0.000071] [G loss: 10.380457]\n",
      "[Epoch 2/200] [Batch 453/600] [D loss: 0.000047] [G loss: 10.382337]\n",
      "[Epoch 2/200] [Batch 454/600] [D loss: 0.000093] [G loss: 10.427817]\n",
      "[Epoch 2/200] [Batch 455/600] [D loss: 0.000055] [G loss: 10.674147]\n",
      "[Epoch 2/200] [Batch 456/600] [D loss: 0.000071] [G loss: 10.529943]\n",
      "[Epoch 2/200] [Batch 457/600] [D loss: 0.000072] [G loss: 10.491716]\n",
      "[Epoch 2/200] [Batch 458/600] [D loss: 0.000066] [G loss: 10.606146]\n",
      "[Epoch 2/200] [Batch 459/600] [D loss: 0.000052] [G loss: 10.420711]\n",
      "[Epoch 2/200] [Batch 460/600] [D loss: 0.000056] [G loss: 10.485157]\n",
      "[Epoch 2/200] [Batch 461/600] [D loss: 0.000063] [G loss: 10.830763]\n",
      "[Epoch 2/200] [Batch 462/600] [D loss: 0.000062] [G loss: 10.620707]\n",
      "[Epoch 2/200] [Batch 463/600] [D loss: 0.000068] [G loss: 10.625873]\n",
      "[Epoch 2/200] [Batch 464/600] [D loss: 0.000075] [G loss: 10.505614]\n",
      "[Epoch 2/200] [Batch 465/600] [D loss: 0.000065] [G loss: 10.676966]\n",
      "[Epoch 2/200] [Batch 466/600] [D loss: 0.000059] [G loss: 10.406866]\n",
      "[Epoch 2/200] [Batch 467/600] [D loss: 0.000073] [G loss: 10.700988]\n",
      "[Epoch 2/200] [Batch 468/600] [D loss: 0.000039] [G loss: 10.355150]\n",
      "[Epoch 2/200] [Batch 469/600] [D loss: 0.000053] [G loss: 10.678348]\n",
      "[Epoch 2/200] [Batch 470/600] [D loss: 0.000043] [G loss: 10.489045]\n",
      "[Epoch 2/200] [Batch 471/600] [D loss: 0.000054] [G loss: 10.586673]\n",
      "[Epoch 2/200] [Batch 472/600] [D loss: 0.000051] [G loss: 10.488003]\n",
      "[Epoch 2/200] [Batch 473/600] [D loss: 0.000047] [G loss: 10.670851]\n",
      "[Epoch 2/200] [Batch 474/600] [D loss: 0.000048] [G loss: 10.394526]\n",
      "[Epoch 2/200] [Batch 475/600] [D loss: 0.000052] [G loss: 10.376513]\n",
      "[Epoch 2/200] [Batch 476/600] [D loss: 0.000046] [G loss: 10.446276]\n",
      "[Epoch 2/200] [Batch 477/600] [D loss: 0.000051] [G loss: 10.405156]\n",
      "[Epoch 2/200] [Batch 478/600] [D loss: 0.000102] [G loss: 10.773589]\n",
      "[Epoch 2/200] [Batch 479/600] [D loss: 0.000050] [G loss: 10.709021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 480/600] [D loss: 0.000069] [G loss: 10.514335]\n",
      "[Epoch 2/200] [Batch 481/600] [D loss: 0.000045] [G loss: 10.533490]\n",
      "[Epoch 2/200] [Batch 482/600] [D loss: 0.000057] [G loss: 10.552097]\n",
      "[Epoch 2/200] [Batch 483/600] [D loss: 0.000051] [G loss: 10.603416]\n",
      "[Epoch 2/200] [Batch 484/600] [D loss: 0.000074] [G loss: 10.296222]\n",
      "[Epoch 2/200] [Batch 485/600] [D loss: 0.000055] [G loss: 10.656799]\n",
      "[Epoch 2/200] [Batch 486/600] [D loss: 0.000055] [G loss: 10.632102]\n",
      "[Epoch 2/200] [Batch 487/600] [D loss: 0.000064] [G loss: 10.545350]\n",
      "[Epoch 2/200] [Batch 488/600] [D loss: 0.000052] [G loss: 10.526454]\n",
      "[Epoch 2/200] [Batch 489/600] [D loss: 0.000068] [G loss: 10.568652]\n",
      "[Epoch 2/200] [Batch 490/600] [D loss: 0.000058] [G loss: 10.583229]\n",
      "[Epoch 2/200] [Batch 491/600] [D loss: 0.000063] [G loss: 10.420376]\n",
      "[Epoch 2/200] [Batch 492/600] [D loss: 0.000049] [G loss: 10.622395]\n",
      "[Epoch 2/200] [Batch 493/600] [D loss: 0.000051] [G loss: 10.746239]\n",
      "[Epoch 2/200] [Batch 494/600] [D loss: 0.000050] [G loss: 10.360263]\n",
      "[Epoch 2/200] [Batch 495/600] [D loss: 0.000043] [G loss: 10.670621]\n",
      "[Epoch 2/200] [Batch 496/600] [D loss: 0.000043] [G loss: 10.847079]\n",
      "[Epoch 2/200] [Batch 497/600] [D loss: 0.000061] [G loss: 10.579548]\n",
      "[Epoch 2/200] [Batch 498/600] [D loss: 0.000064] [G loss: 10.395280]\n",
      "[Epoch 2/200] [Batch 499/600] [D loss: 0.000077] [G loss: 10.383790]\n",
      "[Epoch 2/200] [Batch 500/600] [D loss: 0.000054] [G loss: 10.361202]\n",
      "[Epoch 2/200] [Batch 501/600] [D loss: 0.000070] [G loss: 10.637904]\n",
      "[Epoch 2/200] [Batch 502/600] [D loss: 0.000067] [G loss: 10.307549]\n",
      "[Epoch 2/200] [Batch 503/600] [D loss: 0.000038] [G loss: 10.438241]\n",
      "[Epoch 2/200] [Batch 504/600] [D loss: 0.000042] [G loss: 10.821098]\n",
      "[Epoch 2/200] [Batch 505/600] [D loss: 0.000048] [G loss: 10.546548]\n",
      "[Epoch 2/200] [Batch 506/600] [D loss: 0.000053] [G loss: 10.346555]\n",
      "[Epoch 2/200] [Batch 507/600] [D loss: 0.000056] [G loss: 10.643728]\n",
      "[Epoch 2/200] [Batch 508/600] [D loss: 0.000056] [G loss: 10.284155]\n",
      "[Epoch 2/200] [Batch 509/600] [D loss: 0.000047] [G loss: 10.889288]\n",
      "[Epoch 2/200] [Batch 510/600] [D loss: 0.000054] [G loss: 10.612187]\n",
      "[Epoch 2/200] [Batch 511/600] [D loss: 0.000059] [G loss: 10.456425]\n",
      "[Epoch 2/200] [Batch 512/600] [D loss: 0.000055] [G loss: 10.675080]\n",
      "[Epoch 2/200] [Batch 513/600] [D loss: 0.000070] [G loss: 10.747811]\n",
      "[Epoch 2/200] [Batch 514/600] [D loss: 0.000094] [G loss: 10.527888]\n",
      "[Epoch 2/200] [Batch 515/600] [D loss: 0.000077] [G loss: 10.490602]\n",
      "[Epoch 2/200] [Batch 516/600] [D loss: 0.000044] [G loss: 10.580957]\n",
      "[Epoch 2/200] [Batch 517/600] [D loss: 0.000050] [G loss: 10.476637]\n",
      "[Epoch 2/200] [Batch 518/600] [D loss: 0.000053] [G loss: 10.676628]\n",
      "[Epoch 2/200] [Batch 519/600] [D loss: 0.000040] [G loss: 10.567822]\n",
      "[Epoch 2/200] [Batch 520/600] [D loss: 0.000047] [G loss: 10.713491]\n",
      "[Epoch 2/200] [Batch 521/600] [D loss: 0.000043] [G loss: 10.581188]\n",
      "[Epoch 2/200] [Batch 522/600] [D loss: 0.000043] [G loss: 10.319297]\n",
      "[Epoch 2/200] [Batch 523/600] [D loss: 0.000051] [G loss: 10.508052]\n",
      "[Epoch 2/200] [Batch 524/600] [D loss: 0.000050] [G loss: 10.400579]\n",
      "[Epoch 2/200] [Batch 525/600] [D loss: 0.000045] [G loss: 10.573206]\n",
      "[Epoch 2/200] [Batch 526/600] [D loss: 0.000047] [G loss: 10.665350]\n",
      "[Epoch 2/200] [Batch 527/600] [D loss: 0.000052] [G loss: 10.629264]\n",
      "[Epoch 2/200] [Batch 528/600] [D loss: 0.000058] [G loss: 10.384321]\n",
      "[Epoch 2/200] [Batch 529/600] [D loss: 0.000047] [G loss: 10.567816]\n",
      "[Epoch 2/200] [Batch 530/600] [D loss: 0.000060] [G loss: 10.442325]\n",
      "[Epoch 2/200] [Batch 531/600] [D loss: 0.000042] [G loss: 10.622268]\n",
      "[Epoch 2/200] [Batch 532/600] [D loss: 0.000068] [G loss: 10.577834]\n",
      "[Epoch 2/200] [Batch 533/600] [D loss: 0.000054] [G loss: 10.458147]\n",
      "[Epoch 2/200] [Batch 534/600] [D loss: 0.000045] [G loss: 10.743988]\n",
      "[Epoch 2/200] [Batch 535/600] [D loss: 0.000044] [G loss: 10.493796]\n",
      "[Epoch 2/200] [Batch 536/600] [D loss: 0.000051] [G loss: 10.439192]\n",
      "[Epoch 2/200] [Batch 537/600] [D loss: 0.000044] [G loss: 10.440748]\n",
      "[Epoch 2/200] [Batch 538/600] [D loss: 0.000045] [G loss: 10.572722]\n",
      "[Epoch 2/200] [Batch 539/600] [D loss: 0.000046] [G loss: 10.560790]\n",
      "[Epoch 2/200] [Batch 540/600] [D loss: 0.000044] [G loss: 10.380642]\n",
      "[Epoch 2/200] [Batch 541/600] [D loss: 0.000052] [G loss: 10.700148]\n",
      "[Epoch 2/200] [Batch 542/600] [D loss: 0.000055] [G loss: 10.373753]\n",
      "[Epoch 2/200] [Batch 543/600] [D loss: 0.000033] [G loss: 10.625503]\n",
      "[Epoch 2/200] [Batch 544/600] [D loss: 0.000056] [G loss: 10.747278]\n",
      "[Epoch 2/200] [Batch 545/600] [D loss: 0.000048] [G loss: 10.571559]\n",
      "[Epoch 2/200] [Batch 546/600] [D loss: 0.000055] [G loss: 10.445863]\n",
      "[Epoch 2/200] [Batch 547/600] [D loss: 0.000071] [G loss: 10.579148]\n",
      "[Epoch 2/200] [Batch 548/600] [D loss: 0.000057] [G loss: 10.678261]\n",
      "[Epoch 2/200] [Batch 549/600] [D loss: 0.000049] [G loss: 10.728816]\n",
      "[Epoch 2/200] [Batch 550/600] [D loss: 0.000055] [G loss: 10.726455]\n",
      "[Epoch 2/200] [Batch 551/600] [D loss: 0.000044] [G loss: 10.905133]\n",
      "[Epoch 2/200] [Batch 552/600] [D loss: 0.000044] [G loss: 10.596684]\n",
      "[Epoch 2/200] [Batch 553/600] [D loss: 0.000047] [G loss: 10.917747]\n",
      "[Epoch 2/200] [Batch 554/600] [D loss: 0.000046] [G loss: 10.592628]\n",
      "[Epoch 2/200] [Batch 555/600] [D loss: 0.000066] [G loss: 10.662304]\n",
      "[Epoch 2/200] [Batch 556/600] [D loss: 0.000050] [G loss: 10.752359]\n",
      "[Epoch 2/200] [Batch 557/600] [D loss: 0.000053] [G loss: 10.768846]\n",
      "[Epoch 2/200] [Batch 558/600] [D loss: 0.000046] [G loss: 10.599836]\n",
      "[Epoch 2/200] [Batch 559/600] [D loss: 0.000062] [G loss: 10.620142]\n",
      "[Epoch 2/200] [Batch 560/600] [D loss: 0.000048] [G loss: 10.673248]\n",
      "[Epoch 2/200] [Batch 561/600] [D loss: 0.000056] [G loss: 10.493373]\n",
      "[Epoch 2/200] [Batch 562/600] [D loss: 0.000056] [G loss: 10.455572]\n",
      "[Epoch 2/200] [Batch 563/600] [D loss: 0.000061] [G loss: 10.393085]\n",
      "[Epoch 2/200] [Batch 564/600] [D loss: 0.000082] [G loss: 10.585176]\n",
      "[Epoch 2/200] [Batch 565/600] [D loss: 0.000057] [G loss: 10.561689]\n",
      "[Epoch 2/200] [Batch 566/600] [D loss: 0.000042] [G loss: 10.502059]\n",
      "[Epoch 2/200] [Batch 567/600] [D loss: 0.000043] [G loss: 10.542175]\n",
      "[Epoch 2/200] [Batch 568/600] [D loss: 0.000056] [G loss: 10.775121]\n",
      "[Epoch 2/200] [Batch 569/600] [D loss: 0.000036] [G loss: 10.614820]\n",
      "[Epoch 2/200] [Batch 570/600] [D loss: 0.000046] [G loss: 10.686322]\n",
      "[Epoch 2/200] [Batch 571/600] [D loss: 0.000055] [G loss: 11.000736]\n",
      "[Epoch 2/200] [Batch 572/600] [D loss: 0.000055] [G loss: 10.917128]\n",
      "[Epoch 2/200] [Batch 573/600] [D loss: 0.000059] [G loss: 10.526144]\n",
      "[Epoch 2/200] [Batch 574/600] [D loss: 0.000049] [G loss: 10.639987]\n",
      "[Epoch 2/200] [Batch 575/600] [D loss: 0.000050] [G loss: 10.522883]\n",
      "[Epoch 2/200] [Batch 576/600] [D loss: 0.000045] [G loss: 11.020855]\n",
      "[Epoch 2/200] [Batch 577/600] [D loss: 0.000078] [G loss: 10.585406]\n",
      "[Epoch 2/200] [Batch 578/600] [D loss: 0.000069] [G loss: 10.595057]\n",
      "[Epoch 2/200] [Batch 579/600] [D loss: 0.000052] [G loss: 10.574945]\n",
      "[Epoch 2/200] [Batch 580/600] [D loss: 0.000042] [G loss: 10.743226]\n",
      "[Epoch 2/200] [Batch 581/600] [D loss: 0.000050] [G loss: 10.661818]\n",
      "[Epoch 2/200] [Batch 582/600] [D loss: 0.000044] [G loss: 10.706843]\n",
      "[Epoch 2/200] [Batch 583/600] [D loss: 0.000045] [G loss: 10.575763]\n",
      "[Epoch 2/200] [Batch 584/600] [D loss: 0.000051] [G loss: 10.878196]\n",
      "[Epoch 2/200] [Batch 585/600] [D loss: 0.000047] [G loss: 10.629169]\n",
      "[Epoch 2/200] [Batch 586/600] [D loss: 0.000040] [G loss: 10.851026]\n",
      "[Epoch 2/200] [Batch 587/600] [D loss: 0.000062] [G loss: 10.884302]\n",
      "[Epoch 2/200] [Batch 588/600] [D loss: 0.000040] [G loss: 10.891623]\n",
      "[Epoch 2/200] [Batch 589/600] [D loss: 0.000054] [G loss: 10.799557]\n",
      "[Epoch 2/200] [Batch 590/600] [D loss: 0.000045] [G loss: 10.515716]\n",
      "[Epoch 2/200] [Batch 591/600] [D loss: 0.000049] [G loss: 10.716569]\n",
      "[Epoch 2/200] [Batch 592/600] [D loss: 0.000045] [G loss: 10.661563]\n",
      "[Epoch 2/200] [Batch 593/600] [D loss: 0.000049] [G loss: 10.791577]\n",
      "[Epoch 2/200] [Batch 594/600] [D loss: 0.000045] [G loss: 10.849566]\n",
      "[Epoch 2/200] [Batch 595/600] [D loss: 0.000058] [G loss: 10.743826]\n",
      "[Epoch 2/200] [Batch 596/600] [D loss: 0.000049] [G loss: 10.522837]\n",
      "[Epoch 2/200] [Batch 597/600] [D loss: 0.000048] [G loss: 10.885686]\n",
      "[Epoch 2/200] [Batch 598/600] [D loss: 0.000062] [G loss: 10.922700]\n",
      "[Epoch 2/200] [Batch 599/600] [D loss: 0.000058] [G loss: 10.700349]\n",
      "[Epoch 3/200] [Batch 0/600] [D loss: 0.000056] [G loss: 11.063350]\n",
      "[Epoch 3/200] [Batch 1/600] [D loss: 0.000036] [G loss: 10.789375]\n",
      "[Epoch 3/200] [Batch 2/600] [D loss: 0.000052] [G loss: 10.933690]\n",
      "[Epoch 3/200] [Batch 3/600] [D loss: 0.000046] [G loss: 10.970175]\n",
      "[Epoch 3/200] [Batch 4/600] [D loss: 0.000047] [G loss: 10.884767]\n",
      "[Epoch 3/200] [Batch 5/600] [D loss: 0.000036] [G loss: 10.603019]\n",
      "[Epoch 3/200] [Batch 6/600] [D loss: 0.000054] [G loss: 10.740718]\n",
      "[Epoch 3/200] [Batch 7/600] [D loss: 0.000052] [G loss: 10.831647]\n",
      "[Epoch 3/200] [Batch 8/600] [D loss: 0.000054] [G loss: 10.756248]\n",
      "[Epoch 3/200] [Batch 9/600] [D loss: 0.000033] [G loss: 10.652102]\n",
      "[Epoch 3/200] [Batch 10/600] [D loss: 0.000045] [G loss: 10.577406]\n",
      "[Epoch 3/200] [Batch 11/600] [D loss: 0.000044] [G loss: 10.736026]\n",
      "[Epoch 3/200] [Batch 12/600] [D loss: 0.000057] [G loss: 10.764102]\n",
      "[Epoch 3/200] [Batch 13/600] [D loss: 0.000049] [G loss: 10.689070]\n",
      "[Epoch 3/200] [Batch 14/600] [D loss: 0.000043] [G loss: 10.769762]\n",
      "[Epoch 3/200] [Batch 15/600] [D loss: 0.000039] [G loss: 10.785893]\n",
      "[Epoch 3/200] [Batch 16/600] [D loss: 0.000059] [G loss: 10.795109]\n",
      "[Epoch 3/200] [Batch 17/600] [D loss: 0.000034] [G loss: 10.690670]\n",
      "[Epoch 3/200] [Batch 18/600] [D loss: 0.000047] [G loss: 10.743473]\n",
      "[Epoch 3/200] [Batch 19/600] [D loss: 0.000043] [G loss: 10.716850]\n",
      "[Epoch 3/200] [Batch 20/600] [D loss: 0.000037] [G loss: 10.663300]\n",
      "[Epoch 3/200] [Batch 21/600] [D loss: 0.000060] [G loss: 10.681705]\n",
      "[Epoch 3/200] [Batch 22/600] [D loss: 0.000041] [G loss: 10.638384]\n",
      "[Epoch 3/200] [Batch 23/600] [D loss: 0.000041] [G loss: 10.916864]\n",
      "[Epoch 3/200] [Batch 24/600] [D loss: 0.000071] [G loss: 10.510763]\n",
      "[Epoch 3/200] [Batch 25/600] [D loss: 0.000046] [G loss: 10.751466]\n",
      "[Epoch 3/200] [Batch 26/600] [D loss: 0.000063] [G loss: 11.085541]\n",
      "[Epoch 3/200] [Batch 27/600] [D loss: 0.000044] [G loss: 10.967142]\n",
      "[Epoch 3/200] [Batch 28/600] [D loss: 0.000054] [G loss: 10.881067]\n",
      "[Epoch 3/200] [Batch 29/600] [D loss: 0.000053] [G loss: 10.819982]\n",
      "[Epoch 3/200] [Batch 30/600] [D loss: 0.000045] [G loss: 10.715628]\n",
      "[Epoch 3/200] [Batch 31/600] [D loss: 0.000051] [G loss: 10.742661]\n",
      "[Epoch 3/200] [Batch 32/600] [D loss: 0.000043] [G loss: 10.806707]\n",
      "[Epoch 3/200] [Batch 33/600] [D loss: 0.000041] [G loss: 10.601624]\n",
      "[Epoch 3/200] [Batch 34/600] [D loss: 0.000043] [G loss: 10.826612]\n",
      "[Epoch 3/200] [Batch 35/600] [D loss: 0.000045] [G loss: 10.848601]\n",
      "[Epoch 3/200] [Batch 36/600] [D loss: 0.000052] [G loss: 10.623019]\n",
      "[Epoch 3/200] [Batch 37/600] [D loss: 0.000046] [G loss: 11.023965]\n",
      "[Epoch 3/200] [Batch 38/600] [D loss: 0.000030] [G loss: 10.976862]\n",
      "[Epoch 3/200] [Batch 39/600] [D loss: 0.000041] [G loss: 11.124009]\n",
      "[Epoch 3/200] [Batch 40/600] [D loss: 0.000039] [G loss: 10.701155]\n",
      "[Epoch 3/200] [Batch 41/600] [D loss: 0.000031] [G loss: 11.066768]\n",
      "[Epoch 3/200] [Batch 42/600] [D loss: 0.000038] [G loss: 10.672747]\n",
      "[Epoch 3/200] [Batch 43/600] [D loss: 0.000042] [G loss: 11.111660]\n",
      "[Epoch 3/200] [Batch 44/600] [D loss: 0.000034] [G loss: 10.949571]\n",
      "[Epoch 3/200] [Batch 45/600] [D loss: 0.000045] [G loss: 10.854402]\n",
      "[Epoch 3/200] [Batch 46/600] [D loss: 0.000056] [G loss: 11.033207]\n",
      "[Epoch 3/200] [Batch 47/600] [D loss: 0.000049] [G loss: 10.899356]\n",
      "[Epoch 3/200] [Batch 48/600] [D loss: 0.000053] [G loss: 11.078439]\n",
      "[Epoch 3/200] [Batch 49/600] [D loss: 0.000054] [G loss: 10.642391]\n",
      "[Epoch 3/200] [Batch 50/600] [D loss: 0.000034] [G loss: 10.985205]\n",
      "[Epoch 3/200] [Batch 51/600] [D loss: 0.000041] [G loss: 10.659833]\n",
      "[Epoch 3/200] [Batch 52/600] [D loss: 0.000032] [G loss: 10.638941]\n",
      "[Epoch 3/200] [Batch 53/600] [D loss: 0.000053] [G loss: 10.762513]\n",
      "[Epoch 3/200] [Batch 54/600] [D loss: 0.000043] [G loss: 10.785705]\n",
      "[Epoch 3/200] [Batch 55/600] [D loss: 0.000042] [G loss: 10.915998]\n",
      "[Epoch 3/200] [Batch 56/600] [D loss: 0.000056] [G loss: 10.738036]\n",
      "[Epoch 3/200] [Batch 57/600] [D loss: 0.000044] [G loss: 10.720647]\n",
      "[Epoch 3/200] [Batch 58/600] [D loss: 0.000048] [G loss: 10.834865]\n",
      "[Epoch 3/200] [Batch 59/600] [D loss: 0.000037] [G loss: 10.682276]\n",
      "[Epoch 3/200] [Batch 60/600] [D loss: 0.000050] [G loss: 10.883418]\n",
      "[Epoch 3/200] [Batch 61/600] [D loss: 0.000044] [G loss: 10.817362]\n",
      "[Epoch 3/200] [Batch 62/600] [D loss: 0.000061] [G loss: 10.741594]\n",
      "[Epoch 3/200] [Batch 63/600] [D loss: 0.000051] [G loss: 10.875235]\n",
      "[Epoch 3/200] [Batch 64/600] [D loss: 0.000045] [G loss: 10.928457]\n",
      "[Epoch 3/200] [Batch 65/600] [D loss: 0.000044] [G loss: 10.765098]\n",
      "[Epoch 3/200] [Batch 66/600] [D loss: 0.000037] [G loss: 10.971319]\n",
      "[Epoch 3/200] [Batch 67/600] [D loss: 0.000047] [G loss: 10.736041]\n",
      "[Epoch 3/200] [Batch 68/600] [D loss: 0.000047] [G loss: 10.732100]\n",
      "[Epoch 3/200] [Batch 69/600] [D loss: 0.000042] [G loss: 10.636749]\n",
      "[Epoch 3/200] [Batch 70/600] [D loss: 0.000033] [G loss: 10.497332]\n",
      "[Epoch 3/200] [Batch 71/600] [D loss: 0.000048] [G loss: 10.972793]\n",
      "[Epoch 3/200] [Batch 72/600] [D loss: 0.000057] [G loss: 10.885662]\n",
      "[Epoch 3/200] [Batch 73/600] [D loss: 0.000045] [G loss: 10.796591]\n",
      "[Epoch 3/200] [Batch 74/600] [D loss: 0.000049] [G loss: 10.944832]\n",
      "[Epoch 3/200] [Batch 75/600] [D loss: 0.000044] [G loss: 10.746171]\n",
      "[Epoch 3/200] [Batch 76/600] [D loss: 0.000045] [G loss: 10.844433]\n",
      "[Epoch 3/200] [Batch 77/600] [D loss: 0.000065] [G loss: 10.985529]\n",
      "[Epoch 3/200] [Batch 78/600] [D loss: 0.000042] [G loss: 10.735900]\n",
      "[Epoch 3/200] [Batch 79/600] [D loss: 0.000044] [G loss: 10.887218]\n",
      "[Epoch 3/200] [Batch 80/600] [D loss: 0.000041] [G loss: 10.907251]\n",
      "[Epoch 3/200] [Batch 81/600] [D loss: 0.000045] [G loss: 10.669178]\n",
      "[Epoch 3/200] [Batch 82/600] [D loss: 0.000047] [G loss: 10.618403]\n",
      "[Epoch 3/200] [Batch 83/600] [D loss: 0.000049] [G loss: 10.949825]\n",
      "[Epoch 3/200] [Batch 84/600] [D loss: 0.000075] [G loss: 10.940965]\n",
      "[Epoch 3/200] [Batch 85/600] [D loss: 0.000045] [G loss: 10.736292]\n",
      "[Epoch 3/200] [Batch 86/600] [D loss: 0.000035] [G loss: 10.885803]\n",
      "[Epoch 3/200] [Batch 87/600] [D loss: 0.000049] [G loss: 10.587330]\n",
      "[Epoch 3/200] [Batch 88/600] [D loss: 0.000035] [G loss: 10.835415]\n",
      "[Epoch 3/200] [Batch 89/600] [D loss: 0.000041] [G loss: 10.974963]\n",
      "[Epoch 3/200] [Batch 90/600] [D loss: 0.000056] [G loss: 10.902855]\n",
      "[Epoch 3/200] [Batch 91/600] [D loss: 0.000041] [G loss: 10.870057]\n",
      "[Epoch 3/200] [Batch 92/600] [D loss: 0.000043] [G loss: 11.040309]\n",
      "[Epoch 3/200] [Batch 93/600] [D loss: 0.000036] [G loss: 11.023922]\n",
      "[Epoch 3/200] [Batch 94/600] [D loss: 0.000042] [G loss: 10.767326]\n",
      "[Epoch 3/200] [Batch 95/600] [D loss: 0.000041] [G loss: 10.796383]\n",
      "[Epoch 3/200] [Batch 96/600] [D loss: 0.000048] [G loss: 10.855529]\n",
      "[Epoch 3/200] [Batch 97/600] [D loss: 0.000051] [G loss: 10.807958]\n",
      "[Epoch 3/200] [Batch 98/600] [D loss: 0.000042] [G loss: 11.126504]\n",
      "[Epoch 3/200] [Batch 99/600] [D loss: 0.000041] [G loss: 10.991308]\n",
      "[Epoch 3/200] [Batch 100/600] [D loss: 0.000061] [G loss: 11.196074]\n",
      "[Epoch 3/200] [Batch 101/600] [D loss: 0.000072] [G loss: 10.627157]\n",
      "[Epoch 3/200] [Batch 102/600] [D loss: 0.000096] [G loss: 10.660951]\n",
      "[Epoch 3/200] [Batch 103/600] [D loss: 0.000044] [G loss: 11.001760]\n",
      "[Epoch 3/200] [Batch 104/600] [D loss: 0.000050] [G loss: 10.998504]\n",
      "[Epoch 3/200] [Batch 105/600] [D loss: 0.000029] [G loss: 10.927662]\n",
      "[Epoch 3/200] [Batch 106/600] [D loss: 0.000038] [G loss: 10.924536]\n",
      "[Epoch 3/200] [Batch 107/600] [D loss: 0.000042] [G loss: 10.854147]\n",
      "[Epoch 3/200] [Batch 108/600] [D loss: 0.000040] [G loss: 10.763437]\n",
      "[Epoch 3/200] [Batch 109/600] [D loss: 0.000047] [G loss: 10.943628]\n",
      "[Epoch 3/200] [Batch 110/600] [D loss: 0.000043] [G loss: 11.010401]\n",
      "[Epoch 3/200] [Batch 111/600] [D loss: 0.000041] [G loss: 10.655596]\n",
      "[Epoch 3/200] [Batch 112/600] [D loss: 0.000070] [G loss: 10.950164]\n",
      "[Epoch 3/200] [Batch 113/600] [D loss: 0.000035] [G loss: 10.692231]\n",
      "[Epoch 3/200] [Batch 114/600] [D loss: 0.000039] [G loss: 10.743516]\n",
      "[Epoch 3/200] [Batch 115/600] [D loss: 0.000053] [G loss: 10.593609]\n",
      "[Epoch 3/200] [Batch 116/600] [D loss: 0.000055] [G loss: 11.121156]\n",
      "[Epoch 3/200] [Batch 117/600] [D loss: 0.000037] [G loss: 10.764981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 118/600] [D loss: 0.000050] [G loss: 10.848480]\n",
      "[Epoch 3/200] [Batch 119/600] [D loss: 0.000035] [G loss: 11.079137]\n",
      "[Epoch 3/200] [Batch 120/600] [D loss: 0.000048] [G loss: 10.796770]\n",
      "[Epoch 3/200] [Batch 121/600] [D loss: 0.000048] [G loss: 10.876620]\n",
      "[Epoch 3/200] [Batch 122/600] [D loss: 0.000040] [G loss: 10.792026]\n",
      "[Epoch 3/200] [Batch 123/600] [D loss: 0.000038] [G loss: 10.832501]\n",
      "[Epoch 3/200] [Batch 124/600] [D loss: 0.000047] [G loss: 10.845831]\n",
      "[Epoch 3/200] [Batch 125/600] [D loss: 0.000039] [G loss: 10.984188]\n",
      "[Epoch 3/200] [Batch 126/600] [D loss: 0.000042] [G loss: 10.730351]\n",
      "[Epoch 3/200] [Batch 127/600] [D loss: 0.000062] [G loss: 10.982428]\n",
      "[Epoch 3/200] [Batch 128/600] [D loss: 0.000031] [G loss: 10.770059]\n",
      "[Epoch 3/200] [Batch 129/600] [D loss: 0.000052] [G loss: 10.993185]\n",
      "[Epoch 3/200] [Batch 130/600] [D loss: 0.000039] [G loss: 10.945296]\n",
      "[Epoch 3/200] [Batch 131/600] [D loss: 0.000060] [G loss: 10.993749]\n",
      "[Epoch 3/200] [Batch 132/600] [D loss: 0.000050] [G loss: 11.051389]\n",
      "[Epoch 3/200] [Batch 133/600] [D loss: 0.000047] [G loss: 10.873983]\n",
      "[Epoch 3/200] [Batch 134/600] [D loss: 0.000040] [G loss: 10.911227]\n",
      "[Epoch 3/200] [Batch 135/600] [D loss: 0.000035] [G loss: 10.821898]\n",
      "[Epoch 3/200] [Batch 136/600] [D loss: 0.000049] [G loss: 10.823794]\n",
      "[Epoch 3/200] [Batch 137/600] [D loss: 0.000035] [G loss: 11.001509]\n",
      "[Epoch 3/200] [Batch 138/600] [D loss: 0.000046] [G loss: 10.725522]\n",
      "[Epoch 3/200] [Batch 139/600] [D loss: 0.000048] [G loss: 10.905356]\n",
      "[Epoch 3/200] [Batch 140/600] [D loss: 0.000063] [G loss: 10.965899]\n",
      "[Epoch 3/200] [Batch 141/600] [D loss: 0.000045] [G loss: 10.731982]\n",
      "[Epoch 3/200] [Batch 142/600] [D loss: 0.000043] [G loss: 10.803198]\n",
      "[Epoch 3/200] [Batch 143/600] [D loss: 0.000040] [G loss: 10.800570]\n",
      "[Epoch 3/200] [Batch 144/600] [D loss: 0.000056] [G loss: 10.970983]\n",
      "[Epoch 3/200] [Batch 145/600] [D loss: 0.000040] [G loss: 10.718954]\n",
      "[Epoch 3/200] [Batch 146/600] [D loss: 0.000038] [G loss: 10.710049]\n",
      "[Epoch 3/200] [Batch 147/600] [D loss: 0.000039] [G loss: 11.030628]\n",
      "[Epoch 3/200] [Batch 148/600] [D loss: 0.000040] [G loss: 11.199358]\n",
      "[Epoch 3/200] [Batch 149/600] [D loss: 0.000061] [G loss: 10.868599]\n",
      "[Epoch 3/200] [Batch 150/600] [D loss: 0.000040] [G loss: 10.715140]\n",
      "[Epoch 3/200] [Batch 151/600] [D loss: 0.000040] [G loss: 10.900931]\n",
      "[Epoch 3/200] [Batch 152/600] [D loss: 0.000040] [G loss: 10.879856]\n",
      "[Epoch 3/200] [Batch 153/600] [D loss: 0.000046] [G loss: 11.021532]\n",
      "[Epoch 3/200] [Batch 154/600] [D loss: 0.000027] [G loss: 10.749539]\n",
      "[Epoch 3/200] [Batch 155/600] [D loss: 0.000044] [G loss: 10.801007]\n",
      "[Epoch 3/200] [Batch 156/600] [D loss: 0.000035] [G loss: 10.585725]\n",
      "[Epoch 3/200] [Batch 157/600] [D loss: 0.000035] [G loss: 10.860678]\n",
      "[Epoch 3/200] [Batch 158/600] [D loss: 0.000051] [G loss: 11.124118]\n",
      "[Epoch 3/200] [Batch 159/600] [D loss: 0.000039] [G loss: 10.933866]\n",
      "[Epoch 3/200] [Batch 160/600] [D loss: 0.000030] [G loss: 10.921901]\n",
      "[Epoch 3/200] [Batch 161/600] [D loss: 0.000034] [G loss: 10.986654]\n",
      "[Epoch 3/200] [Batch 162/600] [D loss: 0.000040] [G loss: 10.896335]\n",
      "[Epoch 3/200] [Batch 163/600] [D loss: 0.000041] [G loss: 11.129542]\n",
      "[Epoch 3/200] [Batch 164/600] [D loss: 0.000043] [G loss: 10.756848]\n",
      "[Epoch 3/200] [Batch 165/600] [D loss: 0.000056] [G loss: 10.886741]\n",
      "[Epoch 3/200] [Batch 166/600] [D loss: 0.000046] [G loss: 11.076515]\n",
      "[Epoch 3/200] [Batch 167/600] [D loss: 0.000052] [G loss: 10.935818]\n",
      "[Epoch 3/200] [Batch 168/600] [D loss: 0.000047] [G loss: 10.708495]\n",
      "[Epoch 3/200] [Batch 169/600] [D loss: 0.000046] [G loss: 11.138645]\n",
      "[Epoch 3/200] [Batch 170/600] [D loss: 0.000036] [G loss: 10.889683]\n",
      "[Epoch 3/200] [Batch 171/600] [D loss: 0.000049] [G loss: 10.888208]\n",
      "[Epoch 3/200] [Batch 172/600] [D loss: 0.000049] [G loss: 10.717729]\n",
      "[Epoch 3/200] [Batch 173/600] [D loss: 0.000037] [G loss: 11.111197]\n",
      "[Epoch 3/200] [Batch 174/600] [D loss: 0.000040] [G loss: 10.925802]\n",
      "[Epoch 3/200] [Batch 175/600] [D loss: 0.000040] [G loss: 10.742931]\n",
      "[Epoch 3/200] [Batch 176/600] [D loss: 0.000037] [G loss: 10.851784]\n",
      "[Epoch 3/200] [Batch 177/600] [D loss: 0.000044] [G loss: 10.864333]\n",
      "[Epoch 3/200] [Batch 178/600] [D loss: 0.000029] [G loss: 10.990679]\n",
      "[Epoch 3/200] [Batch 179/600] [D loss: 0.000049] [G loss: 11.157773]\n",
      "[Epoch 3/200] [Batch 180/600] [D loss: 0.000041] [G loss: 10.834743]\n",
      "[Epoch 3/200] [Batch 181/600] [D loss: 0.000069] [G loss: 10.794103]\n",
      "[Epoch 3/200] [Batch 182/600] [D loss: 0.000049] [G loss: 10.651811]\n",
      "[Epoch 3/200] [Batch 183/600] [D loss: 0.000061] [G loss: 11.048610]\n",
      "[Epoch 3/200] [Batch 184/600] [D loss: 0.000034] [G loss: 10.841953]\n",
      "[Epoch 3/200] [Batch 185/600] [D loss: 0.000054] [G loss: 11.156816]\n",
      "[Epoch 3/200] [Batch 186/600] [D loss: 0.000031] [G loss: 10.690601]\n",
      "[Epoch 3/200] [Batch 187/600] [D loss: 0.000042] [G loss: 10.836853]\n",
      "[Epoch 3/200] [Batch 188/600] [D loss: 0.000030] [G loss: 11.095778]\n",
      "[Epoch 3/200] [Batch 189/600] [D loss: 0.000030] [G loss: 10.922940]\n",
      "[Epoch 3/200] [Batch 190/600] [D loss: 0.000040] [G loss: 11.227715]\n",
      "[Epoch 3/200] [Batch 191/600] [D loss: 0.000033] [G loss: 11.147815]\n",
      "[Epoch 3/200] [Batch 192/600] [D loss: 0.000045] [G loss: 10.762433]\n",
      "[Epoch 3/200] [Batch 193/600] [D loss: 0.000066] [G loss: 10.885425]\n",
      "[Epoch 3/200] [Batch 194/600] [D loss: 0.000047] [G loss: 11.006805]\n",
      "[Epoch 3/200] [Batch 195/600] [D loss: 0.000029] [G loss: 11.352525]\n",
      "[Epoch 3/200] [Batch 196/600] [D loss: 0.000034] [G loss: 10.931406]\n",
      "[Epoch 3/200] [Batch 197/600] [D loss: 0.000037] [G loss: 10.991198]\n",
      "[Epoch 3/200] [Batch 198/600] [D loss: 0.000032] [G loss: 10.636655]\n",
      "[Epoch 3/200] [Batch 199/600] [D loss: 0.000045] [G loss: 10.837190]\n",
      "[Epoch 3/200] [Batch 200/600] [D loss: 0.000054] [G loss: 11.074180]\n",
      "[Epoch 3/200] [Batch 201/600] [D loss: 0.000049] [G loss: 11.064793]\n",
      "[Epoch 3/200] [Batch 202/600] [D loss: 0.000040] [G loss: 10.926617]\n",
      "[Epoch 3/200] [Batch 203/600] [D loss: 0.000041] [G loss: 10.875182]\n",
      "[Epoch 3/200] [Batch 204/600] [D loss: 0.000039] [G loss: 10.881000]\n",
      "[Epoch 3/200] [Batch 205/600] [D loss: 0.000038] [G loss: 10.984413]\n",
      "[Epoch 3/200] [Batch 206/600] [D loss: 0.000032] [G loss: 10.853233]\n",
      "[Epoch 3/200] [Batch 207/600] [D loss: 0.000031] [G loss: 11.064781]\n",
      "[Epoch 3/200] [Batch 208/600] [D loss: 0.000031] [G loss: 10.960297]\n",
      "[Epoch 3/200] [Batch 209/600] [D loss: 0.000032] [G loss: 11.280379]\n",
      "[Epoch 3/200] [Batch 210/600] [D loss: 0.000063] [G loss: 10.852575]\n",
      "[Epoch 3/200] [Batch 211/600] [D loss: 0.000039] [G loss: 10.970325]\n",
      "[Epoch 3/200] [Batch 212/600] [D loss: 0.000041] [G loss: 11.180935]\n",
      "[Epoch 3/200] [Batch 213/600] [D loss: 0.000036] [G loss: 11.215970]\n",
      "[Epoch 3/200] [Batch 214/600] [D loss: 0.000040] [G loss: 10.999102]\n",
      "[Epoch 3/200] [Batch 215/600] [D loss: 0.000032] [G loss: 11.089139]\n",
      "[Epoch 3/200] [Batch 216/600] [D loss: 0.000031] [G loss: 11.016319]\n",
      "[Epoch 3/200] [Batch 217/600] [D loss: 0.000049] [G loss: 11.157746]\n",
      "[Epoch 3/200] [Batch 218/600] [D loss: 0.000037] [G loss: 10.907140]\n",
      "[Epoch 3/200] [Batch 219/600] [D loss: 0.000037] [G loss: 11.104977]\n",
      "[Epoch 3/200] [Batch 220/600] [D loss: 0.000050] [G loss: 11.048776]\n",
      "[Epoch 3/200] [Batch 221/600] [D loss: 0.000028] [G loss: 11.066750]\n",
      "[Epoch 3/200] [Batch 222/600] [D loss: 0.000033] [G loss: 11.146350]\n",
      "[Epoch 3/200] [Batch 223/600] [D loss: 0.000032] [G loss: 10.914219]\n",
      "[Epoch 3/200] [Batch 224/600] [D loss: 0.000036] [G loss: 11.205304]\n",
      "[Epoch 3/200] [Batch 225/600] [D loss: 0.000053] [G loss: 11.050389]\n",
      "[Epoch 3/200] [Batch 226/600] [D loss: 0.000027] [G loss: 11.039717]\n",
      "[Epoch 3/200] [Batch 227/600] [D loss: 0.000048] [G loss: 11.165463]\n",
      "[Epoch 3/200] [Batch 228/600] [D loss: 0.000028] [G loss: 10.900647]\n",
      "[Epoch 3/200] [Batch 229/600] [D loss: 0.000037] [G loss: 11.305425]\n",
      "[Epoch 3/200] [Batch 230/600] [D loss: 0.000027] [G loss: 11.061932]\n",
      "[Epoch 3/200] [Batch 231/600] [D loss: 0.000038] [G loss: 11.112631]\n",
      "[Epoch 3/200] [Batch 232/600] [D loss: 0.000041] [G loss: 11.003736]\n",
      "[Epoch 3/200] [Batch 233/600] [D loss: 0.000033] [G loss: 10.862192]\n",
      "[Epoch 3/200] [Batch 234/600] [D loss: 0.000031] [G loss: 10.965335]\n",
      "[Epoch 3/200] [Batch 235/600] [D loss: 0.000042] [G loss: 11.019835]\n",
      "[Epoch 3/200] [Batch 236/600] [D loss: 0.000035] [G loss: 10.786163]\n",
      "[Epoch 3/200] [Batch 237/600] [D loss: 0.000053] [G loss: 11.157071]\n",
      "[Epoch 3/200] [Batch 238/600] [D loss: 0.000041] [G loss: 10.731936]\n",
      "[Epoch 3/200] [Batch 239/600] [D loss: 0.000041] [G loss: 10.779123]\n",
      "[Epoch 3/200] [Batch 240/600] [D loss: 0.000038] [G loss: 11.234296]\n",
      "[Epoch 3/200] [Batch 241/600] [D loss: 0.000034] [G loss: 10.754437]\n",
      "[Epoch 3/200] [Batch 242/600] [D loss: 0.000041] [G loss: 11.048108]\n",
      "[Epoch 3/200] [Batch 243/600] [D loss: 0.000032] [G loss: 10.967685]\n",
      "[Epoch 3/200] [Batch 244/600] [D loss: 0.000032] [G loss: 10.843372]\n",
      "[Epoch 3/200] [Batch 245/600] [D loss: 0.000041] [G loss: 11.231697]\n",
      "[Epoch 3/200] [Batch 246/600] [D loss: 0.000027] [G loss: 11.095516]\n",
      "[Epoch 3/200] [Batch 247/600] [D loss: 0.000034] [G loss: 11.017163]\n",
      "[Epoch 3/200] [Batch 248/600] [D loss: 0.000029] [G loss: 11.097884]\n",
      "[Epoch 3/200] [Batch 249/600] [D loss: 0.000052] [G loss: 11.075354]\n",
      "[Epoch 3/200] [Batch 250/600] [D loss: 0.000027] [G loss: 11.258845]\n",
      "[Epoch 3/200] [Batch 251/600] [D loss: 0.000027] [G loss: 11.004302]\n",
      "[Epoch 3/200] [Batch 252/600] [D loss: 0.000028] [G loss: 11.053267]\n",
      "[Epoch 3/200] [Batch 253/600] [D loss: 0.000029] [G loss: 11.095473]\n",
      "[Epoch 3/200] [Batch 254/600] [D loss: 0.000044] [G loss: 10.883697]\n",
      "[Epoch 3/200] [Batch 255/600] [D loss: 0.000029] [G loss: 11.060195]\n",
      "[Epoch 3/200] [Batch 256/600] [D loss: 0.000032] [G loss: 10.879914]\n",
      "[Epoch 3/200] [Batch 257/600] [D loss: 0.000051] [G loss: 11.160160]\n",
      "[Epoch 3/200] [Batch 258/600] [D loss: 0.000036] [G loss: 11.099645]\n",
      "[Epoch 3/200] [Batch 259/600] [D loss: 0.000036] [G loss: 11.128305]\n",
      "[Epoch 3/200] [Batch 260/600] [D loss: 0.000033] [G loss: 10.932520]\n",
      "[Epoch 3/200] [Batch 261/600] [D loss: 0.000027] [G loss: 10.898794]\n",
      "[Epoch 3/200] [Batch 262/600] [D loss: 0.000029] [G loss: 10.897898]\n",
      "[Epoch 3/200] [Batch 263/600] [D loss: 0.000036] [G loss: 11.158579]\n",
      "[Epoch 3/200] [Batch 264/600] [D loss: 0.000041] [G loss: 10.731828]\n",
      "[Epoch 3/200] [Batch 265/600] [D loss: 0.000037] [G loss: 11.200539]\n",
      "[Epoch 3/200] [Batch 266/600] [D loss: 0.000079] [G loss: 11.200553]\n",
      "[Epoch 3/200] [Batch 267/600] [D loss: 0.000032] [G loss: 10.922810]\n",
      "[Epoch 3/200] [Batch 268/600] [D loss: 0.000035] [G loss: 11.102170]\n",
      "[Epoch 3/200] [Batch 269/600] [D loss: 0.000024] [G loss: 10.704700]\n",
      "[Epoch 3/200] [Batch 270/600] [D loss: 0.000038] [G loss: 11.200123]\n",
      "[Epoch 3/200] [Batch 271/600] [D loss: 0.000023] [G loss: 11.041995]\n",
      "[Epoch 3/200] [Batch 272/600] [D loss: 0.000032] [G loss: 11.213432]\n",
      "[Epoch 3/200] [Batch 273/600] [D loss: 0.000044] [G loss: 11.069911]\n",
      "[Epoch 3/200] [Batch 274/600] [D loss: 0.000028] [G loss: 11.162257]\n",
      "[Epoch 3/200] [Batch 275/600] [D loss: 0.000025] [G loss: 11.045041]\n",
      "[Epoch 3/200] [Batch 276/600] [D loss: 0.000032] [G loss: 10.995365]\n",
      "[Epoch 3/200] [Batch 277/600] [D loss: 0.000030] [G loss: 10.854771]\n",
      "[Epoch 3/200] [Batch 278/600] [D loss: 0.000035] [G loss: 10.990318]\n",
      "[Epoch 3/200] [Batch 279/600] [D loss: 0.000034] [G loss: 11.121649]\n",
      "[Epoch 3/200] [Batch 280/600] [D loss: 0.000035] [G loss: 10.966798]\n",
      "[Epoch 3/200] [Batch 281/600] [D loss: 0.000042] [G loss: 10.900347]\n",
      "[Epoch 3/200] [Batch 282/600] [D loss: 0.000033] [G loss: 11.039572]\n",
      "[Epoch 3/200] [Batch 283/600] [D loss: 0.000033] [G loss: 11.157954]\n",
      "[Epoch 3/200] [Batch 284/600] [D loss: 0.000028] [G loss: 11.429259]\n",
      "[Epoch 3/200] [Batch 285/600] [D loss: 0.000037] [G loss: 10.844426]\n",
      "[Epoch 3/200] [Batch 286/600] [D loss: 0.000037] [G loss: 11.005582]\n",
      "[Epoch 3/200] [Batch 287/600] [D loss: 0.000054] [G loss: 11.031146]\n",
      "[Epoch 3/200] [Batch 288/600] [D loss: 0.000043] [G loss: 11.074664]\n",
      "[Epoch 3/200] [Batch 289/600] [D loss: 0.000037] [G loss: 11.081352]\n",
      "[Epoch 3/200] [Batch 290/600] [D loss: 0.000047] [G loss: 11.238804]\n",
      "[Epoch 3/200] [Batch 291/600] [D loss: 0.000041] [G loss: 11.187770]\n",
      "[Epoch 3/200] [Batch 292/600] [D loss: 0.000031] [G loss: 11.031939]\n",
      "[Epoch 3/200] [Batch 293/600] [D loss: 0.000033] [G loss: 11.471691]\n",
      "[Epoch 3/200] [Batch 294/600] [D loss: 0.000026] [G loss: 11.017635]\n",
      "[Epoch 3/200] [Batch 295/600] [D loss: 0.000033] [G loss: 11.293325]\n",
      "[Epoch 3/200] [Batch 296/600] [D loss: 0.000028] [G loss: 11.195683]\n",
      "[Epoch 3/200] [Batch 297/600] [D loss: 0.000031] [G loss: 11.243899]\n",
      "[Epoch 3/200] [Batch 298/600] [D loss: 0.000027] [G loss: 10.915943]\n",
      "[Epoch 3/200] [Batch 299/600] [D loss: 0.000053] [G loss: 11.134695]\n",
      "[Epoch 3/200] [Batch 300/600] [D loss: 0.000027] [G loss: 11.137262]\n",
      "[Epoch 3/200] [Batch 301/600] [D loss: 0.000037] [G loss: 11.270483]\n",
      "[Epoch 3/200] [Batch 302/600] [D loss: 0.000034] [G loss: 11.337109]\n",
      "[Epoch 3/200] [Batch 303/600] [D loss: 0.000040] [G loss: 11.027696]\n",
      "[Epoch 3/200] [Batch 304/600] [D loss: 0.000031] [G loss: 11.415258]\n",
      "[Epoch 3/200] [Batch 305/600] [D loss: 0.000040] [G loss: 11.163627]\n",
      "[Epoch 3/200] [Batch 306/600] [D loss: 0.000034] [G loss: 11.159968]\n",
      "[Epoch 3/200] [Batch 307/600] [D loss: 0.000038] [G loss: 11.434912]\n",
      "[Epoch 3/200] [Batch 308/600] [D loss: 0.000029] [G loss: 11.084400]\n",
      "[Epoch 3/200] [Batch 309/600] [D loss: 0.000039] [G loss: 10.990592]\n",
      "[Epoch 3/200] [Batch 310/600] [D loss: 0.000035] [G loss: 10.910116]\n",
      "[Epoch 3/200] [Batch 311/600] [D loss: 0.000041] [G loss: 11.137793]\n",
      "[Epoch 3/200] [Batch 312/600] [D loss: 0.000034] [G loss: 11.147146]\n",
      "[Epoch 3/200] [Batch 313/600] [D loss: 0.000030] [G loss: 11.190143]\n",
      "[Epoch 3/200] [Batch 314/600] [D loss: 0.000033] [G loss: 11.037375]\n",
      "[Epoch 3/200] [Batch 315/600] [D loss: 0.000028] [G loss: 11.325476]\n",
      "[Epoch 3/200] [Batch 316/600] [D loss: 0.000028] [G loss: 11.117487]\n",
      "[Epoch 3/200] [Batch 317/600] [D loss: 0.000037] [G loss: 10.639956]\n",
      "[Epoch 3/200] [Batch 318/600] [D loss: 0.000030] [G loss: 11.376749]\n",
      "[Epoch 3/200] [Batch 319/600] [D loss: 0.000033] [G loss: 11.213181]\n",
      "[Epoch 3/200] [Batch 320/600] [D loss: 0.000024] [G loss: 10.838122]\n",
      "[Epoch 3/200] [Batch 321/600] [D loss: 0.000045] [G loss: 11.514940]\n",
      "[Epoch 3/200] [Batch 322/600] [D loss: 0.000048] [G loss: 11.121532]\n",
      "[Epoch 3/200] [Batch 323/600] [D loss: 0.000034] [G loss: 11.328839]\n",
      "[Epoch 3/200] [Batch 324/600] [D loss: 0.000032] [G loss: 11.087564]\n",
      "[Epoch 3/200] [Batch 325/600] [D loss: 0.000050] [G loss: 11.392187]\n",
      "[Epoch 3/200] [Batch 326/600] [D loss: 0.000034] [G loss: 11.038282]\n",
      "[Epoch 3/200] [Batch 327/600] [D loss: 0.000030] [G loss: 10.929673]\n",
      "[Epoch 3/200] [Batch 328/600] [D loss: 0.000036] [G loss: 11.128315]\n",
      "[Epoch 3/200] [Batch 329/600] [D loss: 0.000033] [G loss: 11.087851]\n",
      "[Epoch 3/200] [Batch 330/600] [D loss: 0.000030] [G loss: 11.308020]\n",
      "[Epoch 3/200] [Batch 331/600] [D loss: 0.000033] [G loss: 11.008075]\n",
      "[Epoch 3/200] [Batch 332/600] [D loss: 0.000041] [G loss: 11.275283]\n",
      "[Epoch 3/200] [Batch 333/600] [D loss: 0.000028] [G loss: 11.137957]\n",
      "[Epoch 3/200] [Batch 334/600] [D loss: 0.000032] [G loss: 10.929751]\n",
      "[Epoch 3/200] [Batch 335/600] [D loss: 0.000028] [G loss: 10.862013]\n",
      "[Epoch 3/200] [Batch 336/600] [D loss: 0.000033] [G loss: 11.226238]\n",
      "[Epoch 3/200] [Batch 337/600] [D loss: 0.000050] [G loss: 11.241112]\n",
      "[Epoch 3/200] [Batch 338/600] [D loss: 0.000033] [G loss: 11.294181]\n",
      "[Epoch 3/200] [Batch 339/600] [D loss: 0.000046] [G loss: 11.206287]\n",
      "[Epoch 3/200] [Batch 340/600] [D loss: 0.000055] [G loss: 11.187420]\n",
      "[Epoch 3/200] [Batch 341/600] [D loss: 0.000029] [G loss: 11.095933]\n",
      "[Epoch 3/200] [Batch 342/600] [D loss: 0.000053] [G loss: 11.000453]\n",
      "[Epoch 3/200] [Batch 343/600] [D loss: 0.000051] [G loss: 11.097207]\n",
      "[Epoch 3/200] [Batch 344/600] [D loss: 0.000029] [G loss: 11.335479]\n",
      "[Epoch 3/200] [Batch 345/600] [D loss: 0.000035] [G loss: 11.210785]\n",
      "[Epoch 3/200] [Batch 346/600] [D loss: 0.000033] [G loss: 11.071170]\n",
      "[Epoch 3/200] [Batch 347/600] [D loss: 0.000038] [G loss: 11.171340]\n",
      "[Epoch 3/200] [Batch 348/600] [D loss: 0.000028] [G loss: 11.358346]\n",
      "[Epoch 3/200] [Batch 349/600] [D loss: 0.000025] [G loss: 11.390251]\n",
      "[Epoch 3/200] [Batch 350/600] [D loss: 0.000039] [G loss: 11.071881]\n",
      "[Epoch 3/200] [Batch 351/600] [D loss: 0.000034] [G loss: 11.140656]\n",
      "[Epoch 3/200] [Batch 352/600] [D loss: 0.000035] [G loss: 11.033562]\n",
      "[Epoch 3/200] [Batch 353/600] [D loss: 0.000041] [G loss: 11.032945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 354/600] [D loss: 0.000029] [G loss: 11.063530]\n",
      "[Epoch 3/200] [Batch 355/600] [D loss: 0.000040] [G loss: 11.105552]\n",
      "[Epoch 3/200] [Batch 356/600] [D loss: 0.000043] [G loss: 11.230361]\n",
      "[Epoch 3/200] [Batch 357/600] [D loss: 0.000028] [G loss: 11.306884]\n",
      "[Epoch 3/200] [Batch 358/600] [D loss: 0.000036] [G loss: 11.359654]\n",
      "[Epoch 3/200] [Batch 359/600] [D loss: 0.000026] [G loss: 11.179726]\n",
      "[Epoch 3/200] [Batch 360/600] [D loss: 0.000029] [G loss: 10.985540]\n",
      "[Epoch 3/200] [Batch 361/600] [D loss: 0.000033] [G loss: 10.920074]\n",
      "[Epoch 3/200] [Batch 362/600] [D loss: 0.000027] [G loss: 11.145905]\n",
      "[Epoch 3/200] [Batch 363/600] [D loss: 0.000035] [G loss: 11.363389]\n",
      "[Epoch 3/200] [Batch 364/600] [D loss: 0.000028] [G loss: 11.215069]\n",
      "[Epoch 3/200] [Batch 365/600] [D loss: 0.000041] [G loss: 11.027052]\n",
      "[Epoch 3/200] [Batch 366/600] [D loss: 0.000042] [G loss: 11.178297]\n",
      "[Epoch 3/200] [Batch 367/600] [D loss: 0.000026] [G loss: 10.897076]\n",
      "[Epoch 3/200] [Batch 368/600] [D loss: 0.000031] [G loss: 11.256657]\n",
      "[Epoch 3/200] [Batch 369/600] [D loss: 0.000036] [G loss: 10.744092]\n",
      "[Epoch 3/200] [Batch 370/600] [D loss: 0.000023] [G loss: 10.910951]\n",
      "[Epoch 3/200] [Batch 371/600] [D loss: 0.000036] [G loss: 10.724912]\n",
      "[Epoch 3/200] [Batch 372/600] [D loss: 0.000035] [G loss: 11.330890]\n",
      "[Epoch 3/200] [Batch 373/600] [D loss: 0.000045] [G loss: 11.145956]\n",
      "[Epoch 3/200] [Batch 374/600] [D loss: 0.000027] [G loss: 11.080559]\n",
      "[Epoch 3/200] [Batch 375/600] [D loss: 0.000032] [G loss: 11.116322]\n",
      "[Epoch 3/200] [Batch 376/600] [D loss: 0.000030] [G loss: 10.985377]\n",
      "[Epoch 3/200] [Batch 377/600] [D loss: 0.000033] [G loss: 11.215893]\n",
      "[Epoch 3/200] [Batch 378/600] [D loss: 0.000042] [G loss: 11.059746]\n",
      "[Epoch 3/200] [Batch 379/600] [D loss: 0.000026] [G loss: 11.111554]\n",
      "[Epoch 3/200] [Batch 380/600] [D loss: 0.000023] [G loss: 11.240623]\n",
      "[Epoch 3/200] [Batch 381/600] [D loss: 0.000031] [G loss: 11.216486]\n",
      "[Epoch 3/200] [Batch 382/600] [D loss: 0.000024] [G loss: 11.123178]\n",
      "[Epoch 3/200] [Batch 383/600] [D loss: 0.000030] [G loss: 11.067562]\n",
      "[Epoch 3/200] [Batch 384/600] [D loss: 0.000022] [G loss: 11.056999]\n",
      "[Epoch 3/200] [Batch 385/600] [D loss: 0.000028] [G loss: 11.620091]\n",
      "[Epoch 3/200] [Batch 386/600] [D loss: 0.000032] [G loss: 11.334442]\n",
      "[Epoch 3/200] [Batch 387/600] [D loss: 0.000031] [G loss: 11.317064]\n",
      "[Epoch 3/200] [Batch 388/600] [D loss: 0.000034] [G loss: 10.969897]\n",
      "[Epoch 3/200] [Batch 389/600] [D loss: 0.000026] [G loss: 11.195260]\n",
      "[Epoch 3/200] [Batch 390/600] [D loss: 0.000039] [G loss: 10.915394]\n",
      "[Epoch 3/200] [Batch 391/600] [D loss: 0.000038] [G loss: 11.092411]\n",
      "[Epoch 3/200] [Batch 392/600] [D loss: 0.000036] [G loss: 11.688254]\n",
      "[Epoch 3/200] [Batch 393/600] [D loss: 0.000028] [G loss: 11.428082]\n",
      "[Epoch 3/200] [Batch 394/600] [D loss: 0.000033] [G loss: 11.493447]\n",
      "[Epoch 3/200] [Batch 395/600] [D loss: 0.000080] [G loss: 11.355544]\n",
      "[Epoch 3/200] [Batch 396/600] [D loss: 0.000035] [G loss: 11.399167]\n",
      "[Epoch 3/200] [Batch 397/600] [D loss: 0.000026] [G loss: 11.262845]\n",
      "[Epoch 3/200] [Batch 398/600] [D loss: 0.000043] [G loss: 10.831754]\n",
      "[Epoch 3/200] [Batch 399/600] [D loss: 0.000023] [G loss: 10.986337]\n",
      "[Epoch 3/200] [Batch 400/600] [D loss: 0.000054] [G loss: 11.232583]\n",
      "[Epoch 3/200] [Batch 401/600] [D loss: 0.000024] [G loss: 11.388511]\n",
      "[Epoch 3/200] [Batch 402/600] [D loss: 0.000041] [G loss: 10.829377]\n",
      "[Epoch 3/200] [Batch 403/600] [D loss: 0.000028] [G loss: 11.136899]\n",
      "[Epoch 3/200] [Batch 404/600] [D loss: 0.000027] [G loss: 11.423086]\n",
      "[Epoch 3/200] [Batch 405/600] [D loss: 0.000030] [G loss: 11.223769]\n",
      "[Epoch 3/200] [Batch 406/600] [D loss: 0.000030] [G loss: 11.068093]\n",
      "[Epoch 3/200] [Batch 407/600] [D loss: 0.000022] [G loss: 11.099143]\n",
      "[Epoch 3/200] [Batch 408/600] [D loss: 0.000034] [G loss: 11.211128]\n",
      "[Epoch 3/200] [Batch 409/600] [D loss: 0.000033] [G loss: 11.146938]\n",
      "[Epoch 3/200] [Batch 410/600] [D loss: 0.000032] [G loss: 11.124845]\n",
      "[Epoch 3/200] [Batch 411/600] [D loss: 0.000022] [G loss: 11.452997]\n",
      "[Epoch 3/200] [Batch 412/600] [D loss: 0.000038] [G loss: 11.095808]\n",
      "[Epoch 3/200] [Batch 413/600] [D loss: 0.000028] [G loss: 11.273911]\n",
      "[Epoch 3/200] [Batch 414/600] [D loss: 0.000031] [G loss: 11.236647]\n",
      "[Epoch 3/200] [Batch 415/600] [D loss: 0.000026] [G loss: 11.462360]\n",
      "[Epoch 3/200] [Batch 416/600] [D loss: 0.000034] [G loss: 11.369433]\n",
      "[Epoch 3/200] [Batch 417/600] [D loss: 0.000029] [G loss: 11.271577]\n",
      "[Epoch 3/200] [Batch 418/600] [D loss: 0.000032] [G loss: 10.953879]\n",
      "[Epoch 3/200] [Batch 419/600] [D loss: 0.000039] [G loss: 11.317986]\n",
      "[Epoch 3/200] [Batch 420/600] [D loss: 0.000037] [G loss: 11.203996]\n",
      "[Epoch 3/200] [Batch 421/600] [D loss: 0.000049] [G loss: 11.169072]\n",
      "[Epoch 3/200] [Batch 422/600] [D loss: 0.000035] [G loss: 11.124860]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, 200, i, len(dataloader),\n",
    "                                                            d_loss.item(), g_loss.item()))\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % 400 == 0:\n",
    "            save_image(gen_imgs.data[:25], 'E:/DataSet/etc/dcgan/%d.png' % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
